{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train_sdf_loss_setups.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9yeZRQtBNXlg"},"source":["#### Colab run"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkezriwYKMI1","executionInfo":{"status":"ok","timestamp":1630009901487,"user_tz":-120,"elapsed":17293,"user":{"displayName":"Paul ten Kaate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64","userId":"16522113836666229259"}},"outputId":"bb4e9c52-9a5e-4055-b816-2fc027914d2a"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MxfXBh-4Mq7g","executionInfo":{"status":"ok","timestamp":1630009901789,"user_tz":-120,"elapsed":309,"user":{"displayName":"Paul ten Kaate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64","userId":"16522113836666229259"}},"outputId":"213b5733-5d32-4d08-b8d9-1147b3092aee"},"source":["cd drive/MyDrive/master_thesis/final_code"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/16udig9ZMaNcASs5Maj6kv7tg-TL3PnSE/Master Thesis/final_code\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"diXDczG5Y-79","executionInfo":{"status":"ok","timestamp":1630009907552,"user_tz":-120,"elapsed":5769,"user":{"displayName":"Paul ten Kaate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64","userId":"16522113836666229259"}},"outputId":"558e5698-b0bc-48c1-baff-1396c207ba46"},"source":["!pip install kornia\n","!pip install torchinfo"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting kornia\n","  Downloading kornia-0.5.8-py2.py3-none-any.whl (303 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 36.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 61 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 92 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 102 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 112 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 122 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 153 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 163 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 174 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 184 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 194 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 204 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 215 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 225 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 235 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 245 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 256 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 266 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 276 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 286 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 296 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 303 kB 7.8 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->kornia) (3.7.4.3)\n","Installing collected packages: kornia\n","Successfully installed kornia-0.5.8\n","Collecting torchinfo\n","  Downloading torchinfo-1.5.3-py3-none-any.whl (19 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.5.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZUdP95ixdu2","executionInfo":{"status":"ok","timestamp":1630009907553,"user_tz":-120,"elapsed":28,"user":{"displayName":"Paul ten Kaate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64","userId":"16522113836666229259"}},"outputId":"0860ef8e-df62-4f2b-9a10-8d9290b80eb4"},"source":["!nvidia-smi"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Thu Aug 26 20:31:47 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oLM-4phiTeNM"},"source":["#### Imports"]},{"cell_type":"code","metadata":{"id":"BdmZCajHJ73l","executionInfo":{"status":"ok","timestamp":1630009914044,"user_tz":-120,"elapsed":6498,"user":{"displayName":"Paul ten Kaate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64","userId":"16522113836666229259"}}},"source":["import warnings\n","import time\n","\n","from py_files.args import *\n","\n","from py_files.functions import *\n","\n","from py_files.dataset import *\n","\n","from py_files.cnn_models import *\n","from py_files.mapping_models import *\n","from py_files.pigan_model import *\n","\n","from py_files.load_utils import *\n","from py_files.data_utils import *\n","from py_files.plot_utils import *\n","from py_files.loss_utils import *\n","from py_files.train_utils import *\n","from py_files.save_utils import *"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TCWHjP8GJ73q"},"source":["### Different lambda"]},{"cell_type":"code","metadata":{"id":"lsAX3yainjMQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630053464696,"user_tz":-120,"elapsed":278975,"user":{"displayName":"Paul ten Kaate","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64","userId":"16522113836666229259"}},"outputId":"6fbf79f0-3fa2-46ba-d63d-9af586dc0b84"},"source":["lambdas = [(1e2, 1e2, 1e1, 5e0), \n","           (1e2, 1e2, 3e1, 5e0),\n","           (1e2, 1e2, 1e1, 1e1),\n","           (3e2, 1e2, 1e1, 5e0)\n","           ]\n","\n","for i in range(2):\n","    for lamb in lambdas:\n","        sdf, inter, normal, grad = lamb \n","        ARGS = init_ARGS()\n","\n","        ARGS.training_setup = \"segmentation\"\n","        ARGS.segmentation = \"sdf\"\n","\n","        ARGS.first_omega_0 = 100\n","\n","        ARGS.lambda_sdf = sdf\n","        ARGS.lambda_inter = inter\n","        ARGS.lambda_inter = normal\n","        ARGS.lambda_grad = grad\n","\n","        print(f\"Starting training {ARGS.name}.\")\n","        \n","        print(vars(ARGS))\n","\n","        complete_training(ARGS)  \n","        \n","        torch.cuda.empty_cache()    "],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING: ARGS class initialized.\n","Starting training segmentation_sdf_(100.0, 100.0, 10.0, 5.0)_omega_0_100__run_0.\n","{'device': 'GPU', 'print_models': False, 'save_models': True, 'name': 'segmentation_sdf_(100.0, 100.0, 10.0, 5.0)_omega_0_100__run_0', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'seed': 34, 'n_coords_sample': 5000, 'norm_min_max': [0, 1], 'rotate': True, 'translate': True, 'crop': True, 'stretch': True, 'flip': False, 'translate_max_pixels': 20, 'stretch_factor': 1.2, 'pcmra_epochs': 5000, 'mask_epochs': 5000, 'batch_size': 24, 'eval_every': 50, 'shuffle': True, 'min_lr': 1e-05, 'patience': 50, 'cnn_setup': 'golden', 'mapping_setup': 'golden', 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 100, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'segmentation': 'sdf', 'cnn_lr': 0.0001, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'pcmra_siren_lr': 0.0001, 'cnn_wd': 0, 'siren_wd': 0, 'pcmra_siren_wd': 0, 'lambda_sdf': 100.0, 'lambda_inter': 10.0, 'lambda_normal': 10.0, 'lambda_grad': 5.0, 'sdf_split': 0.5}\n","saved_runs/pi_gan_2021_08_26_20_31_53_segmentation_sdf_(100.0, 100.0, 10.0, 5.0)_omega_0_100__run_0\n","----------------------------------\n","Using device for training: cuda\n","----------------------------------\n","Train subjects: 82\n","Train batch: ('91', '112', '73', '46', '9')\n","Val subjects: 32\n","Val batch: ('10', '121', '11', '129', '131')\n","Test subjects: 30\n","Test batch: ('133', '134', '15', '142', '138')\n","CNN\n","MAPPING\n","SIREN\n","PCMRA_MAPPING\n","PCMRA_SIREN\n","Epoch 0, train loss 11.098404\n","Epoch 0 took 7.4 seconds.\n","mask_loss       Train: 0.703379, \t Eval: 0.703318\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.943035, \t Eval: 0.943931\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","WARNING: ARGS class initialized.\n","Starting training segmentation_sdf_(100.0, 100.0, 30.0, 5.0)_omega_0_100__run_0.\n","{'device': 'GPU', 'print_models': False, 'save_models': True, 'name': 'segmentation_sdf_(100.0, 100.0, 30.0, 5.0)_omega_0_100__run_0', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'seed': 34, 'n_coords_sample': 5000, 'norm_min_max': [0, 1], 'rotate': True, 'translate': True, 'crop': True, 'stretch': True, 'flip': False, 'translate_max_pixels': 20, 'stretch_factor': 1.2, 'pcmra_epochs': 5000, 'mask_epochs': 5000, 'batch_size': 24, 'eval_every': 50, 'shuffle': True, 'min_lr': 1e-05, 'patience': 50, 'cnn_setup': 'golden', 'mapping_setup': 'golden', 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 100, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'segmentation': 'sdf', 'cnn_lr': 0.0001, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'pcmra_siren_lr': 0.0001, 'cnn_wd': 0, 'siren_wd': 0, 'pcmra_siren_wd': 0, 'lambda_sdf': 100.0, 'lambda_inter': 30.0, 'lambda_normal': 10.0, 'lambda_grad': 5.0, 'sdf_split': 0.5}\n","saved_runs/pi_gan_2021_08_26_20_35_00_segmentation_sdf_(100.0, 100.0, 30.0, 5.0)_omega_0_100__run_0\n","----------------------------------\n","Using device for training: cuda\n","----------------------------------\n","Train subjects: 82\n","Train batch: ('43', '66', '67', '99', '120')\n","Val subjects: 32\n","Val batch: ('10', '121', '11', '129', '131')\n","Test subjects: 30\n","Test batch: ('133', '134', '15', '142', '138')\n","CNN\n","MAPPING\n","SIREN\n","PCMRA_MAPPING\n","PCMRA_SIREN\n","Epoch 0, train loss 11.727768\n","Epoch 0 took 7.54 seconds.\n","mask_loss       Train: 0.678655, \t Eval: 0.678641\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 1.000000, \t Eval: 1.000000\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 10, train loss 10.42345\n","Epoch 20, train loss 10.025668\n","Epoch 30, train loss 9.409156\n","Epoch 40, train loss 9.113744\n","Epoch 50, train loss 8.639804\n","Epoch 50 took 7.77 seconds.\n","mask_loss       Train: 0.583304, \t Eval: 0.585445\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.831274, \t Eval: 0.871610\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 60, train loss 7.993494\n","Epoch 70, train loss 7.477278\n","Epoch 80, train loss 6.97439\n","Epoch 90, train loss 6.747247\n","Epoch 100, train loss 6.192475\n","Epoch 100 took 7.76 seconds.\n","mask_loss       Train: 0.526349, \t Eval: 0.520249\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.462454, \t Eval: 0.460186\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 110, train loss 6.037659\n","Epoch 120, train loss 5.907954\n","Epoch 130, train loss 5.706103\n","Epoch 140, train loss 5.353211\n","Epoch 150, train loss 5.701243\n","Epoch 150 took 7.76 seconds.\n","mask_loss       Train: 0.536612, \t Eval: 0.537469\n","dice_loss       Train: 0.523592, \t Eval: 0.530458\n","Epoch 160, train loss 5.211101\n","Epoch 170, train loss 5.281476\n","Epoch 180, train loss 5.064434\n","Epoch 190, train loss 4.932811\n","Epoch 200, train loss 4.88676\n","Epoch 200 took 7.74 seconds.\n","mask_loss       Train: 0.511022, \t Eval: 0.512496\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.351794, \t Eval: 0.357778\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 210, train loss 4.674813\n","Epoch 220, train loss 4.794259\n","Epoch 230, train loss 4.733445\n","Epoch 240, train loss 4.605206\n","Epoch 250, train loss 4.3374\n","Epoch 250 took 7.75 seconds.\n","mask_loss       Train: 0.507936, \t Eval: 0.508060\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.345385, \t Eval: 0.361073\n","New best train loss, saving model.\n","Epoch 260, train loss 4.684251\n","Epoch 270, train loss 4.685821\n","Epoch 280, train loss 4.347463\n","Epoch 290, train loss 4.264325\n","Epoch 300, train loss 4.453944\n","Epoch 300 took 7.74 seconds.\n","mask_loss       Train: 0.508212, \t Eval: 0.509700\n","dice_loss       Train: 0.290266, \t Eval: 0.316126\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 310, train loss 4.479413\n","Epoch 320, train loss 4.536316\n","Epoch 330, train loss 4.228218\n","Epoch 340, train loss 4.28446\n","Epoch 350, train loss 4.062465\n","Epoch 350 took 7.76 seconds.\n","mask_loss       Train: 0.498802, \t Eval: 0.500135\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.340327, \t Eval: 0.348039\n","Epoch 360, train loss 4.361416\n","Epoch 370, train loss 4.287109\n","Epoch 380, train loss 4.232923\n","Epoch 390, train loss 4.28681\n","Epoch 400, train loss 4.092312\n","Epoch 400 took 7.79 seconds.\n","mask_loss       Train: 0.503985, \t Eval: 0.507879\n","dice_loss       Train: 0.310226, \t Eval: 0.328487\n","Epoch 410, train loss 3.849631\n","Epoch 420, train loss 4.031113\n","Epoch 430, train loss 4.118027\n","Epoch 440, train loss 4.046258\n","Epoch 450, train loss 4.054822\n","Epoch 450 took 7.78 seconds.\n","mask_loss       Train: 0.492609, \t Eval: 0.497633\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.280736, \t Eval: 0.292566\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch   460: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch   460: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch   460: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch 460, train loss 3.938877\n","Epoch 470, train loss 3.64594\n","Epoch 480, train loss 3.751229\n","Epoch 490, train loss 3.626568\n","Epoch 500, train loss 3.593581\n","Epoch 500 took 7.77 seconds.\n","mask_loss       Train: 0.497662, \t Eval: 0.501045\n","dice_loss       Train: 0.265349, \t Eval: 0.288614\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 510, train loss 3.689531\n","Epoch 520, train loss 3.619796\n","Epoch 530, train loss 3.644262\n","Epoch 540, train loss 3.543621\n","Epoch 550, train loss 3.546166\n","Epoch 550 took 7.77 seconds.\n","mask_loss       Train: 0.495924, \t Eval: 0.499951\n","dice_loss       Train: 0.259928, \t Eval: 0.275174\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 560, train loss 3.50062\n","Epoch 570, train loss 3.543478\n","Epoch 580, train loss 3.621911\n","Epoch 590, train loss 3.487396\n","Epoch 600, train loss 3.619706\n","Epoch 600 took 7.76 seconds.\n","mask_loss       Train: 0.496815, \t Eval: 0.499898\n","dice_loss       Train: 0.268818, \t Eval: 0.285026\n","Epoch 610, train loss 3.549254\n","Epoch 620, train loss 3.466196\n","Epoch   628: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch   628: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch   628: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch 630, train loss 3.375558\n","Epoch 640, train loss 3.455056\n","Epoch 650, train loss 3.404982\n","Epoch 650 took 7.77 seconds.\n","mask_loss       Train: 0.497830, \t Eval: 0.501590\n","dice_loss       Train: 0.257971, \t Eval: 0.276399\n","New best train loss, saving model.\n","Epoch 660, train loss 3.410338\n","Epoch 670, train loss 3.323593\n","Epoch 680, train loss 3.400702\n","Epoch 690, train loss 3.331773\n","Epoch 700, train loss 3.251246\n","Epoch 700 took 7.75 seconds.\n","mask_loss       Train: 0.493986, \t Eval: 0.497858\n","dice_loss       Train: 0.288104, \t Eval: 0.293412\n","Epoch 710, train loss 3.401202\n","Epoch 720, train loss 3.315002\n","Epoch 730, train loss 3.364705\n","Epoch 740, train loss 3.317551\n","Epoch 750, train loss 3.303144\n","Epoch 750 took 7.75 seconds.\n","mask_loss       Train: 0.494661, \t Eval: 0.499824\n","dice_loss       Train: 0.250363, \t Eval: 0.261689\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 760, train loss 3.292827\n","Epoch 770, train loss 3.290748\n","Epoch 780, train loss 3.303255\n","Epoch 790, train loss 3.299596\n","Epoch 800, train loss 3.282114\n","Epoch 800 took 7.77 seconds.\n","mask_loss       Train: 0.498478, \t Eval: 0.503432\n","dice_loss       Train: 0.258723, \t Eval: 0.271247\n","Epoch 810, train loss 3.356372\n","Epoch 820, train loss 3.225623\n","Epoch   828: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch   828: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch   828: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch 830, train loss 3.213859\n","Epoch 840, train loss 3.226962\n","Epoch 850, train loss 3.209443\n","Epoch 850 took 7.75 seconds.\n","mask_loss       Train: 0.497152, \t Eval: 0.501958\n","dice_loss       Train: 0.254143, \t Eval: 0.265473\n","Epoch 860, train loss 3.236903\n","Epoch 870, train loss 3.297096\n","Epoch 880, train loss 3.143222\n","Epoch 890, train loss 3.254637\n","Epoch 900, train loss 3.302439\n","Epoch 900 took 7.78 seconds.\n","mask_loss       Train: 0.496168, \t Eval: 0.500710\n","dice_loss       Train: 0.226754, \t Eval: 0.249715\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 910, train loss 3.156974\n","Epoch 920, train loss 3.161355\n","Epoch 930, train loss 3.206708\n","Epoch 940, train loss 3.199542\n","Epoch 950, train loss 3.186522\n","Epoch 950 took 7.77 seconds.\n","mask_loss       Train: 0.496382, \t Eval: 0.501712\n","dice_loss       Train: 0.241908, \t Eval: 0.254734\n","Epoch   954: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch   954: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch   954: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 960, train loss 3.269451\n","Epoch 970, train loss 3.087187\n","Epoch 980, train loss 3.157888\n","Epoch 990, train loss 3.113979\n","Epoch 1000, train loss 3.176368\n","Epoch 1000 took 7.75 seconds.\n","mask_loss       Train: 0.495431, \t Eval: 0.500420\n","dice_loss       Train: 0.233324, \t Eval: 0.251442\n","Epoch 1010, train loss 3.170159\n","Epoch 1020, train loss 3.117275\n","Epoch 1030, train loss 3.097353\n","Epoch 1040, train loss 3.221412\n","Epoch 1050, train loss 3.20361\n","Epoch 1050 took 7.75 seconds.\n","mask_loss       Train: 0.493864, \t Eval: 0.499673\n","dice_loss       Train: 0.243213, \t Eval: 0.258354\n","Epoch 1060, train loss 3.152914\n","Epoch 1070, train loss 3.186637\n","Epoch 1080, train loss 3.143892\n","Epoch 1090, train loss 3.105334\n","Epoch 1100, train loss 3.221561\n","Epoch 1100 took 7.74 seconds.\n","mask_loss       Train: 0.494701, \t Eval: 0.499846\n","dice_loss       Train: 0.239300, \t Eval: 0.256288\n","WARNING: ARGS class initialized.\n","Starting training segmentation_sdf_(100.0, 100.0, 10.0, 10.0)_omega_0_100__run_0.\n","{'device': 'GPU', 'print_models': False, 'save_models': True, 'name': 'segmentation_sdf_(100.0, 100.0, 10.0, 10.0)_omega_0_100__run_0', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'seed': 34, 'n_coords_sample': 5000, 'norm_min_max': [0, 1], 'rotate': True, 'translate': True, 'crop': True, 'stretch': True, 'flip': False, 'translate_max_pixels': 20, 'stretch_factor': 1.2, 'pcmra_epochs': 5000, 'mask_epochs': 5000, 'batch_size': 24, 'eval_every': 50, 'shuffle': True, 'min_lr': 1e-05, 'patience': 50, 'cnn_setup': 'golden', 'mapping_setup': 'golden', 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 100, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'segmentation': 'sdf', 'cnn_lr': 0.0001, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'pcmra_siren_lr': 0.0001, 'cnn_wd': 0, 'siren_wd': 0, 'pcmra_siren_wd': 0, 'lambda_sdf': 100.0, 'lambda_inter': 10.0, 'lambda_normal': 10.0, 'lambda_grad': 10.0, 'sdf_split': 0.5}\n","saved_runs/pi_gan_2021_08_26_23_06_45_segmentation_sdf_(100.0, 100.0, 10.0, 10.0)_omega_0_100__run_0\n","----------------------------------\n","Using device for training: cuda\n","----------------------------------\n","Train subjects: 82\n","Train batch: ('104', '20', '116', '21', '68')\n","Val subjects: 32\n","Val batch: ('10', '121', '11', '129', '131')\n","Test subjects: 30\n","Test batch: ('133', '134', '15', '142', '138')\n","CNN\n","MAPPING\n","SIREN\n","PCMRA_MAPPING\n","PCMRA_SIREN\n","Epoch 0, train loss 16.549196\n","Epoch 0 took 7.6 seconds.\n","mask_loss       Train: 0.677087, \t Eval: 0.677174\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 1.000000, \t Eval: 1.000000\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 10, train loss 15.019523\n","Epoch 20, train loss 14.711785\n","Epoch 30, train loss 13.529269\n","Epoch 40, train loss 11.224828\n","Epoch 50, train loss 10.947705\n","Epoch 50 took 7.65 seconds.\n","mask_loss       Train: 0.531436, \t Eval: 0.526252\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.673100, \t Eval: 0.661465\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 60, train loss 9.862454\n","Epoch 70, train loss 9.613217\n","Epoch 80, train loss 9.760414\n","Epoch 90, train loss 9.096755\n","Epoch 100, train loss 8.921592\n","Epoch 100 took 7.66 seconds.\n","mask_loss       Train: 0.509487, \t Eval: 0.504259\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.605179, \t Eval: 0.604802\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 110, train loss 8.66683\n","Epoch 120, train loss 9.21987\n","Epoch 130, train loss 8.76805\n","Epoch 140, train loss 8.360599\n","Epoch 150, train loss 8.508557\n","Epoch 150 took 7.67 seconds.\n","mask_loss       Train: 0.501129, \t Eval: 0.502985\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.539858, \t Eval: 0.538862\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 160, train loss 8.082191\n","Epoch 170, train loss 7.900159\n","Epoch 180, train loss 7.862709\n","Epoch 190, train loss 7.746496\n","Epoch 200, train loss 7.430923\n","Epoch 200 took 7.67 seconds.\n","mask_loss       Train: 0.496965, \t Eval: 0.494846\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.497266, \t Eval: 0.512942\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 210, train loss 7.525792\n","Epoch 220, train loss 7.610246\n","Epoch 230, train loss 7.252467\n","Epoch 240, train loss 6.83095\n","Epoch 250, train loss 6.74122\n","Epoch 250 took 7.67 seconds.\n","mask_loss       Train: 0.480846, \t Eval: 0.479305\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.470576, \t Eval: 0.487187\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 260, train loss 5.834387\n","Epoch 270, train loss 5.903978\n","Epoch 280, train loss 5.490798\n","Epoch 290, train loss 5.49788\n","Epoch 300, train loss 5.178286\n","Epoch 300 took 7.67 seconds.\n","mask_loss       Train: 0.502960, \t Eval: 0.504745\n","dice_loss       Train: 0.421465, \t Eval: 0.410421\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 310, train loss 5.12361\n","Epoch 320, train loss 5.064461\n","Epoch 330, train loss 5.057062\n","Epoch 340, train loss 4.958586\n","Epoch 350, train loss 4.690847\n","Epoch 350 took 7.66 seconds.\n","mask_loss       Train: 0.489462, \t Eval: 0.492197\n","dice_loss       Train: 0.346247, \t Eval: 0.346439\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 360, train loss 4.606946\n","Epoch 370, train loss 4.605924\n","Epoch 380, train loss 4.717508\n","Epoch 390, train loss 4.309972\n","Epoch 400, train loss 4.630297\n","Epoch 400 took 7.66 seconds.\n","mask_loss       Train: 0.486288, \t Eval: 0.487034\n","dice_loss       Train: 0.425414, \t Eval: 0.393671\n","Epoch 410, train loss 4.412633\n","Epoch 420, train loss 4.85864\n","Epoch 430, train loss 4.642081\n","Epoch 440, train loss 4.29435\n","Epoch 450, train loss 4.280455\n","Epoch 450 took 7.68 seconds.\n","mask_loss       Train: 0.488061, \t Eval: 0.490589\n","dice_loss       Train: 0.279367, \t Eval: 0.297777\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 460, train loss 4.416261\n","Epoch 470, train loss 4.308822\n","Epoch 480, train loss 4.420187\n","Epoch 490, train loss 4.33611\n","Epoch 500, train loss 4.199525\n","Epoch 500 took 7.67 seconds.\n","mask_loss       Train: 0.491893, \t Eval: 0.493977\n","dice_loss       Train: 0.303610, \t Eval: 0.306467\n","Epoch 510, train loss 4.184196\n","Epoch 520, train loss 4.224618\n","Epoch 530, train loss 4.12572\n","Epoch 540, train loss 4.061121\n","Epoch 550, train loss 4.145757\n","Epoch 550 took 7.69 seconds.\n","mask_loss       Train: 0.490500, \t Eval: 0.493861\n","dice_loss       Train: 0.290078, \t Eval: 0.302814\n","Epoch 560, train loss 4.092801\n","Epoch 570, train loss 3.948131\n","Epoch 580, train loss 4.218154\n","Epoch 590, train loss 4.028094\n","Epoch   597: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch   597: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch   597: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch 600, train loss 3.784246\n","Epoch 600 took 7.67 seconds.\n","mask_loss       Train: 0.488998, \t Eval: 0.490691\n","dice_loss       Train: 0.272100, \t Eval: 0.282745\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 610, train loss 3.803411\n","Epoch 620, train loss 3.821954\n","Epoch 630, train loss 3.648653\n","Epoch 640, train loss 3.571201\n","Epoch 650, train loss 3.612675\n","Epoch 650 took 7.7 seconds.\n","mask_loss       Train: 0.489361, \t Eval: 0.490532\n","dice_loss       Train: 0.258223, \t Eval: 0.273536\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 660, train loss 3.811823\n","Epoch 670, train loss 3.750632\n","Epoch 680, train loss 3.66562\n","Epoch 690, train loss 3.528173\n","Epoch 700, train loss 3.642504\n","Epoch 700 took 7.69 seconds.\n","mask_loss       Train: 0.488317, \t Eval: 0.490791\n","dice_loss       Train: 0.273282, \t Eval: 0.289651\n","Epoch 710, train loss 3.647191\n","Epoch 720, train loss 3.598573\n","Epoch 730, train loss 3.649829\n","Epoch 740, train loss 3.532649\n","Epoch 750, train loss 3.55996\n","Epoch 750 took 7.7 seconds.\n","mask_loss       Train: 0.489195, \t Eval: 0.490438\n","dice_loss       Train: 0.271041, \t Eval: 0.282861\n","Epoch 760, train loss 3.47179\n","Epoch 770, train loss 3.469118\n","Epoch 780, train loss 3.549735\n","Epoch 790, train loss 3.590302\n","Epoch 800, train loss 3.613836\n","Epoch 800 took 7.7 seconds.\n","mask_loss       Train: 0.491332, \t Eval: 0.494502\n","dice_loss       Train: 0.239742, \t Eval: 0.258600\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch   808: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch   808: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch   808: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch 810, train loss 3.473216\n","Epoch 820, train loss 3.399201\n","Epoch 830, train loss 3.462312\n","Epoch 840, train loss 3.456253\n","Epoch 850, train loss 3.373598\n","Epoch 850 took 7.67 seconds.\n","mask_loss       Train: 0.488142, \t Eval: 0.490735\n","dice_loss       Train: 0.258847, \t Eval: 0.265882\n","Epoch 860, train loss 3.347379\n","Epoch 870, train loss 3.328401\n","Epoch 880, train loss 3.330885\n","Epoch 890, train loss 3.385415\n","Epoch 900, train loss 3.391847\n","Epoch 900 took 7.7 seconds.\n","mask_loss       Train: 0.488235, \t Eval: 0.490810\n","dice_loss       Train: 0.253876, \t Eval: 0.270163\n","Epoch 910, train loss 3.3472\n","Epoch 920, train loss 3.270406\n","Epoch 930, train loss 3.337523\n","Epoch 940, train loss 3.255141\n","Epoch 950, train loss 3.288967\n","Epoch 950 took 7.68 seconds.\n","mask_loss       Train: 0.487791, \t Eval: 0.490757\n","dice_loss       Train: 0.255077, \t Eval: 0.270266\n","Epoch 960, train loss 3.279624\n","Epoch 970, train loss 3.298656\n","Epoch 980, train loss 3.367322\n","Epoch 990, train loss 3.297046\n","Epoch 1000, train loss 3.33273\n","Epoch 1000 took 7.67 seconds.\n","mask_loss       Train: 0.490679, \t Eval: 0.493585\n","dice_loss       Train: 0.233934, \t Eval: 0.256534\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch  1006: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch  1006: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch  1006: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch 1010, train loss 3.249393\n","Epoch 1020, train loss 3.180379\n","Epoch 1030, train loss 3.19444\n","Epoch 1040, train loss 3.167907\n","Epoch 1050, train loss 3.212899\n","Epoch 1050 took 7.69 seconds.\n","mask_loss       Train: 0.490528, \t Eval: 0.493558\n","dice_loss       Train: 0.237839, \t Eval: 0.255775\n","New best eval  loss, saving model.\n","Epoch 1060, train loss 3.218426\n","Epoch 1070, train loss 3.191365\n","Epoch 1080, train loss 3.131337\n","Epoch 1090, train loss 3.289225\n","Epoch 1100, train loss 3.199331\n","Epoch 1100 took 7.7 seconds.\n","mask_loss       Train: 0.490871, \t Eval: 0.493653\n","dice_loss       Train: 0.236262, \t Eval: 0.256044\n","Epoch 1110, train loss 3.177825\n","Epoch 1120, train loss 3.227196\n","Epoch 1130, train loss 3.129902\n","Epoch 1140, train loss 3.185665\n","Epoch 1150, train loss 3.135473\n","Epoch 1150 took 7.69 seconds.\n","mask_loss       Train: 0.489979, \t Eval: 0.493255\n","dice_loss       Train: 0.242919, \t Eval: 0.262484\n","Epoch 1160, train loss 3.118342\n","Epoch 1170, train loss 3.148188\n","Epoch  1174: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch  1174: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch  1174: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 1180, train loss 3.180354\n","Epoch 1190, train loss 3.251903\n","Epoch 1200, train loss 3.184231\n","Epoch 1200 took 7.7 seconds.\n","mask_loss       Train: 0.489708, \t Eval: 0.493393\n","dice_loss       Train: 0.232155, \t Eval: 0.251038\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 1210, train loss 3.043099\n","Epoch 1220, train loss 3.193425\n","Epoch 1230, train loss 3.167469\n","Epoch 1240, train loss 3.073468\n","Epoch 1250, train loss 3.086008\n","Epoch 1250 took 7.67 seconds.\n","mask_loss       Train: 0.488271, \t Eval: 0.491677\n","dice_loss       Train: 0.249782, \t Eval: 0.263178\n","Epoch 1260, train loss 3.239299\n","Epoch 1270, train loss 3.120036\n","Epoch 1280, train loss 3.103855\n","Epoch 1290, train loss 3.088177\n","Epoch 1300, train loss 3.151453\n","Epoch 1300 took 7.67 seconds.\n","mask_loss       Train: 0.492047, \t Eval: 0.496155\n","dice_loss       Train: 0.216832, \t Eval: 0.243074\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 1310, train loss 3.139388\n","Epoch 1320, train loss 3.154921\n","Epoch 1330, train loss 3.110428\n","Epoch 1340, train loss 3.197321\n","Epoch 1350, train loss 3.106704\n","Epoch 1350 took 7.68 seconds.\n","mask_loss       Train: 0.489295, \t Eval: 0.492779\n","dice_loss       Train: 0.232763, \t Eval: 0.250720\n","Epoch 1360, train loss 3.185177\n","Epoch 1370, train loss 3.157275\n","Epoch 1380, train loss 3.210879\n","Epoch 1390, train loss 3.11647\n","Epoch 1400, train loss 3.159577\n","Epoch 1400 took 7.69 seconds.\n","mask_loss       Train: 0.490117, \t Eval: 0.494351\n","dice_loss       Train: 0.231681, \t Eval: 0.251616\n","Epoch 1410, train loss 3.220966\n","Epoch 1420, train loss 3.040678\n","Epoch 1430, train loss 3.100149\n","Epoch 1440, train loss 3.081864\n","Epoch 1450, train loss 3.153937\n","Epoch 1450 took 7.68 seconds.\n","mask_loss       Train: 0.488052, \t Eval: 0.492203\n","dice_loss       Train: 0.247258, \t Eval: 0.263899\n","Epoch 1460, train loss 3.115523\n","Epoch 1470, train loss 3.124613\n","Epoch 1480, train loss 3.108481\n","Epoch 1490, train loss 3.06501\n","Epoch 1500, train loss 3.074623\n","Epoch 1500 took 7.67 seconds.\n","mask_loss       Train: 0.489295, \t Eval: 0.492957\n","dice_loss       Train: 0.221588, \t Eval: 0.247161\n","WARNING: ARGS class initialized.\n","Starting training segmentation_sdf_(300.0, 100.0, 10.0, 5.0)_omega_0_100__run_0.\n","{'device': 'GPU', 'print_models': False, 'save_models': True, 'name': 'segmentation_sdf_(300.0, 100.0, 10.0, 5.0)_omega_0_100__run_0', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'seed': 34, 'n_coords_sample': 5000, 'norm_min_max': [0, 1], 'rotate': True, 'translate': True, 'crop': True, 'stretch': True, 'flip': False, 'translate_max_pixels': 20, 'stretch_factor': 1.2, 'pcmra_epochs': 5000, 'mask_epochs': 5000, 'batch_size': 24, 'eval_every': 50, 'shuffle': True, 'min_lr': 1e-05, 'patience': 50, 'cnn_setup': 'golden', 'mapping_setup': 'golden', 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 100, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'segmentation': 'sdf', 'cnn_lr': 0.0001, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'pcmra_siren_lr': 0.0001, 'cnn_wd': 0, 'siren_wd': 0, 'pcmra_siren_wd': 0, 'lambda_sdf': 300.0, 'lambda_inter': 10.0, 'lambda_normal': 10.0, 'lambda_grad': 5.0, 'sdf_split': 0.5}\n","saved_runs/pi_gan_2021_08_27_02_30_46_segmentation_sdf_(300.0, 100.0, 10.0, 5.0)_omega_0_100__run_0\n","----------------------------------\n","Using device for training: cuda\n","----------------------------------\n","Train subjects: 82\n","Train batch: ('3', '117', '107', '27', '24')\n","Val subjects: 32\n","Val batch: ('10', '121', '11', '129', '131')\n","Test subjects: 30\n","Test batch: ('133', '134', '15', '142', '138')\n","CNN\n","MAPPING\n","SIREN\n","PCMRA_MAPPING\n","PCMRA_SIREN\n","Epoch 0, train loss 14.003654\n","Epoch 0 took 7.7 seconds.\n","mask_loss       Train: 0.701018, \t Eval: 0.701235\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.943035, \t Eval: 0.943931\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","WARNING: ARGS class initialized.\n","Starting training segmentation_sdf_(100.0, 100.0, 10.0, 5.0)_omega_0_100__run_1.\n","{'device': 'GPU', 'print_models': False, 'save_models': True, 'name': 'segmentation_sdf_(100.0, 100.0, 10.0, 5.0)_omega_0_100__run_1', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'seed': 34, 'n_coords_sample': 5000, 'norm_min_max': [0, 1], 'rotate': True, 'translate': True, 'crop': True, 'stretch': True, 'flip': False, 'translate_max_pixels': 20, 'stretch_factor': 1.2, 'pcmra_epochs': 5000, 'mask_epochs': 5000, 'batch_size': 24, 'eval_every': 50, 'shuffle': True, 'min_lr': 1e-05, 'patience': 50, 'cnn_setup': 'golden', 'mapping_setup': 'golden', 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 100, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'segmentation': 'sdf', 'cnn_lr': 0.0001, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'pcmra_siren_lr': 0.0001, 'cnn_wd': 0, 'siren_wd': 0, 'pcmra_siren_wd': 0, 'lambda_sdf': 100.0, 'lambda_inter': 10.0, 'lambda_normal': 10.0, 'lambda_grad': 5.0, 'sdf_split': 0.5}\n","saved_runs/pi_gan_2021_08_27_02_31_56_segmentation_sdf_(100.0, 100.0, 10.0, 5.0)_omega_0_100__run_1\n","----------------------------------\n","Using device for training: cuda\n","----------------------------------\n","Train subjects: 82\n","Train batch: ('118', '46', '91', '95', '77')\n","Val subjects: 32\n","Val batch: ('10', '121', '11', '129', '131')\n","Test subjects: 30\n","Test batch: ('133', '134', '15', '142', '138')\n","CNN\n","MAPPING\n","SIREN\n","PCMRA_MAPPING\n","PCMRA_SIREN\n","Epoch 0, train loss 11.497258\n","Epoch 0 took 7.81 seconds.\n","mask_loss       Train: 0.709706, \t Eval: 0.709744\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.943035, \t Eval: 0.943931\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","WARNING: ARGS class initialized.\n","Starting training segmentation_sdf_(100.0, 100.0, 30.0, 5.0)_omega_0_100__run_1.\n","{'device': 'GPU', 'print_models': False, 'save_models': True, 'name': 'segmentation_sdf_(100.0, 100.0, 30.0, 5.0)_omega_0_100__run_1', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'seed': 34, 'n_coords_sample': 5000, 'norm_min_max': [0, 1], 'rotate': True, 'translate': True, 'crop': True, 'stretch': True, 'flip': False, 'translate_max_pixels': 20, 'stretch_factor': 1.2, 'pcmra_epochs': 5000, 'mask_epochs': 5000, 'batch_size': 24, 'eval_every': 50, 'shuffle': True, 'min_lr': 1e-05, 'patience': 50, 'cnn_setup': 'golden', 'mapping_setup': 'golden', 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 100, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'segmentation': 'sdf', 'cnn_lr': 0.0001, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'pcmra_siren_lr': 0.0001, 'cnn_wd': 0, 'siren_wd': 0, 'pcmra_siren_wd': 0, 'lambda_sdf': 100.0, 'lambda_inter': 30.0, 'lambda_normal': 10.0, 'lambda_grad': 5.0, 'sdf_split': 0.5}\n","saved_runs/pi_gan_2021_08_27_02_33_08_segmentation_sdf_(100.0, 100.0, 30.0, 5.0)_omega_0_100__run_1\n","----------------------------------\n","Using device for training: cuda\n","----------------------------------\n","Train subjects: 82\n","Train batch: ('28', '42', '75', '46', '90')\n","Val subjects: 32\n","Val batch: ('10', '121', '11', '129', '131')\n","Test subjects: 30\n","Test batch: ('133', '134', '15', '142', '138')\n","CNN\n","MAPPING\n","SIREN\n","PCMRA_MAPPING\n","PCMRA_SIREN\n","Epoch 0, train loss 11.69559\n","Epoch 0 took 7.78 seconds.\n","mask_loss       Train: 0.707866, \t Eval: 0.707852\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.943035, \t Eval: 0.943931\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","WARNING: ARGS class initialized.\n","Starting training segmentation_sdf_(100.0, 100.0, 10.0, 10.0)_omega_0_100__run_1.\n","{'device': 'GPU', 'print_models': False, 'save_models': True, 'name': 'segmentation_sdf_(100.0, 100.0, 10.0, 10.0)_omega_0_100__run_1', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'seed': 34, 'n_coords_sample': 5000, 'norm_min_max': [0, 1], 'rotate': True, 'translate': True, 'crop': True, 'stretch': True, 'flip': False, 'translate_max_pixels': 20, 'stretch_factor': 1.2, 'pcmra_epochs': 5000, 'mask_epochs': 5000, 'batch_size': 24, 'eval_every': 50, 'shuffle': True, 'min_lr': 1e-05, 'patience': 50, 'cnn_setup': 'golden', 'mapping_setup': 'golden', 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 100, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'segmentation': 'sdf', 'cnn_lr': 0.0001, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'pcmra_siren_lr': 0.0001, 'cnn_wd': 0, 'siren_wd': 0, 'pcmra_siren_wd': 0, 'lambda_sdf': 100.0, 'lambda_inter': 10.0, 'lambda_normal': 10.0, 'lambda_grad': 10.0, 'sdf_split': 0.5}\n","saved_runs/pi_gan_2021_08_27_02_34_25_segmentation_sdf_(100.0, 100.0, 10.0, 10.0)_omega_0_100__run_1\n","----------------------------------\n","Using device for training: cuda\n","----------------------------------\n","Train subjects: 82\n","Train batch: ('99', '95', '18', '43', '71')\n","Val subjects: 32\n","Val batch: ('10', '121', '11', '129', '131')\n","Test subjects: 30\n","Test batch: ('133', '134', '15', '142', '138')\n","CNN\n","MAPPING\n","SIREN\n","PCMRA_MAPPING\n","PCMRA_SIREN\n","Epoch 0, train loss 16.148507\n","Epoch 0 took 7.72 seconds.\n","mask_loss       Train: 0.680813, \t Eval: 0.680746\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 1.000000, \t Eval: 1.000000\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 10, train loss 14.721174\n","Epoch 20, train loss 11.274163\n","Epoch 30, train loss 10.758888\n","Epoch 40, train loss 10.028641\n","Epoch 50, train loss 10.068217\n","Epoch 50 took 7.7 seconds.\n","mask_loss       Train: 0.666397, \t Eval: 0.662370\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.815218, \t Eval: 0.814525\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 60, train loss 9.456701\n","Epoch 70, train loss 8.538601\n","Epoch 80, train loss 9.788233\n","Epoch 90, train loss 8.555551\n","Epoch 100, train loss 8.432441\n","Epoch 100 took 7.68 seconds.\n","mask_loss       Train: 0.671876, \t Eval: 0.670627\n","dice_loss       Train: 0.743601, \t Eval: 0.746632\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 110, train loss 8.234222\n","Epoch 120, train loss 8.38287\n","Epoch 130, train loss 8.238997\n","Epoch 140, train loss 9.410071\n","Epoch 150, train loss 8.528613\n","Epoch 150 took 7.66 seconds.\n","mask_loss       Train: 0.674369, \t Eval: 0.673272\n","dice_loss       Train: 0.711447, \t Eval: 0.704164\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 160, train loss 8.291776\n","Epoch 170, train loss 13.134379\n","Epoch 180, train loss 8.904099\n","Epoch   189: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch   189: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch   189: reducing learning rate of group 0 to 5.0000e-05.\n","Epoch 190, train loss 8.388193\n","Epoch 200, train loss 8.305888\n","Epoch 200 took 7.67 seconds.\n","mask_loss       Train: 0.677991, \t Eval: 0.678308\n","dice_loss       Train: 0.925952, \t Eval: 0.970862\n","Epoch 210, train loss 8.35066\n","Epoch 220, train loss 8.383692\n","Epoch 230, train loss 8.244354\n","Epoch   240: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch   240: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch   240: reducing learning rate of group 0 to 2.5000e-05.\n","Epoch 240, train loss 8.146156\n","Epoch 250, train loss 8.114861\n","Epoch 250 took 7.67 seconds.\n","mask_loss       Train: 0.671420, \t Eval: 0.672032\n","dice_loss       Train: 0.709461, \t Eval: 0.709538\n","New best train loss, saving model.\n","Epoch 260, train loss 8.063941\n","Epoch 270, train loss 8.123931\n","Epoch 280, train loss 8.119228\n","Epoch 290, train loss 8.063305\n","Epoch 300, train loss 8.058058\n","Epoch 300 took 7.7 seconds.\n","mask_loss       Train: 0.671866, \t Eval: 0.672205\n","dice_loss       Train: 0.705940, \t Eval: 0.707227\n","New best train loss, saving model.\n","Epoch 310, train loss 8.036205\n","Epoch 320, train loss 8.033527\n","Epoch 330, train loss 7.971444\n","Epoch 340, train loss 7.956497\n","Epoch 350, train loss 7.911183\n","Epoch 350 took 7.76 seconds.\n","mask_loss       Train: 0.671336, \t Eval: 0.671710\n","dice_loss       Train: 0.676163, \t Eval: 0.670309\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 360, train loss 7.895391\n","Epoch 370, train loss 7.780961\n","Epoch 380, train loss 7.760724\n","Epoch 390, train loss 7.714819\n","Epoch 400, train loss 7.78024\n","Epoch 400 took 7.76 seconds.\n","mask_loss       Train: 0.669530, \t Eval: 0.669571\n","dice_loss       Train: 0.671400, \t Eval: 0.676766\n","New best train loss, saving model.\n","Epoch 410, train loss 7.843346\n","Epoch 420, train loss 7.770294\n","Epoch 430, train loss 7.812026\n","Epoch 440, train loss 7.763537\n","Epoch 450, train loss 7.723829\n","Epoch 450 took 7.76 seconds.\n","mask_loss       Train: 0.666672, \t Eval: 0.667280\n","dice_loss       Train: 0.681953, \t Eval: 0.676900\n","Epoch 460, train loss 7.894843\n","Epoch 470, train loss 7.823426\n","Epoch 480, train loss 7.753627\n","Epoch 490, train loss 7.730899\n","Epoch   494: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch   494: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch   494: reducing learning rate of group 0 to 1.2500e-05.\n","Epoch 500, train loss 7.726593\n","Epoch 500 took 7.71 seconds.\n","mask_loss       Train: 0.666778, \t Eval: 0.667011\n","dice_loss       Train: 0.685440, \t Eval: 0.684447\n","Epoch 510, train loss 7.678923\n","Epoch 520, train loss 7.710742\n","Epoch 530, train loss 7.699777\n","Epoch 540, train loss 7.744588\n","Epoch 550, train loss 7.79188\n","Epoch 550 took 7.75 seconds.\n","mask_loss       Train: 0.665569, \t Eval: 0.665927\n","New best train loss, saving model.\n","dice_loss       Train: 0.668252, \t Eval: 0.667416\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 560, train loss 7.682583\n","Epoch 570, train loss 7.674805\n","Epoch 580, train loss 7.732255\n","Epoch 590, train loss 7.660335\n","Epoch 600, train loss 7.696029\n","Epoch 600 took 7.74 seconds.\n","mask_loss       Train: 0.666989, \t Eval: 0.667358\n","dice_loss       Train: 0.666688, \t Eval: 0.670397\n","New best train loss, saving model.\n","Epoch 610, train loss 7.76665\n","Epoch 620, train loss 7.604152\n","Epoch 630, train loss 7.680268\n","Epoch 640, train loss 7.626132\n","Epoch 650, train loss 7.644927\n","Epoch 650 took 7.76 seconds.\n","mask_loss       Train: 0.666388, \t Eval: 0.666408\n","dice_loss       Train: 0.670635, \t Eval: 0.671380\n","Epoch 660, train loss 7.620462\n","Epoch 670, train loss 7.63729\n","Epoch 680, train loss 7.614355\n","Epoch   690: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch   690: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch   690: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 690, train loss 7.650476\n","Epoch 700, train loss 7.655697\n","Epoch 700 took 7.76 seconds.\n","mask_loss       Train: 0.667175, \t Eval: 0.667316\n","dice_loss       Train: 0.649244, \t Eval: 0.652348\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 710, train loss 7.57076\n","Epoch 720, train loss 7.578526\n","Epoch 730, train loss 7.607074\n","Epoch 740, train loss 7.634981\n","Epoch 750, train loss 7.588715\n","Epoch 750 took 7.75 seconds.\n","mask_loss       Train: 0.666552, \t Eval: 0.666761\n","dice_loss       Train: 0.656419, \t Eval: 0.664502\n","Epoch 760, train loss 7.595458\n","Epoch 770, train loss 7.628109\n","Epoch 780, train loss 7.621967\n","Epoch 790, train loss 7.593093\n","Epoch 800, train loss 7.578712\n","Epoch 800 took 7.75 seconds.\n","mask_loss       Train: 0.666562, \t Eval: 0.666883\n","dice_loss       Train: 0.665861, \t Eval: 0.673944\n","Epoch 810, train loss 7.570962\n","Epoch 820, train loss 7.58466\n","Epoch 830, train loss 7.594188\n","Epoch 840, train loss 7.597224\n","Epoch 850, train loss 7.572139\n","Epoch 850 took 7.74 seconds.\n","mask_loss       Train: 0.665633, \t Eval: 0.665845\n","dice_loss       Train: 0.659761, \t Eval: 0.668101\n","Epoch 860, train loss 7.558951\n","Epoch 870, train loss 7.581577\n","Epoch 880, train loss 7.57727\n","Epoch 890, train loss 7.492929\n","Epoch 900, train loss 7.517768\n","Epoch 900 took 7.74 seconds.\n","mask_loss       Train: 0.669154, \t Eval: 0.669549\n","dice_loss       Train: 0.640319, \t Eval: 0.645486\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 910, train loss 7.496666\n","Epoch 920, train loss 7.460665\n","Epoch 930, train loss 7.485403\n","Epoch 940, train loss 7.43369\n","Epoch 950, train loss 7.425604\n","Epoch 950 took 7.75 seconds.\n","mask_loss       Train: 0.667200, \t Eval: 0.667401\n","dice_loss       Train: 0.661477, \t Eval: 0.666683\n","Epoch 960, train loss 7.459348\n","Epoch 970, train loss 7.398897\n","Epoch 980, train loss 7.464169\n","Epoch 990, train loss 7.356954\n","Epoch 1000, train loss 7.437341\n","Epoch 1000 took 7.76 seconds.\n","mask_loss       Train: 0.665720, \t Eval: 0.665789\n","dice_loss       Train: 0.692557, \t Eval: 0.698966\n","Epoch 1010, train loss 7.431195\n","Epoch 1020, train loss 7.339573\n","Epoch 1030, train loss 7.429468\n","Epoch 1040, train loss 7.322888\n","Epoch 1050, train loss 7.413637\n","Epoch 1050 took 7.77 seconds.\n","mask_loss       Train: 0.667992, \t Eval: 0.668338\n","dice_loss       Train: 0.657379, \t Eval: 0.645812\n","Epoch 1060, train loss 7.353071\n","Epoch 1070, train loss 7.430829\n","Epoch 1080, train loss 7.380238\n","Epoch 1090, train loss 7.33268\n","Epoch 1100, train loss 7.305251\n","Epoch 1100 took 7.74 seconds.\n","mask_loss       Train: 0.667096, \t Eval: 0.667223\n","dice_loss       Train: 0.635792, \t Eval: 0.622803\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 1110, train loss 7.240196\n","Epoch 1120, train loss 7.243866\n","Epoch 1130, train loss 7.273364\n","Epoch 1140, train loss 7.258796\n","Epoch 1150, train loss 7.218017\n","Epoch 1150 took 7.71 seconds.\n","mask_loss       Train: 0.666717, \t Eval: 0.666814\n","dice_loss       Train: 0.625429, \t Eval: 0.631154\n","New best train loss, saving model.\n","Epoch 1160, train loss 7.236072\n","Epoch 1170, train loss 7.107727\n","Epoch 1180, train loss 7.089316\n","Epoch 1190, train loss 7.083014\n","Epoch 1200, train loss 7.126741\n","Epoch 1200 took 7.77 seconds.\n","mask_loss       Train: 0.667446, \t Eval: 0.667750\n","dice_loss       Train: 0.570072, \t Eval: 0.579699\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 1210, train loss 7.066718\n","Epoch 1220, train loss 7.222673\n","Epoch 1230, train loss 7.223045\n","Epoch 1240, train loss 7.100547\n","Epoch 1250, train loss 6.993562\n","Epoch 1250 took 7.78 seconds.\n","mask_loss       Train: 0.668838, \t Eval: 0.668942\n","dice_loss       Train: 0.519360, \t Eval: 0.535141\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 1260, train loss 7.006647\n","Epoch 1270, train loss 6.95042\n","Epoch 1280, train loss 6.975869\n","Epoch 1290, train loss 6.997211\n","Epoch 1300, train loss 6.898717\n","Epoch 1300 took 7.8 seconds.\n","mask_loss       Train: 0.668246, \t Eval: 0.668384\n","dice_loss       Train: 0.513227, \t Eval: 0.499196\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 1310, train loss 6.949442\n","Epoch 1320, train loss 6.979434\n","Epoch 1330, train loss 6.940446\n","Epoch 1340, train loss 6.887467\n","Epoch 1350, train loss 6.843932\n","Epoch 1350 took 7.78 seconds.\n","mask_loss       Train: 0.666661, \t Eval: 0.666850\n","dice_loss       Train: 0.516829, \t Eval: 0.510805\n","Epoch 1360, train loss 6.884603\n","Epoch 1370, train loss 6.819723\n","Epoch 1380, train loss 6.820086\n","Epoch 1390, train loss 6.814358\n","Epoch 1400, train loss 6.839213\n","Epoch 1400 took 7.77 seconds.\n","mask_loss       Train: 0.665267, \t Eval: 0.665678\n","New best train loss, saving model.\n","dice_loss       Train: 0.489601, \t Eval: 0.501814\n","New best train loss, saving model.\n","Epoch 1410, train loss 6.838205\n","Epoch 1420, train loss 6.777967\n","Epoch 1430, train loss 6.816799\n","Epoch 1440, train loss 6.804809\n","Epoch 1450, train loss 6.803613\n","Epoch 1450 took 7.79 seconds.\n","mask_loss       Train: 0.666050, \t Eval: 0.666282\n","dice_loss       Train: 0.490132, \t Eval: 0.480811\n","New best eval  loss, saving model.\n","Epoch 1460, train loss 6.631117\n","Epoch 1470, train loss 6.623912\n","Epoch 1480, train loss 6.648705\n","Epoch 1490, train loss 6.691513\n","Epoch 1500, train loss 6.568246\n","Epoch 1500 took 7.77 seconds.\n","mask_loss       Train: 0.665574, \t Eval: 0.665624\n","dice_loss       Train: 0.493446, \t Eval: 0.504071\n","Epoch 1510, train loss 6.534344\n","Epoch 1520, train loss 6.583865\n","Epoch 1530, train loss 6.601892\n","Epoch 1540, train loss 6.509848\n","Epoch 1550, train loss 6.458602\n","Epoch 1550 took 7.77 seconds.\n","mask_loss       Train: 0.662170, \t Eval: 0.662481\n","New best train loss, saving model.\n","dice_loss       Train: 0.471449, \t Eval: 0.483146\n","New best train loss, saving model.\n","Epoch 1560, train loss 6.42211\n","Epoch 1570, train loss 6.394989\n","Epoch 1580, train loss 6.454519\n","Epoch 1590, train loss 6.480491\n","Epoch 1600, train loss 6.370822\n","Epoch 1600 took 7.77 seconds.\n","mask_loss       Train: 0.663096, \t Eval: 0.663481\n","dice_loss       Train: 0.444513, \t Eval: 0.457245\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 1610, train loss 6.403367\n","Epoch 1620, train loss 6.370863\n","Epoch 1630, train loss 6.440017\n","Epoch 1640, train loss 6.340128\n","Epoch 1650, train loss 6.228216\n","Epoch 1650 took 7.8 seconds.\n","mask_loss       Train: 0.661778, \t Eval: 0.661847\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.472193, \t Eval: 0.475088\n","Epoch 1660, train loss 6.289448\n","Epoch 1670, train loss 6.257742\n","Epoch 1680, train loss 6.173266\n","Epoch 1690, train loss 6.176908\n","Epoch 1700, train loss 6.07798\n","Epoch 1700 took 7.78 seconds.\n","mask_loss       Train: 0.662308, \t Eval: 0.662752\n","dice_loss       Train: 0.444628, \t Eval: 0.457805\n","Epoch 1710, train loss 6.238118\n","Epoch 1720, train loss 6.128968\n","Epoch 1730, train loss 6.308871\n","Epoch 1740, train loss 6.232847\n","Epoch 1750, train loss 6.061314\n","Epoch 1750 took 7.77 seconds.\n","mask_loss       Train: 0.660933, \t Eval: 0.661328\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.443615, \t Eval: 0.452462\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 1760, train loss 6.136974\n","Epoch 1770, train loss 6.093583\n","Epoch 1780, train loss 6.23623\n","Epoch 1790, train loss 6.046819\n","Epoch 1800, train loss 5.888895\n","Epoch 1800 took 7.78 seconds.\n","mask_loss       Train: 0.661881, \t Eval: 0.662300\n","dice_loss       Train: 0.431211, \t Eval: 0.444090\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 1810, train loss 6.111523\n","Epoch 1820, train loss 6.025751\n","Epoch 1830, train loss 5.928725\n","Epoch 1840, train loss 5.909321\n","Epoch 1850, train loss 5.907778\n","Epoch 1850 took 7.78 seconds.\n","mask_loss       Train: 0.660970, \t Eval: 0.661410\n","dice_loss       Train: 0.401489, \t Eval: 0.409276\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 1860, train loss 5.937065\n","Epoch 1870, train loss 5.764641\n","Epoch 1880, train loss 5.878223\n","Epoch 1890, train loss 5.898389\n","Epoch 1900, train loss 5.708443\n","Epoch 1900 took 7.79 seconds.\n","mask_loss       Train: 0.660204, \t Eval: 0.660717\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.441410, \t Eval: 0.460654\n","Epoch 1910, train loss 5.684537\n","Epoch 1920, train loss 5.829767\n","Epoch 1930, train loss 5.896682\n","Epoch 1940, train loss 5.75447\n","Epoch 1950, train loss 5.697495\n","Epoch 1950 took 7.77 seconds.\n","mask_loss       Train: 0.660560, \t Eval: 0.660999\n","dice_loss       Train: 0.398453, \t Eval: 0.410914\n","New best train loss, saving model.\n","Epoch 1960, train loss 5.746934\n","Epoch 1970, train loss 5.671201\n","Epoch 1980, train loss 5.68928\n","Epoch 1990, train loss 5.51467\n","Epoch 2000, train loss 5.707457\n","Epoch 2000 took 7.79 seconds.\n","mask_loss       Train: 0.659396, \t Eval: 0.660216\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.407677, \t Eval: 0.425239\n","Epoch 2010, train loss 5.480604\n","Epoch 2020, train loss 5.736936\n","Epoch 2030, train loss 5.676158\n","Epoch 2040, train loss 5.598892\n","Epoch 2050, train loss 5.520023\n","Epoch 2050 took 7.78 seconds.\n","mask_loss       Train: 0.658495, \t Eval: 0.659191\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.414439, \t Eval: 0.411662\n","Epoch 2060, train loss 5.715159\n","Epoch 2070, train loss 5.591464\n","Epoch 2080, train loss 5.650249\n","Epoch 2090, train loss 5.299398\n","Epoch 2100, train loss 5.398604\n","Epoch 2100 took 7.78 seconds.\n","mask_loss       Train: 0.657719, \t Eval: 0.658629\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.397831, \t Eval: 0.413256\n","New best train loss, saving model.\n","Epoch 2110, train loss 5.427255\n","Epoch 2120, train loss 5.240895\n","Epoch 2130, train loss 5.449996\n","Epoch 2140, train loss 5.402424\n","Epoch 2150, train loss 5.355144\n","Epoch 2150 took 7.76 seconds.\n","mask_loss       Train: 0.659092, \t Eval: 0.659600\n","dice_loss       Train: 0.388460, \t Eval: 0.396088\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 2160, train loss 5.425104\n","Epoch 2170, train loss 5.233061\n","Epoch 2180, train loss 5.424906\n","Epoch 2190, train loss 5.355573\n","Epoch 2200, train loss 5.447034\n","Epoch 2200 took 7.77 seconds.\n","mask_loss       Train: 0.657653, \t Eval: 0.658301\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.361179, \t Eval: 0.377006\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 2210, train loss 5.316271\n","Epoch 2220, train loss 5.390179\n","Epoch 2230, train loss 5.096127\n","Epoch 2240, train loss 5.330216\n","Epoch 2250, train loss 5.348791\n","Epoch 2250 took 7.78 seconds.\n","mask_loss       Train: 0.659013, \t Eval: 0.659309\n","dice_loss       Train: 0.426656, \t Eval: 0.428195\n","Epoch 2260, train loss 5.117395\n","Epoch 2270, train loss 5.283998\n","Epoch 2280, train loss 5.333252\n","Epoch 2290, train loss 5.18917\n","Epoch 2300, train loss 4.963394\n","Epoch 2300 took 7.77 seconds.\n","mask_loss       Train: 0.659107, \t Eval: 0.659654\n","dice_loss       Train: 0.366512, \t Eval: 0.385793\n","Epoch 2310, train loss 4.976067\n","Epoch 2320, train loss 5.070525\n","Epoch 2330, train loss 5.071516\n","Epoch 2340, train loss 5.251141\n","Epoch 2350, train loss 5.075572\n","Epoch 2350 took 7.77 seconds.\n","mask_loss       Train: 0.658346, \t Eval: 0.658952\n","dice_loss       Train: 0.372778, \t Eval: 0.382839\n","Epoch 2360, train loss 5.060227\n","Epoch 2370, train loss 5.133676\n","Epoch 2380, train loss 5.088395\n","Epoch 2390, train loss 5.031599\n","Epoch 2400, train loss 5.11508\n","Epoch 2400 took 7.77 seconds.\n","mask_loss       Train: 0.655604, \t Eval: 0.656358\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 0.364735, \t Eval: 0.378188\n","WARNING: ARGS class initialized.\n","Starting training segmentation_sdf_(300.0, 100.0, 10.0, 5.0)_omega_0_100__run_1.\n","{'device': 'GPU', 'print_models': False, 'save_models': True, 'name': 'segmentation_sdf_(300.0, 100.0, 10.0, 5.0)_omega_0_100__run_1', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'seed': 34, 'n_coords_sample': 5000, 'norm_min_max': [0, 1], 'rotate': True, 'translate': True, 'crop': True, 'stretch': True, 'flip': False, 'translate_max_pixels': 20, 'stretch_factor': 1.2, 'pcmra_epochs': 5000, 'mask_epochs': 5000, 'batch_size': 24, 'eval_every': 50, 'shuffle': True, 'min_lr': 1e-05, 'patience': 50, 'cnn_setup': 'golden', 'mapping_setup': 'golden', 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 100, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'segmentation': 'sdf', 'cnn_lr': 0.0001, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'pcmra_siren_lr': 0.0001, 'cnn_wd': 0, 'siren_wd': 0, 'pcmra_siren_wd': 0, 'lambda_sdf': 300.0, 'lambda_inter': 10.0, 'lambda_normal': 10.0, 'lambda_grad': 5.0, 'sdf_split': 0.5}\n","saved_runs/pi_gan_2021_08_27_08_02_43_segmentation_sdf_(300.0, 100.0, 10.0, 5.0)_omega_0_100__run_1\n","----------------------------------\n","Using device for training: cuda\n","----------------------------------\n","Train subjects: 82\n","Train batch: ('50', '74', '26', '2', '17')\n","Val subjects: 32\n","Val batch: ('10', '121', '11', '129', '131')\n","Test subjects: 30\n","Test batch: ('133', '134', '15', '142', '138')\n","CNN\n","MAPPING\n","SIREN\n","PCMRA_MAPPING\n","PCMRA_SIREN\n","Epoch 0, train loss 13.908859\n","Epoch 0 took 7.69 seconds.\n","mask_loss       Train: 0.690313, \t Eval: 0.690245\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","dice_loss       Train: 1.000000, \t Eval: 1.000000\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 10, train loss 12.045913\n","Epoch 20, train loss 11.508231\n","Epoch 30, train loss 11.376338\n","Epoch 40, train loss 11.152309\n","Epoch 50, train loss 11.076333\n","Epoch 50 took 7.74 seconds.\n","mask_loss       Train: 0.697884, \t Eval: 0.697889\n","dice_loss       Train: 0.943035, \t Eval: 0.943931\n","New best train loss, saving model.\n","New best eval  loss, saving model.\n","Epoch 60, train loss 10.958718\n","Epoch 70, train loss 10.905177\n","Epoch 80, train loss 10.86022\n","Epoch 90, train loss 10.707402\n","Epoch 100, train loss 10.728827\n","Epoch 100 took 7.78 seconds.\n","mask_loss       Train: 0.698995, \t Eval: 0.698984\n","dice_loss       Train: 0.943035, \t Eval: 0.943931\n","Epoch 110, train loss 10.72813\n","Epoch 120, train loss 10.592278\n","Epoch 130, train loss 10.657161\n","Epoch 140, train loss 10.44753\n","Epoch 150, train loss 10.495452\n","Epoch 150 took 7.8 seconds.\n","mask_loss       Train: 0.698623, \t Eval: 0.698606\n","dice_loss       Train: 0.943035, \t Eval: 0.943931\n","Epoch 160, train loss 10.354577\n","Epoch 170, train loss 10.469441\n","Epoch 180, train loss 10.46805\n","Epoch 190, train loss 10.331345\n","Epoch 200, train loss 10.24835\n","Epoch 200 took 7.77 seconds.\n","mask_loss       Train: 0.698586, \t Eval: 0.698583\n","dice_loss       Train: 0.943035, \t Eval: 0.943931\n","Epoch 210, train loss 10.273008\n","Epoch 220, train loss 10.275335\n","Epoch 230, train loss 10.325509\n","Epoch 240, train loss 10.140519\n","Epoch 250, train loss 10.056934\n","Epoch 250 took 7.78 seconds.\n","mask_loss       Train: 0.698604, \t Eval: 0.698655\n","dice_loss       Train: 0.943035, \t Eval: 0.943931\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ptM-FvgXmZD_"},"source":[""],"execution_count":null,"outputs":[]}]}