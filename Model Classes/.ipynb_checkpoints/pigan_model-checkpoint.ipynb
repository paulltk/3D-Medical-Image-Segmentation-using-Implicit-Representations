{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "import numpy as np\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import math\n",
    "from einops import repeat, rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def leaky_relu(p = 0.2):\n",
    "    return nn.LeakyReLU(p)\n",
    "\n",
    "def to_value(t):\n",
    "    return t.clone().detach().item()\n",
    "\n",
    "def get_module_device(module):\n",
    "    return next(module.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, lr_mul = 0.1, bias = True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "        self.lr_mul = lr_mul\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight * self.lr_mul, bias=self.bias * self.lr_mul)\n",
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, *, dim, dim_out, depth = 3, lr_mul = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            layers.extend([EqualLinear(dim, dim, lr_mul), leaky_relu()])\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        self.to_gamma = nn.Linear(dim, dim_out)\n",
    "        self.to_beta = nn.Linear(dim, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return self.to_gamma(x), self.to_beta(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sin activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine(nn.Module):\n",
    "    def __init__(self, w0 = 1.):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIREN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siren(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, w0 = 1., c = 6., is_first = False, use_bias = True, activation = None):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.is_first = is_first\n",
    "\n",
    "        weight = torch.zeros(dim_out, dim_in)\n",
    "        bias = torch.zeros(dim_out) if use_bias else None\n",
    "        self.init_(weight, bias, c = c, w0 = w0)\n",
    "\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        self.bias = nn.Parameter(bias) if use_bias else None\n",
    "        self.activation = Sine(w0) if activation is None else activation\n",
    "\n",
    "    def init_(self, weight, bias, c, w0):\n",
    "        dim = self.dim_in\n",
    "\n",
    "        w_std = (1 / dim) if self.is_first else (math.sqrt(c / dim) / w0)\n",
    "        weight.uniform_(-w_std, w_std)\n",
    "\n",
    "        if bias is not None:\n",
    "            bias.uniform_(-w_std, w_std)\n",
    "\n",
    "    def forward(self, x, gamma = None, beta = None):\n",
    "        out =  F.linear(x, self.weight, self.bias)\n",
    "        \n",
    "        # FiLM modulation\n",
    "\n",
    "        if exists(gamma):\n",
    "            out = out * gamma\n",
    "\n",
    "        if exists(beta):\n",
    "            out = out + beta\n",
    "\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SirenNet(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out, num_layers, w0 = 1., w0_initial = 30., use_bias = True, final_activation = None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for ind in range(num_layers):\n",
    "            is_first = ind == 0\n",
    "            layer_w0 = w0_initial if is_first else w0\n",
    "            layer_dim_in = dim_in if is_first else dim_hidden\n",
    "\n",
    "            self.layers.append(Siren(\n",
    "                dim_in = layer_dim_in,\n",
    "                dim_out = dim_hidden,\n",
    "                w0 = layer_w0,\n",
    "                use_bias = use_bias,\n",
    "                is_first = is_first\n",
    "            ))\n",
    "\n",
    "#         self.last_layer = Siren(dim_in = dim_hidden, dim_out = dim_out, w0 = w0, use_bias = use_bias, activation = final_activation)\n",
    "\n",
    "    def forward(self, x, gamma, beta):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, gamma, beta)\n",
    "#         return self.last_layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete SIREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SirenGenerator(nn.Module):\n",
    "    def __init__(self, dim, dim_hidden,siren_num_layers = 5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mapping = MappingNetwork(dim = dim, dim_out = dim_hidden)\n",
    "\n",
    "        self.siren = SirenNet(dim_in = 3, dim_hidden = dim_hidden, \n",
    "                              dim_out = dim_hidden,num_layers = siren_num_layers)\n",
    "\n",
    "        self.last_layer = nn.Linear(dim_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, latent, coords):\n",
    "        gamma, beta = self.mapping(latent)\n",
    "        \n",
    "        out = self.siren(coords, gamma, beta)\n",
    "        \n",
    "        out = self.sigmoid(self.last_layer(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Imported PI-Gan model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# siren = SirenGenerator(128, 256).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent = torch.randn((1, 128)).cuda()\n",
    "# coords = torch.randn((1, 30000, 3)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = siren(latent, coords)\n",
    "\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitc17f53f707db4b89be7c32a22adf91a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
