{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coordconv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from \n",
    "# https://github.com/mkocabas/CoordConv-pytorch/blob/master/CoordConv.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AddCoords(nn.Module):\n",
    "\n",
    "    def __init__(self, with_r=False):\n",
    "        super().__init__()\n",
    "        self.with_r = with_r\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: shape(batch, channel, x_dim, y_dim)\n",
    "        \"\"\"\n",
    "        batch_size, _, x_dim, y_dim = input_tensor.size()\n",
    "\n",
    "        xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n",
    "        yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n",
    "\n",
    "        xx_channel = xx_channel.float() / (x_dim - 1)\n",
    "        yy_channel = yy_channel.float() / (y_dim - 1)\n",
    "\n",
    "        xx_channel = xx_channel * 2 - 1\n",
    "        yy_channel = yy_channel * 2 - 1\n",
    "\n",
    "        xx_channel = xx_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n",
    "        yy_channel = yy_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n",
    "\n",
    "        ret = torch.cat([\n",
    "            input_tensor,\n",
    "            xx_channel.type_as(input_tensor),\n",
    "            yy_channel.type_as(input_tensor)], dim=1)\n",
    "\n",
    "        if self.with_r:\n",
    "            rr = torch.sqrt(torch.pow(xx_channel.type_as(input_tensor) - 0.5, 2) + torch.pow(yy_channel.type_as(input_tensor) - 0.5, 2))\n",
    "            ret = torch.cat([ret, rr], dim=1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "class CoordConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, with_r=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.addcoords = AddCoords(with_r=with_r)\n",
    "        in_size = in_channels+2\n",
    "        if with_r:\n",
    "            in_size += 1\n",
    "        self.conv = nn.Conv2d(in_size, out_channels, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = self.addcoords(x)\n",
    "        ret = self.conv(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nerf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken and modified from https://colab.research.google.com/drive/1rO8xo0TemN67d4mTpakrKrLp03b9bgCX#scrollTo=JovhcSy1NIhr\n",
    "# will need to be refactored from 3d input to 5d (with ray direction)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat, rearrange\n",
    "\n",
    "def meshgrid_xy(tensor1, tensor2):\n",
    "    ii, jj = torch.meshgrid(tensor1, tensor2)\n",
    "    return ii.transpose(-1, -2), jj.transpose(-1, -2)\n",
    "\n",
    "def cumprod_exclusive(tensor):\n",
    "    cumprod = torch.cumprod(tensor, dim = -1)\n",
    "    cumprod = torch.roll(cumprod, 1, -1)\n",
    "    cumprod[..., 0] = 1.\n",
    "    return cumprod\n",
    "\n",
    "def get_ray_bundle(height, width, focal_length, tform_cam2world):\n",
    "    ii, jj = meshgrid_xy(\n",
    "      torch.arange(width).to(tform_cam2world),\n",
    "      torch.arange(height).to(tform_cam2world)\n",
    "    )\n",
    "\n",
    "    directions = torch.stack([(ii - width * .5) / focal_length,\n",
    "                            -(jj - height * .5) / focal_length,\n",
    "                            -torch.ones_like(ii)\n",
    "                           ], dim=-1)\n",
    "    ray_directions = torch.sum(directions[..., None, :] * tform_cam2world[:3, :3], dim=-1)\n",
    "    ray_origins = tform_cam2world[:3, -1].expand(ray_directions.shape)\n",
    "    return ray_origins, ray_directions\n",
    "\n",
    "def compute_query_points_from_rays(\n",
    "    ray_origins,\n",
    "    ray_directions,\n",
    "    near_thresh,\n",
    "    far_thresh,\n",
    "    num_samples,\n",
    "    randomize = True\n",
    "):\n",
    "    depth_values = torch.linspace(near_thresh, far_thresh, num_samples).to(ray_origins)\n",
    "    if randomize is True:\n",
    "        noise_shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
    "        depth_values = depth_values \\\n",
    "            + torch.rand(noise_shape).to(ray_origins) * (far_thresh\n",
    "                - near_thresh) / num_samples\n",
    "    query_points = ray_origins[..., None, :] + ray_directions[..., None, :] * depth_values[..., :, None]\n",
    "    return query_points, depth_values\n",
    "\n",
    "def render_volume_density(\n",
    "    radiance_field,\n",
    "    ray_origins,\n",
    "    depth_values\n",
    "):\n",
    "    sigma_a = F.relu(radiance_field[..., 3])\n",
    "    rgb = torch.sigmoid(radiance_field[..., :3])\n",
    "    one_e_10 = torch.tensor([1e10], dtype=ray_origins.dtype, device=ray_origins.device)\n",
    "    dists = torch.cat((depth_values[..., 1:] - depth_values[..., :-1],\n",
    "                  one_e_10.expand(depth_values[..., :1].shape)), dim=-1)\n",
    "    alpha = 1. - torch.exp(-sigma_a * dists)\n",
    "    weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "    rgb_map = (weights[..., None] * rgb).sum(dim=-2)\n",
    "    depth_map = (weights * depth_values).sum(dim=-1)\n",
    "    acc_map = weights.sum(-1)\n",
    "\n",
    "    return rgb_map, depth_map, acc_map\n",
    "\n",
    "def get_image_from_nerf_model(\n",
    "    model,\n",
    "    latents,\n",
    "    height,\n",
    "    width,\n",
    "    focal_length = 140,\n",
    "    tform_cam2world = torch.eye(4),\n",
    "    near_thresh = 2.,\n",
    "    far_thresh = 6.,\n",
    "    depth_samples_per_ray = 32\n",
    "):\n",
    "    tform_cam2world = tform_cam2world.to(latents)\n",
    "\n",
    "    ray_origins, ray_directions = get_ray_bundle(height, width, focal_length,\n",
    "                                               tform_cam2world)\n",
    "\n",
    "    query_points, depth_values = compute_query_points_from_rays(\n",
    "      ray_origins, ray_directions, near_thresh, far_thresh, depth_samples_per_ray\n",
    "    )\n",
    "\n",
    "    flattened_query_points = query_points.reshape((-1, 3))\n",
    "\n",
    "    images = []\n",
    "    for latent in latents.unbind(0):\n",
    "        predictions = []\n",
    "        predictions.append(model(latent, flattened_query_points))\n",
    "\n",
    "        radiance_field_flattened = torch.cat(predictions, dim=0)\n",
    "\n",
    "        unflattened_shape = list(query_points.shape[:-1]) + [4]\n",
    "        radiance_field = torch.reshape(radiance_field_flattened, unflattened_shape)\n",
    "\n",
    "        rgb_predicted, _, _ = render_volume_density(radiance_field, ray_origins, depth_values)\n",
    "        image = rearrange(rgb_predicted, 'h w c -> c h w')\n",
    "        images.append(image)\n",
    "\n",
    "    return torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad as torch_grad\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "assert torch.cuda.is_available(), 'You need to have an Nvidia GPU with CUDA installed.'\n",
    "\n",
    "# helper\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def leaky_relu(p = 0.2):\n",
    "    return nn.LeakyReLU(p)\n",
    "\n",
    "def to_value(t):\n",
    "    return t.clone().detach().item()\n",
    "\n",
    "def get_module_device(module):\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "# losses\n",
    "\n",
    "def gradient_penalty(images, output, weight = 10):\n",
    "    batch_size, device = images.shape[0], images.device\n",
    "    gradients = torch_grad(outputs=output, inputs=images,\n",
    "                           grad_outputs=torch.ones(output.size(), device=device),\n",
    "                           create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.reshape(batch_size, -1)\n",
    "    l2 = ((gradients.norm(2, dim = 1) - 1) ** 2).mean()\n",
    "    return weight * l2\n",
    "\n",
    "# sin activation\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def __init__(self, w0 = 1.):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "# siren layer\n",
    "\n",
    "class Siren(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, w0 = 1., c = 6., is_first = False, use_bias = True, activation = None):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.is_first = is_first\n",
    "\n",
    "        weight = torch.zeros(dim_out, dim_in)\n",
    "        bias = torch.zeros(dim_out) if use_bias else None\n",
    "        self.init_(weight, bias, c = c, w0 = w0)\n",
    "\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        self.bias = nn.Parameter(bias) if use_bias else None\n",
    "        self.activation = Sine(w0) if activation is None else activation\n",
    "\n",
    "    def init_(self, weight, bias, c, w0):\n",
    "        dim = self.dim_in\n",
    "\n",
    "        w_std = (1 / dim) if self.is_first else (math.sqrt(c / dim) / w0)\n",
    "        weight.uniform_(-w_std, w_std)\n",
    "\n",
    "        if bias is not None:\n",
    "            bias.uniform_(-w_std, w_std)\n",
    "\n",
    "    def forward(self, x, gamma = None, beta = None):\n",
    "        out =  F.linear(x, self.weight, self.bias)\n",
    "\n",
    "        # FiLM modulation\n",
    "\n",
    "        if exists(gamma):\n",
    "            out = out * gamma\n",
    "\n",
    "        if exists(beta):\n",
    "            out = out + beta\n",
    "\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "# mapping network\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, lr_mul = 0.1, bias = True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "        self.lr_mul = lr_mul\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight * self.lr_mul, bias=self.bias * self.lr_mul)\n",
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, *, dim, dim_out, depth = 3, lr_mul = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            layers.extend([EqualLinear(dim, dim, lr_mul), leaky_relu()])\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        self.to_gamma = nn.Linear(dim, dim_out)\n",
    "        self.to_beta = nn.Linear(dim, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, dim = -1)\n",
    "        x = self.net(x)\n",
    "        return self.to_gamma(x), self.to_beta(x)\n",
    "\n",
    "# siren network\n",
    "\n",
    "class SirenNet(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out, num_layers, w0 = 1., w0_initial = 30., use_bias = True, final_activation = None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for ind in range(num_layers):\n",
    "            is_first = ind == 0\n",
    "            layer_w0 = w0_initial if is_first else w0\n",
    "            layer_dim_in = dim_in if is_first else dim_hidden\n",
    "\n",
    "            self.layers.append(Siren(\n",
    "                dim_in = layer_dim_in,\n",
    "                dim_out = dim_hidden,\n",
    "                w0 = layer_w0,\n",
    "                use_bias = use_bias,\n",
    "                is_first = is_first\n",
    "            ))\n",
    "\n",
    "        self.last_layer = Siren(dim_in = dim_hidden, dim_out = dim_out, w0 = w0, use_bias = use_bias, activation = final_activation)\n",
    "\n",
    "    def forward(self, x, gamma, beta):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, gamma, beta)\n",
    "        return self.last_layer(x)\n",
    "\n",
    "# generator\n",
    "\n",
    "class SirenGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_hidden,\n",
    "        siren_num_layers = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mapping = MappingNetwork(\n",
    "            dim = dim,\n",
    "            dim_out = dim_hidden\n",
    "        )\n",
    "\n",
    "        self.siren = SirenNet(\n",
    "            dim_in = 3,\n",
    "            dim_hidden = dim_hidden,\n",
    "            dim_out = dim_hidden,\n",
    "            num_layers = siren_num_layers\n",
    "        )\n",
    "\n",
    "        self.to_alpha = nn.Linear(dim_hidden, 1)\n",
    "\n",
    "        self.to_rgb_siren = Siren(\n",
    "            dim_in = dim_hidden,\n",
    "            dim_out = dim_hidden\n",
    "        )\n",
    "\n",
    "        self.to_rgb = nn.Linear(dim_hidden, 3)\n",
    "\n",
    "    def forward(self, latent, coors, batch_size = 8192):\n",
    "        gamma, beta = self.mapping(latent)\n",
    "\n",
    "        outs = []\n",
    "        for coor in coors.split(batch_size):\n",
    "            gamma_, beta_ = map(lambda t: rearrange(t, 'n -> () n'), (gamma, beta))\n",
    "            x = self.siren(coor, gamma_, beta_)\n",
    "            alpha = self.to_alpha(x)\n",
    "\n",
    "            x = self.to_rgb_siren(x, gamma, beta)\n",
    "            rgb = self.to_rgb(x)\n",
    "            out = torch.cat((rgb, alpha), dim = -1)\n",
    "            outs.append(out)\n",
    "\n",
    "        return torch.cat(outs)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        dim,\n",
    "        dim_hidden,\n",
    "        siren_num_layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.nerf_model = SirenGenerator(\n",
    "            dim = dim,\n",
    "            dim_hidden = dim_hidden,\n",
    "            siren_num_layers = siren_num_layers\n",
    "        )\n",
    "\n",
    "    def set_image_size(self, image_size):\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def forward(self, latents):\n",
    "        image_size = self.image_size\n",
    "        device, b = latents.device, latents.shape[0]\n",
    "\n",
    "        generated_images = get_image_from_nerf_model(\n",
    "            self.nerf_model,\n",
    "            latents,\n",
    "            image_size,\n",
    "            image_size\n",
    "        )\n",
    "\n",
    "        return generated_images\n",
    "\n",
    "# discriminator\n",
    "\n",
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out):\n",
    "        super().__init__()\n",
    "        self.res = CoordConv(dim, dim_out, kernel_size = 1, stride = 2)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            CoordConv(dim, dim_out, kernel_size = 3, padding = 1),\n",
    "            leaky_relu(),\n",
    "            CoordConv(dim_out, dim_out, kernel_size = 3, padding = 1),\n",
    "            leaky_relu()\n",
    "        )\n",
    "\n",
    "        self.down = nn.AvgPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.res(x)\n",
    "        x = self.net(x)\n",
    "        x = self.down(x)\n",
    "        x = x + res\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        init_chan = 64,\n",
    "        max_chan = 400,\n",
    "        init_resolution = 32,\n",
    "        add_layer_iters = 10000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        resolutions = math.log2(image_size)\n",
    "        assert resolutions.is_integer(), 'image size must be a power of 2'\n",
    "        assert math.log2(init_resolution).is_integer(), 'initial resolution must be power of 2'\n",
    "\n",
    "        resolutions = int(resolutions)\n",
    "        layers = resolutions - 1\n",
    "\n",
    "        chans = list(reversed(list(map(lambda t: 2 ** (11 - t), range(layers)))))\n",
    "        chans = list(map(lambda n: min(max_chan, n), chans))\n",
    "        chans = [init_chan, *chans]\n",
    "        final_chan = chans[-1]\n",
    "\n",
    "        self.from_rgb_layers = nn.ModuleList([])\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.image_size = image_size\n",
    "        self.resolutions = list(map(lambda t: 2 ** (7 - t), range(layers)))\n",
    "\n",
    "        for resolution, in_chan, out_chan in zip(self.resolutions, chans[:-1], chans[1:]):\n",
    "\n",
    "            from_rgb_layer = nn.Sequential(\n",
    "                CoordConv(3, in_chan, kernel_size = 1),\n",
    "                leaky_relu()\n",
    "            ) if resolution >= init_resolution else None\n",
    "\n",
    "            self.from_rgb_layers.append(from_rgb_layer)\n",
    "\n",
    "            self.layers.append(DiscriminatorBlock(\n",
    "                dim = in_chan,\n",
    "                dim_out = out_chan\n",
    "            ))\n",
    "\n",
    "        self.final_conv = CoordConv(final_chan, 1, kernel_size = 2)\n",
    "\n",
    "        self.add_layer_iters = add_layer_iters\n",
    "        self.register_buffer('alpha', torch.tensor(0.))\n",
    "        self.register_buffer('resolution', torch.tensor(init_resolution))\n",
    "        self.register_buffer('iterations', torch.tensor(0.))\n",
    "\n",
    "    def increase_resolution_(self):\n",
    "        if self.resolution >= self.image_size:\n",
    "            return\n",
    "\n",
    "        self.alpha += self.alpha + (1 - self.alpha)\n",
    "        self.iterations.fill_(0.)\n",
    "        self.resolution *= 2\n",
    "\n",
    "    def update_iter_(self):\n",
    "        self.iterations += 1\n",
    "        self.alpha -= (1 / self.add_layer_iters)\n",
    "        self.alpha.clamp_(min = 0.)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = img\n",
    "\n",
    "        for resolution, from_rgb, layer in zip(self.resolutions, self.from_rgb_layers, self.layers):\n",
    "            if self.resolution < resolution:\n",
    "                continue\n",
    "\n",
    "            if self.resolution == resolution:\n",
    "                x = from_rgb(x)\n",
    "\n",
    "            if bool(resolution == (self.resolution // 2)) and bool(self.alpha > 0):\n",
    "                x_down = F.interpolate(img, scale_factor = 0.5)\n",
    "                x = x * (1 - self.alpha) + from_rgb(x_down) * self.alpha\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "        out = self.final_conv(x)\n",
    "        return out\n",
    "\n",
    "# pi-GAN class\n",
    "\n",
    "class piGAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        dim,\n",
    "        init_resolution = 32,\n",
    "        generator_dim_hidden = 256,\n",
    "        siren_num_layers = 6,\n",
    "        add_layer_iters = 10000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        self.G = Generator(\n",
    "            image_size = image_size,\n",
    "            dim = dim,\n",
    "            dim_hidden = generator_dim_hidden,\n",
    "            siren_num_layers = siren_num_layers\n",
    "\n",
    "        )\n",
    "\n",
    "        self.D = Discriminator(\n",
    "            image_size = image_size,\n",
    "            add_layer_iters = add_layer_iters,\n",
    "            init_resolution = init_resolution\n",
    "        )\n",
    "\n",
    "# dataset\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for i in iterable:\n",
    "            yield i\n",
    "\n",
    "def resize_to_minimum_size(min_size, image):\n",
    "    if max(*image.size) < min_size:\n",
    "        return torchvision.transforms.functional.resize(image, min_size)\n",
    "    return image\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder,\n",
    "        image_size,\n",
    "        transparent = False,\n",
    "        aug_prob = 0.,\n",
    "        exts = ['jpg', 'jpeg', 'png']\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.folder = folder\n",
    "        self.image_size = image_size\n",
    "        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
    "        assert len(self.paths) > 0, f'No images were found in {folder} for training'\n",
    "        self.create_transform(image_size)\n",
    "\n",
    "    def create_transform(self, image_size):\n",
    "        self.transform = T.Compose([\n",
    "            T.Lambda(partial(resize_to_minimum_size, image_size)),\n",
    "            T.Resize(image_size),\n",
    "            T.CenterCrop(image_size),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path)\n",
    "        return self.transform(img)\n",
    "\n",
    "# trainer\n",
    "\n",
    "def sample_generator(G, batch_size):\n",
    "    dim = G.dim\n",
    "    rand_latents = torch.randn(batch_size, dim).cuda()\n",
    "    return G(rand_latents)\n",
    "\n",
    "class Trainer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        gan,\n",
    "        folder,\n",
    "        add_layers_iters = 10000,\n",
    "        batch_size = 8,\n",
    "        gradient_accumulate_every = 4,\n",
    "        sample_every = 100,\n",
    "        log_every = 10,\n",
    "        num_train_steps = 50000,\n",
    "        lr_gen = 5e-5,\n",
    "        lr_discr = 4e-4,\n",
    "        target_lr_gen = 1e-5,\n",
    "        target_lr_discr = 1e-4,\n",
    "        lr_decay_span = 10000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        gan.D.add_layer_iters = add_layers_iters\n",
    "        self.add_layers_iters = add_layers_iters\n",
    "\n",
    "        self.gan = gan.cuda()\n",
    "\n",
    "        self.optim_D = Adam(self.gan.D.parameters(), betas=(0, 0.9), lr = lr_discr)\n",
    "        self.optim_G = Adam(self.gan.G.parameters(), betas=(0, 0.9), lr = lr_gen)\n",
    "\n",
    "        D_decay_fn = lambda i: max(1 - i / lr_decay_span, 0) + (target_lr_discr / lr_discr) * min(i / lr_decay_span, 1)\n",
    "        G_decay_fn = lambda i: max(1 - i / lr_decay_span, 0) + (target_lr_gen / lr_gen) * min(i / lr_decay_span, 1)\n",
    "\n",
    "        self.sched_D = LambdaLR(self.optim_D, D_decay_fn)\n",
    "        self.sched_G = LambdaLR(self.optim_G, G_decay_fn)\n",
    "\n",
    "        self.iterations = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.num_train_steps = num_train_steps\n",
    "\n",
    "        self.log_every = log_every\n",
    "        self.sample_every = sample_every\n",
    "        self.gradient_accumulate_every = gradient_accumulate_every\n",
    "\n",
    "        self.dataset = ImageDataset(folder = folder, image_size = gan.D.resolution.item())\n",
    "        self.dataloader = cycle(DataLoader(self.dataset, batch_size = batch_size, shuffle = True, drop_last = True))\n",
    "\n",
    "        self.last_loss_D = 0\n",
    "        self.last_loss_G = 0\n",
    "\n",
    "    def step(self):\n",
    "        D, G, batch_size, dim, accumulate_every = self.gan.D, self.gan.G, self.batch_size, self.gan.dim, self.gradient_accumulate_every\n",
    "\n",
    "        # set appropriate image size\n",
    "\n",
    "        if self.iterations % self.add_layers_iters == 0:\n",
    "            if self.iterations != 0:\n",
    "                D.increase_resolution_()\n",
    "\n",
    "            image_size = D.resolution.item()\n",
    "            G.set_image_size(image_size)\n",
    "            self.dataset.create_transform(image_size)\n",
    "\n",
    "        # gp\n",
    "\n",
    "        apply_gp = self.iterations % 4 == 0\n",
    "\n",
    "        # train discriminator\n",
    "\n",
    "        D.train()\n",
    "        loss_D = 0\n",
    "\n",
    "        for _ in range(accumulate_every):\n",
    "            images = next(self.dataloader)\n",
    "            images = images.cuda().requires_grad_()\n",
    "            real_out = D(images)\n",
    "\n",
    "            fake_imgs = sample_generator(G, batch_size)\n",
    "            fake_out = D(fake_imgs.clone().detach())\n",
    "\n",
    "            divergence = (F.relu(1 + real_out) + F.relu(1 - fake_out)).mean()\n",
    "            loss = divergence\n",
    "\n",
    "            if apply_gp:\n",
    "                gp = gradient_penalty(images, real_out)\n",
    "                self.last_loss_gp = to_value(gp)\n",
    "                loss = loss + gp\n",
    "\n",
    "            (loss / accumulate_every).backward()\n",
    "            loss_D += to_value(divergence) / accumulate_every\n",
    "\n",
    "        self.last_loss_D = loss_D\n",
    "\n",
    "        self.optim_D.step()\n",
    "        self.optim_D.zero_grad()\n",
    "\n",
    "        # train generator\n",
    "\n",
    "        G.train()\n",
    "        loss_G = 0\n",
    "\n",
    "        for _ in range(accumulate_every):\n",
    "            fake_out = sample_generator(G, batch_size)\n",
    "            loss = D(fake_out).mean()\n",
    "            (loss / accumulate_every).backward()\n",
    "            loss_G += to_value(loss) / accumulate_every\n",
    "\n",
    "        self.last_loss_G = loss_G\n",
    "\n",
    "        self.optim_G.step()\n",
    "        self.optim_G.zero_grad()\n",
    "\n",
    "        # update schedulers\n",
    "\n",
    "        self.sched_D.step()\n",
    "        self.sched_G.step()\n",
    "\n",
    "        self.iterations += 1\n",
    "        D.update_iter_()\n",
    "\n",
    "    def forward(self):\n",
    "        for _ in trange(self.num_train_steps):\n",
    "            self.step()\n",
    "\n",
    "            if self.iterations % self.log_every == 0:\n",
    "                print(f'I: {self.gan.D.resolution.item()} | D: {self.last_loss_D:.2f} | G: {self.last_loss_G:.2f} | GP: {self.last_loss_gp:.2f}')\n",
    "\n",
    "            if self.iterations % self.sample_every == 0:\n",
    "                i = self.iterations // self.sample_every\n",
    "                imgs = sample_generator(self.gan.G, 4)\n",
    "                imgs.clamp_(0., 1.)\n",
    "                save_image(imgs, f'./{i}.png', nrow = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitc17f53f707db4b89be7c32a22adf91a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
