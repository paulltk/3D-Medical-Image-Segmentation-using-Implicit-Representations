{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/data_classes/new_dataset.ipynb to script\n",
      "[NbConvertApp] Writing 4177 bytes to py_files/new_dataset.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/model_classes/cnn_model.ipynb to script\n",
      "[NbConvertApp] Writing 6584 bytes to py_files/cnn_model.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/model_classes/pigan_model.ipynb to script\n",
      "[NbConvertApp] Writing 3587 bytes to py_files/pigan_model.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/pi-gan/pi_gan_functions.ipynb to script\n",
      "[NbConvertApp] Writing 21735 bytes to py_files/pi_gan_functions.py\n",
      "Saved py files.\n"
     ]
    }
   ],
   "source": [
    "%run /home/ptenkaate/scratch/Master-Thesis/convert_ipynb_to_py_files.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all helper functions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math \n",
    "import skimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from py_files.new_dataset import *\n",
    "\n",
    "from py_files.cnn_model import *\n",
    "from py_files.pigan_model import *\n",
    "\n",
    "from py_files.pi_gan_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():  \n",
    "    \n",
    "    ##### path to wich the model should be saved #####\n",
    "    path = get_folder(ARGS)\n",
    "    \n",
    "    ##### save ARGS #####\n",
    "    with open(f\"{path}/ARGS.txt\", \"w\") as f:\n",
    "        print(vars(ARGS), file=f)\n",
    "        \n",
    "    ##### data preparation #####\n",
    "    train_dl, val_dl, test_dl = initialize_dataloaders(ARGS) \n",
    "    \n",
    "            \n",
    "    ##### initialize models and optimizers #####\n",
    "    models, optims, schedulers = load_models_and_optims(ARGS)\n",
    "\n",
    "    ##### load pretrained model #####\n",
    "    if ARGS.pretrained: \n",
    "        print(f\"Loading pretrained model from '{ARGS.pretrained}'.\")\n",
    "        load_models(ARGS.pretrained, ARGS.pretrained_best, \n",
    "                    models, optims)\n",
    "    \n",
    "    ##### loss function #####\n",
    "    criterions = [nn.BCELoss(), nn.MSELoss()]\n",
    "#     criterions = [nn.BCELoss(), nn.L1Loss()]\n",
    "        \n",
    "    ##### epoch, train loss mean, train loss std, #####\n",
    "    ##### val loss mean, val loss std #####\n",
    "    mask_losses = np.empty((0, 5))\n",
    "    pcmra_losses = np.empty((0, 5))\n",
    "    dice_losses = np.empty((0, 5))\n",
    "\n",
    "    batch_count = 0     \n",
    "    \n",
    "    for ep in range(ARGS.epochs):\n",
    "    \n",
    "        t = time.time() \n",
    "\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "\n",
    "        t_loss_mean, t_loss_std, \\\n",
    "        t_p_loss_mean, t_p_loss_std, \\\n",
    "        batch_count = train_epoch(train_dl, models, optims, schedulers,\n",
    "                                  criterions, batch_count, ARGS)\n",
    "        \n",
    "        print(f\"Epoch {ep}, train loss: {t_loss_mean}, train pcmra loss: {t_p_loss_mean}\")\n",
    "        \n",
    "        if ep % ARGS.eval_every == 0: \n",
    "\n",
    "            print(f\"Epoch {ep} took {round(time.time() - t, 2)} seconds.\")\n",
    "            \n",
    "            t_mask_mean, t_mask_std, \\\n",
    "            t_pcmra_mean, t_pcmra_std, \\\n",
    "            t_dice_mean, t_dice_std = val_epoch(train_dl, models, criterions, ARGS)\n",
    "            \n",
    "            v_mask_mean, v_mask_std, \\\n",
    "            v_pcmra_mean, v_pcmra_std, \\\n",
    "            v_dice_mean, v_dice_std = val_epoch(val_dl, models, criterions, ARGS)\n",
    "            \n",
    "            mask_losses = np.append(mask_losses, [[ep ,t_mask_mean, t_mask_std, \n",
    "                                         v_mask_mean, v_mask_std]], axis=0)\n",
    "            \n",
    "            pcmra_losses = np.append(pcmra_losses, [[ep ,t_pcmra_mean, t_pcmra_std, \n",
    "                                         v_pcmra_mean, v_pcmra_std]], axis=0)\n",
    "            \n",
    "            dice_losses = np.append(dice_losses, [[ep ,t_dice_mean, t_dice_std, \n",
    "                                         v_dice_mean, v_dice_std]], axis=0)\n",
    "            \n",
    "            save_info(path, mask_losses, pcmra_losses, dice_losses, models, optims, save_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as .ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ARGS class initialized.\n",
      "{'device': 'GPU', 'print_models': False, 'name': '', 'pretrained': None, 'pretrained_best': 'train', 'reconstruction': 'pcmra', 'share_mapping': False, 'pcmra_lambda': 5, 'mask_lambda': 1.0, 'dataset': 'small', 'rotate': True, 'translate': True, 'flip': False, 'norm_min_max': [0, 1], 'seed': 34, 'epochs': 70, 'batch_size': 8, 'eval_every': 5, 'shuffle': True, 'n_coords_sample': 20000, 'cnn_setup': 1, 'mapping_setup': 2, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30.0, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'scheduler_on': 'combined'}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 1596\n",
      "[['16-01-22_Jarik_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27 Valentine kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27_Claudia_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-03_Feiko_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Luuk_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Michelle_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-12_Marjolein_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-03-18_Ruud_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-04-13_Pim_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-05-25_Emile_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)']]\n",
      "Val subjects: 28\n",
      "Test subjects: 28\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "PCMRA_SIREN\n",
      "Epoch 0, train loss: 0.190195, train pcmra loss: 0.004894\n",
      "Epoch 0 took 59.65 seconds.\n",
      "Train mask loss: \t 0.10485,     pcmra loss: \t 0.00312,     \tdice loss: \t 0.98344.\n",
      "Eval  mask loss: \t 0.10947,     pcmra loss: \t 0.00366,     \tdice loss: \t 0.98014.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 1, train loss: 0.092227, train pcmra loss: 0.003403\n",
      "Epoch 2, train loss: 0.073943, train pcmra loss: 0.003051\n",
      "Epoch 3, train loss: 0.065657, train pcmra loss: 0.002816\n",
      "Epoch 4, train loss: 0.060277, train pcmra loss: 0.0027\n",
      "Epoch 5, train loss: 0.056151, train pcmra loss: 0.002582\n",
      "Epoch 5 took 58.86 seconds.\n",
      "Train mask loss: \t 0.05524,     pcmra loss: \t 0.00244,     \tdice loss: \t 0.48401.\n",
      "Eval  mask loss: \t 0.06091,     pcmra loss: \t 0.00286,     \tdice loss: \t 0.50141.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 6, train loss: 0.052876, train pcmra loss: 0.002471\n",
      "Epoch 7, train loss: 0.050972, train pcmra loss: 0.002437\n",
      "Epoch 8, train loss: 0.048128, train pcmra loss: 0.002328\n",
      "Epoch 9, train loss: 0.0467, train pcmra loss: 0.002267\n",
      "Epoch 10, train loss: 0.044184, train pcmra loss: 0.002171\n",
      "Epoch 10 took 60.2 seconds.\n",
      "Train mask loss: \t 0.04319,     pcmra loss: \t 0.00197,     \tdice loss: \t 0.39969.\n",
      "Eval  mask loss: \t 0.05987,     pcmra loss: \t 0.00231,     \tdice loss: \t 0.46082.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 11, train loss: 0.043889, train pcmra loss: 0.002163\n",
      "Epoch 12, train loss: 0.040926, train pcmra loss: 0.002087\n",
      "Epoch 13, train loss: 0.040122, train pcmra loss: 0.002051\n",
      "Epoch 14, train loss: 0.039406, train pcmra loss: 0.001999\n",
      "Epoch 15, train loss: 0.038137, train pcmra loss: 0.001967\n",
      "Epoch 15 took 57.85 seconds.\n",
      "Train mask loss: \t 0.03751,     pcmra loss: \t 0.00181,     \tdice loss: \t 0.29646.\n",
      "Eval  mask loss: \t 0.0574,     pcmra loss: \t 0.00232,     \tdice loss: \t 0.36429.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 16, train loss: 0.037024, train pcmra loss: 0.001952\n",
      "Epoch 17, train loss: 0.035865, train pcmra loss: 0.001847\n",
      "Epoch 18, train loss: 0.034751, train pcmra loss: 0.001836\n",
      "Epoch 19, train loss: 0.035545, train pcmra loss: 0.001841\n",
      "Epoch 20, train loss: 0.032968, train pcmra loss: 0.001706\n",
      "Epoch 20 took 58.37 seconds.\n",
      "Train mask loss: \t 0.03141,     pcmra loss: \t 0.00173,     \tdice loss: \t 0.25335.\n",
      "Eval  mask loss: \t 0.05216,     pcmra loss: \t 0.00229,     \tdice loss: \t 0.35577.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 21, train loss: 0.031879, train pcmra loss: 0.001702\n",
      "Epoch 22, train loss: 0.032348, train pcmra loss: 0.001731\n",
      "Epoch 23, train loss: 0.031417, train pcmra loss: 0.001657\n",
      "Epoch 24, train loss: 0.03002, train pcmra loss: 0.001604\n",
      "Epoch 25, train loss: 0.028833, train pcmra loss: 0.001556\n",
      "Epoch 25 took 59.72 seconds.\n",
      "Train mask loss: \t 0.02771,     pcmra loss: \t 0.00159,     \tdice loss: \t 0.21704.\n",
      "Eval  mask loss: \t 0.05931,     pcmra loss: \t 0.00234,     \tdice loss: \t 0.345.\n",
      "New best train loss, saving model.\n",
      "Epoch 26, train loss: 0.029202, train pcmra loss: 0.001554\n",
      "Epoch 27, train loss: 0.029475, train pcmra loss: 0.001562\n",
      "Epoch 28, train loss: 0.028328, train pcmra loss: 0.001524\n",
      "Epoch 29, train loss: 0.027477, train pcmra loss: 0.001492\n",
      "Epoch 30, train loss: 0.027128, train pcmra loss: 0.00146\n",
      "Epoch 30 took 59.73 seconds.\n",
      "Train mask loss: \t 0.02478,     pcmra loss: \t 0.00142,     \tdice loss: \t 0.19533.\n",
      "Eval  mask loss: \t 0.05964,     pcmra loss: \t 0.00241,     \tdice loss: \t 0.33305.\n",
      "New best train loss, saving model.\n",
      "Epoch 31, train loss: 0.026763, train pcmra loss: 0.001447\n",
      "Epoch 32, train loss: 0.026164, train pcmra loss: 0.001403\n",
      "Epoch 33, train loss: 0.026034, train pcmra loss: 0.001362\n",
      "Epoch 34, train loss: 0.025983, train pcmra loss: 0.001417\n",
      "Epoch 35, train loss: 0.025617, train pcmra loss: 0.001418\n",
      "Epoch 35 took 60.22 seconds.\n",
      "Train mask loss: \t 0.02441,     pcmra loss: \t 0.00134,     \tdice loss: \t 0.18504.\n",
      "Eval  mask loss: \t 0.05978,     pcmra loss: \t 0.00228,     \tdice loss: \t 0.33542.\n",
      "New best train loss, saving model.\n",
      "Epoch 36, train loss: 0.024317, train pcmra loss: 0.001351\n",
      "Epoch 37, train loss: 0.024923, train pcmra loss: 0.001327\n",
      "Epoch 38, train loss: 0.023996, train pcmra loss: 0.001287\n",
      "Epoch 39, train loss: 0.024577, train pcmra loss: 0.001312\n",
      "Epoch 40, train loss: 0.023705, train pcmra loss: 0.001269\n",
      "Epoch 40 took 59.6 seconds.\n",
      "Train mask loss: \t 0.02186,     pcmra loss: \t 0.00133,     \tdice loss: \t 0.17057.\n",
      "Eval  mask loss: \t 0.06232,     pcmra loss: \t 0.00236,     \tdice loss: \t 0.33173.\n",
      "New best train loss, saving model.\n",
      "Epoch 41, train loss: 0.023136, train pcmra loss: 0.001254\n",
      "Epoch 42, train loss: 0.023127, train pcmra loss: 0.001238\n",
      "Epoch 43, train loss: 0.023706, train pcmra loss: 0.001269\n",
      "Epoch 44, train loss: 0.022637, train pcmra loss: 0.001218\n",
      "Epoch 45, train loss: 0.022516, train pcmra loss: 0.001232\n",
      "Epoch 45 took 59.47 seconds.\n",
      "Train mask loss: \t 0.02125,     pcmra loss: \t 0.00118,     \tdice loss: \t 0.1736.\n",
      "Eval  mask loss: \t 0.06597,     pcmra loss: \t 0.00232,     \tdice loss: \t 0.33888.\n",
      "New best train loss, saving model.\n",
      "Epoch 46, train loss: 0.021338, train pcmra loss: 0.001158\n",
      "Epoch 47, train loss: 0.022487, train pcmra loss: 0.001206\n",
      "Epoch 48, train loss: 0.021031, train pcmra loss: 0.001138\n",
      "Epoch 49, train loss: 0.021231, train pcmra loss: 0.001141\n",
      "Epoch 50, train loss: 0.021287, train pcmra loss: 0.001175\n",
      "Epoch 50 took 59.56 seconds.\n",
      "Train mask loss: \t 0.02236,     pcmra loss: \t 0.0012,     \tdice loss: \t 0.16781.\n",
      "Eval  mask loss: \t 0.06625,     pcmra loss: \t 0.00236,     \tdice loss: \t 0.33262.\n",
      "New best train loss, saving model.\n",
      "Epoch 51, train loss: 0.020815, train pcmra loss: 0.001135\n",
      "Epoch 52, train loss: 0.020515, train pcmra loss: 0.001075\n",
      "Epoch 53, train loss: 0.020681, train pcmra loss: 0.001128\n",
      "Epoch 54, train loss: 0.019963, train pcmra loss: 0.001061\n",
      "Epoch 55, train loss: 0.020784, train pcmra loss: 0.001105\n",
      "Epoch 55 took 59.04 seconds.\n",
      "Train mask loss: \t 0.02173,     pcmra loss: \t 0.00106,     \tdice loss: \t 0.16224.\n",
      "Eval  mask loss: \t 0.06482,     pcmra loss: \t 0.00233,     \tdice loss: \t 0.33114.\n",
      "New best train loss, saving model.\n",
      "Epoch 56, train loss: 0.020625, train pcmra loss: 0.001121\n",
      "Epoch 57, train loss: 0.019726, train pcmra loss: 0.00104\n",
      "Epoch 58, train loss: 0.0199, train pcmra loss: 0.001083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, train loss: 0.019759, train pcmra loss: 0.001038\n",
      "Epoch 60, train loss: 0.018794, train pcmra loss: 0.00097\n",
      "Epoch 60 took 59.6 seconds.\n",
      "Train mask loss: \t 0.01728,     pcmra loss: \t 0.00099,     \tdice loss: \t 0.12812.\n",
      "Eval  mask loss: \t 0.06702,     pcmra loss: \t 0.00232,     \tdice loss: \t 0.31964.\n",
      "New best train loss, saving model.\n",
      "Epoch 61, train loss: 0.019628, train pcmra loss: 0.001049\n",
      "Epoch 62, train loss: 0.019354, train pcmra loss: 0.001028\n",
      "Epoch 63, train loss: 0.019499, train pcmra loss: 0.001021\n",
      "Epoch 64, train loss: 0.018803, train pcmra loss: 0.001011\n",
      "Epoch 65, train loss: 0.018929, train pcmra loss: 0.000973\n",
      "Epoch 65 took 59.52 seconds.\n",
      "Train mask loss: \t 0.01776,     pcmra loss: \t 0.00094,     \tdice loss: \t 0.13507.\n",
      "Eval  mask loss: \t 0.06727,     pcmra loss: \t 0.00236,     \tdice loss: \t 0.31996.\n",
      "New best train loss, saving model.\n",
      "Epoch 66, train loss: 0.018333, train pcmra loss: 0.000979\n",
      "Epoch 67, train loss: 0.018274, train pcmra loss: 0.000976\n",
      "Epoch 68, train loss: 0.017799, train pcmra loss: 0.000936\n",
      "Epoch 69, train loss: 0.018275, train pcmra loss: 0.000961\n",
      "{'device': 'GPU', 'print_models': False, 'name': '', 'pretrained': None, 'pretrained_best': 'train', 'reconstruction': 'pcmra', 'share_mapping': False, 'pcmra_lambda': 10, 'mask_lambda': 1.0, 'dataset': 'small', 'rotate': True, 'translate': True, 'flip': False, 'norm_min_max': [0, 1], 'seed': 34, 'epochs': 70, 'batch_size': 8, 'eval_every': 5, 'shuffle': True, 'n_coords_sample': 20000, 'cnn_setup': 1, 'mapping_setup': 2, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30.0, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'scheduler_on': 'combined'}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 1596\n",
      "[['16-01-22_Jarik_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27 Valentine kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27_Claudia_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-03_Feiko_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Luuk_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Michelle_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-12_Marjolein_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-03-18_Ruud_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-04-13_Pim_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-05-25_Emile_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)']]\n",
      "Val subjects: 28\n",
      "Test subjects: 28\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "PCMRA_SIREN\n",
      "Epoch 0, train loss: 0.192594, train pcmra loss: 0.004629\n",
      "Epoch 0 took 60.58 seconds.\n",
      "Train mask loss: \t 0.10578,     pcmra loss: \t 0.00344,     \tdice loss: \t 0.99333.\n",
      "Eval  mask loss: \t 0.10931,     pcmra loss: \t 0.00346,     \tdice loss: \t 0.98509.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 1, train loss: 0.089198, train pcmra loss: 0.003157\n",
      "Epoch 2, train loss: 0.072426, train pcmra loss: 0.002935\n",
      "Epoch 3, train loss: 0.065619, train pcmra loss: 0.002806\n",
      "Epoch 4, train loss: 0.060665, train pcmra loss: 0.00265\n",
      "Epoch 5, train loss: 0.056405, train pcmra loss: 0.002536\n",
      "Epoch 5 took 60.72 seconds.\n",
      "Train mask loss: \t 0.05394,     pcmra loss: \t 0.00253,     \tdice loss: \t 0.47193.\n",
      "Eval  mask loss: \t 0.06415,     pcmra loss: \t 0.00257,     \tdice loss: \t 0.52626.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 6, train loss: 0.054206, train pcmra loss: 0.002462\n",
      "Epoch 7, train loss: 0.050827, train pcmra loss: 0.00238\n",
      "Epoch 8, train loss: 0.048877, train pcmra loss: 0.002315\n",
      "Epoch 9, train loss: 0.047993, train pcmra loss: 0.002293\n",
      "Epoch 10, train loss: 0.046291, train pcmra loss: 0.002205\n",
      "Epoch 10 took 60.2 seconds.\n",
      "Train mask loss: \t 0.04333,     pcmra loss: \t 0.00229,     \tdice loss: \t 0.3689.\n",
      "Eval  mask loss: \t 0.05824,     pcmra loss: \t 0.00267,     \tdice loss: \t 0.40654.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 11, train loss: 0.044219, train pcmra loss: 0.002139\n",
      "Epoch 12, train loss: 0.0413, train pcmra loss: 0.002051\n",
      "Epoch 13, train loss: 0.040948, train pcmra loss: 0.002023\n",
      "Epoch 14, train loss: 0.039262, train pcmra loss: 0.001941\n",
      "Epoch 15, train loss: 0.038961, train pcmra loss: 0.001892\n",
      "Epoch 15 took 60.01 seconds.\n",
      "Train mask loss: \t 0.03863,     pcmra loss: \t 0.00191,     \tdice loss: \t 0.31986.\n",
      "Eval  mask loss: \t 0.05696,     pcmra loss: \t 0.00235,     \tdice loss: \t 0.38119.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 16, train loss: 0.038371, train pcmra loss: 0.001964\n",
      "Epoch 17, train loss: 0.036432, train pcmra loss: 0.001879\n",
      "Epoch 18, train loss: 0.034747, train pcmra loss: 0.001782\n",
      "Epoch 19, train loss: 0.034538, train pcmra loss: 0.001751\n",
      "Epoch 20, train loss: 0.033654, train pcmra loss: 0.001735\n",
      "Epoch 20 took 59.67 seconds.\n",
      "Train mask loss: \t 0.03141,     pcmra loss: \t 0.00162,     \tdice loss: \t 0.24452.\n",
      "Eval  mask loss: \t 0.05472,     pcmra loss: \t 0.00222,     \tdice loss: \t 0.35476.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 21, train loss: 0.03323, train pcmra loss: 0.001699\n",
      "Epoch 22, train loss: 0.033348, train pcmra loss: 0.001744\n",
      "Epoch 23, train loss: 0.031144, train pcmra loss: 0.001613\n",
      "Epoch 24, train loss: 0.029687, train pcmra loss: 0.001547\n",
      "Epoch 25, train loss: 0.029879, train pcmra loss: 0.001568\n",
      "Epoch 25 took 59.91 seconds.\n",
      "Train mask loss: \t 0.03107,     pcmra loss: \t 0.0016,     \tdice loss: \t 0.22718.\n",
      "Eval  mask loss: \t 0.05846,     pcmra loss: \t 0.00228,     \tdice loss: \t 0.35281.\n",
      "New best train loss, saving model.\n",
      "Epoch 26, train loss: 0.029853, train pcmra loss: 0.001589\n",
      "Epoch 27, train loss: 0.028503, train pcmra loss: 0.0015\n",
      "Epoch 28, train loss: 0.027657, train pcmra loss: 0.001475\n",
      "Epoch 29, train loss: 0.027716, train pcmra loss: 0.001456\n",
      "Epoch 30, train loss: 0.027375, train pcmra loss: 0.001437\n",
      "Epoch 30 took 59.74 seconds.\n",
      "Train mask loss: \t 0.02664,     pcmra loss: \t 0.00154,     \tdice loss: \t 0.20325.\n",
      "Eval  mask loss: \t 0.06289,     pcmra loss: \t 0.00234,     \tdice loss: \t 0.35631.\n",
      "New best train loss, saving model.\n",
      "Epoch 31, train loss: 0.027011, train pcmra loss: 0.001446\n",
      "Epoch 32, train loss: 0.025479, train pcmra loss: 0.001358\n",
      "Epoch 33, train loss: 0.0258, train pcmra loss: 0.001359\n",
      "Epoch 34, train loss: 0.026077, train pcmra loss: 0.001393\n",
      "Epoch 35, train loss: 0.02515, train pcmra loss: 0.001331\n",
      "Epoch 35 took 59.72 seconds.\n",
      "Train mask loss: \t 0.02343,     pcmra loss: \t 0.00129,     \tdice loss: \t 0.17465.\n",
      "Eval  mask loss: \t 0.06329,     pcmra loss: \t 0.00226,     \tdice loss: \t 0.34517.\n",
      "New best train loss, saving model.\n",
      "Epoch 36, train loss: 0.024247, train pcmra loss: 0.001288\n",
      "Epoch 37, train loss: 0.023969, train pcmra loss: 0.001286\n",
      "Epoch 38, train loss: 0.023903, train pcmra loss: 0.001268\n",
      "Epoch 39, train loss: 0.023523, train pcmra loss: 0.001225\n",
      "Epoch 40, train loss: 0.023481, train pcmra loss: 0.001248\n",
      "Epoch 40 took 60.04 seconds.\n",
      "Train mask loss: \t 0.02407,     pcmra loss: \t 0.00125,     \tdice loss: \t 0.18464.\n",
      "Eval  mask loss: \t 0.06455,     pcmra loss: \t 0.00237,     \tdice loss: \t 0.34374.\n",
      "New best train loss, saving model.\n",
      "Epoch 41, train loss: 0.023029, train pcmra loss: 0.00124\n",
      "Epoch 42, train loss: 0.022828, train pcmra loss: 0.001208\n",
      "Epoch 43, train loss: 0.022214, train pcmra loss: 0.001172\n",
      "Epoch 44, train loss: 0.022472, train pcmra loss: 0.001218\n",
      "Epoch 45, train loss: 0.022475, train pcmra loss: 0.001203\n",
      "Epoch 45 took 59.65 seconds.\n",
      "Train mask loss: \t 0.02259,     pcmra loss: \t 0.0012,     \tdice loss: \t 0.16794.\n",
      "Eval  mask loss: \t 0.0598,     pcmra loss: \t 0.00226,     \tdice loss: \t 0.32331.\n",
      "New best train loss, saving model.\n",
      "Epoch 46, train loss: 0.021671, train pcmra loss: 0.001174\n",
      "Epoch 47, train loss: 0.021669, train pcmra loss: 0.001154\n",
      "Epoch 48, train loss: 0.020855, train pcmra loss: 0.001126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, train loss: 0.021031, train pcmra loss: 0.001122\n",
      "Epoch 50, train loss: 0.01992, train pcmra loss: 0.001061\n",
      "Epoch 50 took 60.35 seconds.\n",
      "Train mask loss: \t 0.01958,     pcmra loss: \t 0.00101,     \tdice loss: \t 0.14972.\n",
      "Eval  mask loss: \t 0.06692,     pcmra loss: \t 0.00229,     \tdice loss: \t 0.33619.\n",
      "New best train loss, saving model.\n",
      "Epoch 51, train loss: 0.02042, train pcmra loss: 0.001082\n",
      "Epoch 52, train loss: 0.02001, train pcmra loss: 0.001057\n",
      "Epoch 53, train loss: 0.020023, train pcmra loss: 0.001043\n",
      "Epoch 54, train loss: 0.020363, train pcmra loss: 0.001049\n",
      "Epoch 55, train loss: 0.019708, train pcmra loss: 0.001048\n",
      "Epoch 55 took 60.67 seconds.\n",
      "Train mask loss: \t 0.02166,     pcmra loss: \t 0.00099,     \tdice loss: \t 0.16818.\n",
      "Eval  mask loss: \t 0.06929,     pcmra loss: \t 0.00242,     \tdice loss: \t 0.34777.\n",
      "New best train loss, saving model.\n",
      "Epoch 56, train loss: 0.019517, train pcmra loss: 0.001017\n",
      "Epoch 57, train loss: 0.019659, train pcmra loss: 0.001032\n",
      "Epoch 58, train loss: 0.019002, train pcmra loss: 0.000991\n",
      "Epoch 59, train loss: 0.019091, train pcmra loss: 0.001001\n",
      "Epoch 60, train loss: 0.019699, train pcmra loss: 0.001039\n",
      "Epoch 60 took 58.99 seconds.\n",
      "Train mask loss: \t 0.01765,     pcmra loss: \t 0.00096,     \tdice loss: \t 0.13743.\n",
      "Eval  mask loss: \t 0.06638,     pcmra loss: \t 0.00236,     \tdice loss: \t 0.31704.\n",
      "New best train loss, saving model.\n",
      "Epoch 61, train loss: 0.0189, train pcmra loss: 0.001003\n",
      "Epoch 62, train loss: 0.018591, train pcmra loss: 0.000978\n",
      "Epoch 63, train loss: 0.019, train pcmra loss: 0.000984\n",
      "Epoch 64, train loss: 0.018659, train pcmra loss: 0.00097\n",
      "Epoch 65, train loss: 0.018137, train pcmra loss: 0.000934\n",
      "Epoch 65 took 60.29 seconds.\n",
      "Train mask loss: \t 0.01804,     pcmra loss: \t 0.00099,     \tdice loss: \t 0.14544.\n",
      "Eval  mask loss: \t 0.06832,     pcmra loss: \t 0.00234,     \tdice loss: \t 0.33212.\n",
      "New best train loss, saving model.\n",
      "Epoch 66, train loss: 0.018262, train pcmra loss: 0.000937\n",
      "Epoch 67, train loss: 0.018303, train pcmra loss: 0.000956\n",
      "Epoch 68, train loss: 0.018216, train pcmra loss: 0.000954\n",
      "Epoch 69, train loss: 0.017859, train pcmra loss: 0.000927\n",
      "{'device': 'GPU', 'print_models': False, 'name': '', 'pretrained': None, 'pretrained_best': 'train', 'reconstruction': 'pcmra', 'share_mapping': False, 'pcmra_lambda': 5, 'mask_lambda': 1.0, 'dataset': 'small', 'rotate': True, 'translate': True, 'flip': False, 'norm_min_max': [0, 1], 'seed': 34, 'epochs': 70, 'batch_size': 24, 'eval_every': 5, 'shuffle': True, 'n_coords_sample': 5000, 'cnn_setup': 1, 'mapping_setup': 2, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30.0, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'scheduler_on': 'combined'}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 1596\n",
      "[['16-01-22_Jarik_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27 Valentine kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27_Claudia_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-03_Feiko_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Luuk_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Michelle_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-12_Marjolein_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-03-18_Ruud_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-04-13_Pim_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-05-25_Emile_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)']]\n",
      "Val subjects: 28\n",
      "Test subjects: 28\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "PCMRA_SIREN\n",
      "Epoch 0, train loss: 0.299101, train pcmra loss: 0.006018\n",
      "Epoch 0 took 40.55 seconds.\n",
      "Train mask loss: \t 0.17383,     pcmra loss: \t 0.00432,     \tdice loss: \t 1.0.\n",
      "Eval  mask loss: \t 0.17947,     pcmra loss: \t 0.00369,     \tdice loss: \t 1.0.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 1, train loss: 0.142448, train pcmra loss: 0.003724\n",
      "Epoch 2, train loss: 0.104295, train pcmra loss: 0.003163\n",
      "Epoch 3, train loss: 0.085917, train pcmra loss: 0.002927\n",
      "Epoch 4, train loss: 0.075306, train pcmra loss: 0.002848\n",
      "Epoch 5, train loss: 0.068003, train pcmra loss: 0.002667\n",
      "Epoch 5 took 40.76 seconds.\n",
      "Train mask loss: \t 0.06553,     pcmra loss: \t 0.00254,     \tdice loss: \t 0.46326.\n",
      "Eval  mask loss: \t 0.07103,     pcmra loss: \t 0.00261,     \tdice loss: \t 0.48412.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 6, train loss: 0.061632, train pcmra loss: 0.002535\n",
      "Epoch 7, train loss: 0.058291, train pcmra loss: 0.002534\n",
      "Epoch 8, train loss: 0.054835, train pcmra loss: 0.002371\n",
      "Epoch 9, train loss: 0.052656, train pcmra loss: 0.002316\n",
      "Epoch 10, train loss: 0.050568, train pcmra loss: 0.00228\n",
      "Epoch 10 took 40.65 seconds.\n",
      "Train mask loss: \t 0.04902,     pcmra loss: \t 0.00219,     \tdice loss: \t 0.39642.\n",
      "Eval  mask loss: \t 0.06623,     pcmra loss: \t 0.00228,     \tdice loss: \t 0.48998.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 11, train loss: 0.048686, train pcmra loss: 0.002175\n",
      "Epoch 12, train loss: 0.045687, train pcmra loss: 0.002114\n",
      "Epoch 13, train loss: 0.043136, train pcmra loss: 0.002061\n",
      "Epoch 14, train loss: 0.043403, train pcmra loss: 0.002068\n",
      "Epoch 15, train loss: 0.042146, train pcmra loss: 0.002037\n",
      "Epoch 15 took 40.75 seconds.\n",
      "Train mask loss: \t 0.04032,     pcmra loss: \t 0.00199,     \tdice loss: \t 0.32919.\n",
      "Eval  mask loss: \t 0.05651,     pcmra loss: \t 0.0022,     \tdice loss: \t 0.43292.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 16, train loss: 0.039248, train pcmra loss: 0.001913\n",
      "Epoch 17, train loss: 0.039231, train pcmra loss: 0.001939\n",
      "Epoch 18, train loss: 0.038605, train pcmra loss: 0.00186\n",
      "Epoch 19, train loss: 0.038372, train pcmra loss: 0.001905\n",
      "Epoch 20, train loss: 0.035606, train pcmra loss: 0.001778\n",
      "Epoch 20 took 41.31 seconds.\n",
      "Train mask loss: \t 0.03811,     pcmra loss: \t 0.00203,     \tdice loss: \t 0.28136.\n",
      "Eval  mask loss: \t 0.0598,     pcmra loss: \t 0.00228,     \tdice loss: \t 0.37769.\n",
      "New best train loss, saving model.\n",
      "Epoch 21, train loss: 0.035495, train pcmra loss: 0.001778\n",
      "Epoch 22, train loss: 0.035291, train pcmra loss: 0.001766\n",
      "Epoch 23, train loss: 0.034316, train pcmra loss: 0.001764\n",
      "Epoch 24, train loss: 0.032868, train pcmra loss: 0.001694\n",
      "Epoch 25, train loss: 0.032922, train pcmra loss: 0.001712\n",
      "Epoch 25 took 41.03 seconds.\n",
      "Train mask loss: \t 0.03143,     pcmra loss: \t 0.00161,     \tdice loss: \t 0.23156.\n",
      "Eval  mask loss: \t 0.05883,     pcmra loss: \t 0.00218,     \tdice loss: \t 0.35772.\n",
      "New best train loss, saving model.\n",
      "Epoch 26, train loss: 0.031227, train pcmra loss: 0.001636\n",
      "Epoch 27, train loss: 0.031126, train pcmra loss: 0.001567\n",
      "Epoch 28, train loss: 0.030716, train pcmra loss: 0.001606\n",
      "Epoch 29, train loss: 0.030877, train pcmra loss: 0.00159\n",
      "Epoch 30, train loss: 0.029886, train pcmra loss: 0.001554\n",
      "Epoch 30 took 40.78 seconds.\n",
      "Train mask loss: \t 0.02889,     pcmra loss: \t 0.00157,     \tdice loss: \t 0.22119.\n",
      "Eval  mask loss: \t 0.05986,     pcmra loss: \t 0.0023,     \tdice loss: \t 0.3695.\n",
      "New best train loss, saving model.\n",
      "Epoch 31, train loss: 0.029049, train pcmra loss: 0.001556\n",
      "Epoch 32, train loss: 0.028885, train pcmra loss: 0.001499\n",
      "Epoch 33, train loss: 0.028026, train pcmra loss: 0.001502\n",
      "Epoch 34, train loss: 0.028185, train pcmra loss: 0.001467\n",
      "Epoch 35, train loss: 0.027805, train pcmra loss: 0.001472\n",
      "Epoch 35 took 40.79 seconds.\n",
      "Train mask loss: \t 0.02618,     pcmra loss: \t 0.00155,     \tdice loss: \t 0.19972.\n",
      "Eval  mask loss: \t 0.05515,     pcmra loss: \t 0.00212,     \tdice loss: \t 0.34158.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 36, train loss: 0.027399, train pcmra loss: 0.001457\n",
      "Epoch 37, train loss: 0.026314, train pcmra loss: 0.001426\n",
      "Epoch 38, train loss: 0.027024, train pcmra loss: 0.00145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, train loss: 0.026106, train pcmra loss: 0.001367\n",
      "Epoch 40, train loss: 0.02588, train pcmra loss: 0.00134\n",
      "Epoch 40 took 40.73 seconds.\n",
      "Train mask loss: \t 0.02495,     pcmra loss: \t 0.00144,     \tdice loss: \t 0.18983.\n",
      "Eval  mask loss: \t 0.0631,     pcmra loss: \t 0.00218,     \tdice loss: \t 0.35214.\n",
      "New best train loss, saving model.\n",
      "Epoch 41, train loss: 0.025245, train pcmra loss: 0.00135\n",
      "Epoch 42, train loss: 0.024614, train pcmra loss: 0.001322\n",
      "Epoch 43, train loss: 0.025127, train pcmra loss: 0.001332\n",
      "Epoch 44, train loss: 0.025862, train pcmra loss: 0.00138\n",
      "Epoch 45, train loss: 0.025018, train pcmra loss: 0.001348\n",
      "Epoch 45 took 40.92 seconds.\n",
      "Train mask loss: \t 0.02492,     pcmra loss: \t 0.00125,     \tdice loss: \t 0.18463.\n",
      "Eval  mask loss: \t 0.06198,     pcmra loss: \t 0.00214,     \tdice loss: \t 0.34358.\n",
      "New best train loss, saving model.\n",
      "Epoch 46, train loss: 0.024255, train pcmra loss: 0.001279\n",
      "Epoch 47, train loss: 0.023569, train pcmra loss: 0.001218\n",
      "Epoch 48, train loss: 0.02311, train pcmra loss: 0.001233\n",
      "Epoch 49, train loss: 0.022742, train pcmra loss: 0.001224\n",
      "Epoch 50, train loss: 0.022635, train pcmra loss: 0.001194\n",
      "Epoch 50 took 40.78 seconds.\n",
      "Train mask loss: \t 0.02224,     pcmra loss: \t 0.0013,     \tdice loss: \t 0.18499.\n",
      "Eval  mask loss: \t 0.05995,     pcmra loss: \t 0.00211,     \tdice loss: \t 0.3433.\n",
      "New best train loss, saving model.\n",
      "Epoch 51, train loss: 0.023424, train pcmra loss: 0.001239\n",
      "Epoch 52, train loss: 0.022678, train pcmra loss: 0.001189\n",
      "Epoch 53, train loss: 0.022516, train pcmra loss: 0.001216\n",
      "Epoch 54, train loss: 0.02203, train pcmra loss: 0.00118\n",
      "Epoch 55, train loss: 0.021933, train pcmra loss: 0.001159\n",
      "Epoch 55 took 40.84 seconds.\n",
      "Train mask loss: \t 0.02233,     pcmra loss: \t 0.00118,     \tdice loss: \t 0.16636.\n",
      "Eval  mask loss: \t 0.05839,     pcmra loss: \t 0.0021,     \tdice loss: \t 0.32194.\n",
      "New best train loss, saving model.\n",
      "Epoch 56, train loss: 0.022116, train pcmra loss: 0.001163\n",
      "Epoch 57, train loss: 0.022785, train pcmra loss: 0.001194\n",
      "Epoch 58, train loss: 0.021683, train pcmra loss: 0.001161\n",
      "Epoch 59, train loss: 0.021503, train pcmra loss: 0.001163\n",
      "Epoch 60, train loss: 0.022332, train pcmra loss: 0.001142\n",
      "Epoch 60 took 41.04 seconds.\n",
      "Train mask loss: \t 0.02144,     pcmra loss: \t 0.00118,     \tdice loss: \t 0.15735.\n",
      "Eval  mask loss: \t 0.06077,     pcmra loss: \t 0.00213,     \tdice loss: \t 0.31505.\n",
      "New best train loss, saving model.\n",
      "Epoch 61, train loss: 0.021764, train pcmra loss: 0.00114\n",
      "Epoch 62, train loss: 0.021065, train pcmra loss: 0.001105\n",
      "Epoch 63, train loss: 0.020518, train pcmra loss: 0.001077\n",
      "Epoch 64, train loss: 0.023032, train pcmra loss: 0.001244\n",
      "Epoch 65, train loss: 0.021152, train pcmra loss: 0.001115\n",
      "Epoch 65 took 40.98 seconds.\n",
      "Train mask loss: \t 0.02081,     pcmra loss: \t 0.00112,     \tdice loss: \t 0.17248.\n",
      "Eval  mask loss: \t 0.06418,     pcmra loss: \t 0.00212,     \tdice loss: \t 0.34175.\n",
      "New best train loss, saving model.\n",
      "Epoch 66, train loss: 0.020855, train pcmra loss: 0.001067\n",
      "Epoch 67, train loss: 0.021206, train pcmra loss: 0.001098\n",
      "Epoch 68, train loss: 0.021055, train pcmra loss: 0.001104\n",
      "Epoch 69, train loss: 0.020178, train pcmra loss: 0.001058\n",
      "{'device': 'GPU', 'print_models': False, 'name': '', 'pretrained': None, 'pretrained_best': 'train', 'reconstruction': 'pcmra', 'share_mapping': False, 'pcmra_lambda': 10, 'mask_lambda': 1.0, 'dataset': 'small', 'rotate': True, 'translate': True, 'flip': False, 'norm_min_max': [0, 1], 'seed': 34, 'epochs': 70, 'batch_size': 24, 'eval_every': 5, 'shuffle': True, 'n_coords_sample': 5000, 'cnn_setup': 1, 'mapping_setup': 2, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30.0, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'scheduler_on': 'combined'}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 1596\n",
      "[['16-01-22_Jarik_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27 Valentine kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27_Claudia_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-03_Feiko_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Luuk_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Michelle_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-12_Marjolein_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-03-18_Ruud_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-04-13_Pim_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-05-25_Emile_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)']]\n",
      "Val subjects: 28\n",
      "Test subjects: 28\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "PCMRA_SIREN\n",
      "Epoch 0, train loss: 0.29519, train pcmra loss: 0.005928\n",
      "Epoch 0 took 42.81 seconds.\n",
      "Train mask loss: \t 0.17243,     pcmra loss: \t 0.0047,     \tdice loss: \t 1.0.\n",
      "Eval  mask loss: \t 0.17795,     pcmra loss: \t 0.00468,     \tdice loss: \t 1.0.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 1, train loss: 0.147481, train pcmra loss: 0.003778\n",
      "Epoch 2, train loss: 0.106182, train pcmra loss: 0.003155\n",
      "Epoch 3, train loss: 0.08673, train pcmra loss: 0.002945\n",
      "Epoch 4, train loss: 0.075862, train pcmra loss: 0.002789\n",
      "Epoch 5, train loss: 0.067719, train pcmra loss: 0.002616\n",
      "Epoch 5 took 40.71 seconds.\n",
      "Train mask loss: \t 0.06221,     pcmra loss: \t 0.00257,     \tdice loss: \t 0.52574.\n",
      "Eval  mask loss: \t 0.07152,     pcmra loss: \t 0.00252,     \tdice loss: \t 0.61329.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 6, train loss: 0.061689, train pcmra loss: 0.002535\n",
      "Epoch 7, train loss: 0.057707, train pcmra loss: 0.002481\n",
      "Epoch 8, train loss: 0.054165, train pcmra loss: 0.002344\n",
      "Epoch 9, train loss: 0.052304, train pcmra loss: 0.002262\n",
      "Epoch 10, train loss: 0.049892, train pcmra loss: 0.002236\n",
      "Epoch 10 took 40.76 seconds.\n",
      "Train mask loss: \t 0.04896,     pcmra loss: \t 0.00215,     \tdice loss: \t 0.40692.\n",
      "Eval  mask loss: \t 0.06634,     pcmra loss: \t 0.00225,     \tdice loss: \t 0.50293.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 11, train loss: 0.047416, train pcmra loss: 0.002201\n",
      "Epoch 12, train loss: 0.045733, train pcmra loss: 0.002111\n",
      "Epoch 13, train loss: 0.042941, train pcmra loss: 0.002007\n",
      "Epoch 14, train loss: 0.042486, train pcmra loss: 0.002034\n",
      "Epoch 15, train loss: 0.040451, train pcmra loss: 0.001973\n",
      "Epoch 15 took 40.79 seconds.\n",
      "Train mask loss: \t 0.03888,     pcmra loss: \t 0.00197,     \tdice loss: \t 0.28974.\n",
      "Eval  mask loss: \t 0.05447,     pcmra loss: \t 0.0022,     \tdice loss: \t 0.39108.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 16, train loss: 0.039738, train pcmra loss: 0.00192\n",
      "Epoch 17, train loss: 0.037882, train pcmra loss: 0.001872\n",
      "Epoch 18, train loss: 0.038722, train pcmra loss: 0.001894\n",
      "Epoch 19, train loss: 0.037222, train pcmra loss: 0.001854\n",
      "Epoch 20, train loss: 0.035353, train pcmra loss: 0.001781\n",
      "Epoch 20 took 40.83 seconds.\n",
      "Train mask loss: \t 0.03403,     pcmra loss: \t 0.00165,     \tdice loss: \t 0.26376.\n",
      "Eval  mask loss: \t 0.05567,     pcmra loss: \t 0.00211,     \tdice loss: \t 0.3702.\n",
      "New best train loss, saving model.\n",
      "Epoch 21, train loss: 0.035145, train pcmra loss: 0.001754\n",
      "Epoch 22, train loss: 0.033784, train pcmra loss: 0.001742\n",
      "Epoch 23, train loss: 0.033439, train pcmra loss: 0.001704\n",
      "Epoch 24, train loss: 0.033268, train pcmra loss: 0.001689\n",
      "Epoch 25, train loss: 0.031223, train pcmra loss: 0.001632\n",
      "Epoch 25 took 40.77 seconds.\n",
      "Train mask loss: \t 0.03008,     pcmra loss: \t 0.00171,     \tdice loss: \t 0.24517.\n",
      "Eval  mask loss: \t 0.05656,     pcmra loss: \t 0.0021,     \tdice loss: \t 0.37265.\n",
      "New best train loss, saving model.\n",
      "Epoch 26, train loss: 0.031583, train pcmra loss: 0.001677\n",
      "Epoch 27, train loss: 0.030823, train pcmra loss: 0.001581\n",
      "Epoch 28, train loss: 0.030373, train pcmra loss: 0.001588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, train loss: 0.030631, train pcmra loss: 0.001563\n",
      "Epoch 30, train loss: 0.029297, train pcmra loss: 0.00154\n",
      "Epoch 30 took 40.65 seconds.\n",
      "Train mask loss: \t 0.02824,     pcmra loss: \t 0.00157,     \tdice loss: \t 0.2172.\n",
      "Eval  mask loss: \t 0.05789,     pcmra loss: \t 0.00203,     \tdice loss: \t 0.36193.\n",
      "New best train loss, saving model.\n",
      "Epoch 31, train loss: 0.028544, train pcmra loss: 0.001506\n",
      "Epoch 32, train loss: 0.029808, train pcmra loss: 0.001602\n",
      "Epoch 33, train loss: 0.027441, train pcmra loss: 0.001497\n",
      "Epoch 34, train loss: 0.027723, train pcmra loss: 0.001467\n",
      "Epoch 35, train loss: 0.027129, train pcmra loss: 0.001437\n",
      "Epoch 35 took 40.33 seconds.\n",
      "Train mask loss: \t 0.02864,     pcmra loss: \t 0.00158,     \tdice loss: \t 0.21852.\n",
      "Eval  mask loss: \t 0.06117,     pcmra loss: \t 0.00205,     \tdice loss: \t 0.34503.\n",
      "New best train loss, saving model.\n",
      "Epoch 36, train loss: 0.028665, train pcmra loss: 0.001501\n",
      "Epoch 37, train loss: 0.026064, train pcmra loss: 0.001395\n",
      "Epoch 38, train loss: 0.026057, train pcmra loss: 0.001412\n",
      "Epoch 39, train loss: 0.025403, train pcmra loss: 0.001371\n",
      "Epoch 40, train loss: 0.025758, train pcmra loss: 0.001393\n",
      "Epoch 40 took 40.87 seconds.\n",
      "Train mask loss: \t 0.02418,     pcmra loss: \t 0.00139,     \tdice loss: \t 0.17645.\n",
      "Eval  mask loss: \t 0.05862,     pcmra loss: \t 0.00205,     \tdice loss: \t 0.34059.\n",
      "New best train loss, saving model.\n",
      "Epoch 41, train loss: 0.025759, train pcmra loss: 0.001396\n",
      "Epoch 42, train loss: 0.024466, train pcmra loss: 0.001296\n",
      "Epoch 43, train loss: 0.024889, train pcmra loss: 0.001321\n",
      "Epoch 44, train loss: 0.024078, train pcmra loss: 0.001295\n",
      "Epoch 45, train loss: 0.025655, train pcmra loss: 0.001356\n",
      "Epoch 45 took 40.83 seconds.\n",
      "Train mask loss: \t 0.02305,     pcmra loss: \t 0.00125,     \tdice loss: \t 0.17589.\n",
      "Eval  mask loss: \t 0.0569,     pcmra loss: \t 0.00203,     \tdice loss: \t 0.323.\n",
      "New best train loss, saving model.\n",
      "Epoch 46, train loss: 0.023588, train pcmra loss: 0.001245\n",
      "Epoch 47, train loss: 0.023525, train pcmra loss: 0.001251\n",
      "Epoch 48, train loss: 0.023712, train pcmra loss: 0.001254\n",
      "Epoch 49, train loss: 0.023656, train pcmra loss: 0.001241\n",
      "Epoch 50, train loss: 0.022636, train pcmra loss: 0.001225\n",
      "Epoch 50 took 40.98 seconds.\n",
      "Train mask loss: \t 0.0233,     pcmra loss: \t 0.00129,     \tdice loss: \t 0.17745.\n",
      "Eval  mask loss: \t 0.06542,     pcmra loss: \t 0.00209,     \tdice loss: \t 0.34827.\n",
      "New best train loss, saving model.\n",
      "Epoch 51, train loss: 0.023631, train pcmra loss: 0.001261\n",
      "Epoch 52, train loss: 0.022626, train pcmra loss: 0.001242\n",
      "Epoch 53, train loss: 0.022512, train pcmra loss: 0.001196\n",
      "Epoch 54, train loss: 0.022413, train pcmra loss: 0.001209\n",
      "Epoch 55, train loss: 0.023654, train pcmra loss: 0.001268\n",
      "Epoch 55 took 40.6 seconds.\n",
      "Train mask loss: \t 0.02369,     pcmra loss: \t 0.00133,     \tdice loss: \t 0.1816.\n",
      "Eval  mask loss: \t 0.05873,     pcmra loss: \t 0.00212,     \tdice loss: \t 0.32902.\n",
      "New best train loss, saving model.\n",
      "Epoch 56, train loss: 0.02295, train pcmra loss: 0.001218\n",
      "Epoch 57, train loss: 0.021504, train pcmra loss: 0.001145\n",
      "Epoch 58, train loss: 0.021607, train pcmra loss: 0.001146\n",
      "Epoch 59, train loss: 0.022864, train pcmra loss: 0.001168\n",
      "Epoch 60, train loss: 0.021294, train pcmra loss: 0.001126\n",
      "Epoch 60 took 40.85 seconds.\n",
      "Train mask loss: \t 0.02123,     pcmra loss: \t 0.00119,     \tdice loss: \t 0.15577.\n",
      "Eval  mask loss: \t 0.06828,     pcmra loss: \t 0.0021,     \tdice loss: \t 0.3396.\n",
      "New best train loss, saving model.\n",
      "Epoch 61, train loss: 0.021255, train pcmra loss: 0.001151\n",
      "Epoch 62, train loss: 0.021149, train pcmra loss: 0.001127\n",
      "Epoch 63, train loss: 0.021729, train pcmra loss: 0.001147\n",
      "Epoch 64, train loss: 0.021029, train pcmra loss: 0.001127\n",
      "Epoch 65, train loss: 0.020652, train pcmra loss: 0.001087\n",
      "Epoch 65 took 40.72 seconds.\n",
      "Train mask loss: \t 0.02222,     pcmra loss: \t 0.00113,     \tdice loss: \t 0.1711.\n",
      "Eval  mask loss: \t 0.06294,     pcmra loss: \t 0.00207,     \tdice loss: \t 0.33712.\n",
      "New best train loss, saving model.\n",
      "Epoch 66, train loss: 0.021337, train pcmra loss: 0.001131\n",
      "Epoch 67, train loss: 0.020748, train pcmra loss: 0.001074\n",
      "Epoch 68, train loss: 0.020644, train pcmra loss: 0.001091\n",
      "Epoch 69, train loss: 0.020207, train pcmra loss: 0.001082\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()    \n",
    "\n",
    "ARGS = init_ARGS()\n",
    "\n",
    "ARGS.n_coords_sample, ARGS.batch_size = -1, 2\n",
    "ARGS.pcmra_lambda = 10\n",
    "ARGS.epochs = 70\n",
    "ARGS.print_models = False\n",
    "ARGS.rotate, ARGS.translate, ARGS.flip = True, True, False\n",
    "\n",
    "\n",
    "ARGS.scheduler_on = \"combined\"\n",
    "ARGS.reconstruction = \"pcmra\"\n",
    "\n",
    "ARGS.share_mapping = False\n",
    "\n",
    "ARGS.cnn_setup = 1\n",
    "ARGS.mapping_setup = 2\n",
    "\n",
    "print(vars(ARGS))\n",
    "\n",
    "train()  \n",
    "\n",
    "torch.cuda.empty_cache()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ARGS class initialized.\n",
      "{'device': 'GPU', 'print_models': False, 'name': '', 'pretrained': None, 'pretrained_best': 'train', 'reconstruction': 'mask', 'share_mapping': False, 'pcmra_lambda': 5, 'mask_lambda': 1.0, 'dataset': 'small', 'rotate': True, 'translate': True, 'flip': False, 'norm_min_max': [0, 1], 'seed': 34, 'epochs': 70, 'batch_size': 8, 'eval_every': 5, 'shuffle': True, 'n_coords_sample': 20000, 'cnn_setup': 1, 'mapping_setup': 2, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30.0, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'scheduler_on': 'combined'}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 1596\n",
      "[['16-01-22_Jarik_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27 Valentine kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27_Claudia_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-03_Feiko_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Luuk_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Michelle_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-12_Marjolein_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-03-18_Ruud_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-04-13_Pim_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-05-25_Emile_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)']]\n",
      "Val subjects: 28\n",
      "Test subjects: 28\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "Epoch 0, train loss: 0.187985, train pcmra loss: 0.0\n",
      "Epoch 0 took 42.53 seconds.\n",
      "Train mask loss: \t 0.10934,     pcmra loss: \t 0.0,     \tdice loss: \t 1.0.\n",
      "Eval  mask loss: \t 0.11114,     pcmra loss: \t 0.0,     \tdice loss: \t 1.0.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 1, train loss: 0.090025, train pcmra loss: 0.0\n",
      "Epoch 2, train loss: 0.071932, train pcmra loss: 0.0\n",
      "Epoch 3, train loss: 0.061429, train pcmra loss: 0.0\n",
      "Epoch 4, train loss: 0.054337, train pcmra loss: 0.0\n",
      "Epoch 5, train loss: 0.049784, train pcmra loss: 0.0\n",
      "Epoch 5 took 42.77 seconds.\n",
      "Train mask loss: \t 0.04984,     pcmra loss: \t 0.0,     \tdice loss: \t 0.40278.\n",
      "Eval  mask loss: \t 0.0533,     pcmra loss: \t 0.0,     \tdice loss: \t 0.42762.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 6, train loss: 0.046931, train pcmra loss: 0.0\n",
      "Epoch 7, train loss: 0.045293, train pcmra loss: 0.0\n",
      "Epoch 8, train loss: 0.042872, train pcmra loss: 0.0\n",
      "Epoch 9, train loss: 0.04179, train pcmra loss: 0.0\n",
      "Epoch 10, train loss: 0.038722, train pcmra loss: 0.0\n",
      "Epoch 10 took 42.36 seconds.\n",
      "Train mask loss: \t 0.03807,     pcmra loss: \t 0.0,     \tdice loss: \t 0.31078.\n",
      "Eval  mask loss: \t 0.0446,     pcmra loss: \t 0.0,     \tdice loss: \t 0.33053.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 11, train loss: 0.037776, train pcmra loss: 0.0\n",
      "Epoch 12, train loss: 0.036676, train pcmra loss: 0.0\n",
      "Epoch 13, train loss: 0.035619, train pcmra loss: 0.0\n",
      "Epoch 14, train loss: 0.036201, train pcmra loss: 0.0\n",
      "Epoch 15, train loss: 0.034218, train pcmra loss: 0.0\n",
      "Epoch 15 took 42.09 seconds.\n",
      "Train mask loss: \t 0.03384,     pcmra loss: \t 0.0,     \tdice loss: \t 0.26039.\n",
      "Eval  mask loss: \t 0.04617,     pcmra loss: \t 0.0,     \tdice loss: \t 0.30286.\n",
      "New best train loss, saving model.\n",
      "Epoch 16, train loss: 0.033088, train pcmra loss: 0.0\n",
      "Epoch 17, train loss: 0.032117, train pcmra loss: 0.0\n",
      "Epoch 18, train loss: 0.031906, train pcmra loss: 0.0\n",
      "Epoch 19, train loss: 0.031157, train pcmra loss: 0.0\n",
      "Epoch 20, train loss: 0.030283, train pcmra loss: 0.0\n",
      "Epoch 20 took 43.33 seconds.\n",
      "Train mask loss: \t 0.0317,     pcmra loss: \t 0.0,     \tdice loss: \t 0.23674.\n",
      "Eval  mask loss: \t 0.0447,     pcmra loss: \t 0.0,     \tdice loss: \t 0.28776.\n",
      "New best train loss, saving model.\n",
      "Epoch 21, train loss: 0.029566, train pcmra loss: 0.0\n",
      "Epoch 22, train loss: 0.029842, train pcmra loss: 0.0\n",
      "Epoch 23, train loss: 0.028649, train pcmra loss: 0.0\n",
      "Epoch 24, train loss: 0.028395, train pcmra loss: 0.0\n",
      "Epoch 25, train loss: 0.028191, train pcmra loss: 0.0\n",
      "Epoch 25 took 42.86 seconds.\n",
      "Train mask loss: \t 0.03157,     pcmra loss: \t 0.0,     \tdice loss: \t 0.24494.\n",
      "Eval  mask loss: \t 0.04893,     pcmra loss: \t 0.0,     \tdice loss: \t 0.3116.\n",
      "New best train loss, saving model.\n",
      "Epoch 26, train loss: 0.027979, train pcmra loss: 0.0\n",
      "Epoch 27, train loss: 0.026877, train pcmra loss: 0.0\n",
      "Epoch 28, train loss: 0.026747, train pcmra loss: 0.0\n",
      "Epoch 29, train loss: 0.025868, train pcmra loss: 0.0\n",
      "Epoch 30, train loss: 0.02507, train pcmra loss: 0.0\n",
      "Epoch 30 took 43.04 seconds.\n",
      "Train mask loss: \t 0.02526,     pcmra loss: \t 0.0,     \tdice loss: \t 0.18891.\n",
      "Eval  mask loss: \t 0.04486,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27178.\n",
      "New best train loss, saving model.\n",
      "Epoch 31, train loss: 0.025084, train pcmra loss: 0.0\n",
      "Epoch 32, train loss: 0.024618, train pcmra loss: 0.0\n",
      "Epoch 33, train loss: 0.024642, train pcmra loss: 0.0\n",
      "Epoch 34, train loss: 0.023759, train pcmra loss: 0.0\n",
      "Epoch 35, train loss: 0.023083, train pcmra loss: 0.0\n",
      "Epoch 35 took 42.49 seconds.\n",
      "Train mask loss: \t 0.02158,     pcmra loss: \t 0.0,     \tdice loss: \t 0.1665.\n",
      "Eval  mask loss: \t 0.04635,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27042.\n",
      "New best train loss, saving model.\n",
      "Epoch 36, train loss: 0.023089, train pcmra loss: 0.0\n",
      "Epoch 37, train loss: 0.023747, train pcmra loss: 0.0\n",
      "Epoch 38, train loss: 0.023034, train pcmra loss: 0.0\n",
      "Epoch 39, train loss: 0.021906, train pcmra loss: 0.0\n",
      "Epoch 40, train loss: 0.021325, train pcmra loss: 0.0\n",
      "Epoch 40 took 43.06 seconds.\n",
      "Train mask loss: \t 0.02067,     pcmra loss: \t 0.0,     \tdice loss: \t 0.15274.\n",
      "Eval  mask loss: \t 0.04638,     pcmra loss: \t 0.0,     \tdice loss: \t 0.26146.\n",
      "New best train loss, saving model.\n",
      "Epoch 41, train loss: 0.022063, train pcmra loss: 0.0\n",
      "Epoch 42, train loss: 0.022081, train pcmra loss: 0.0\n",
      "Epoch 43, train loss: 0.020707, train pcmra loss: 0.0\n",
      "Epoch 44, train loss: 0.021035, train pcmra loss: 0.0\n",
      "Epoch 45, train loss: 0.021029, train pcmra loss: 0.0\n",
      "Epoch 45 took 42.49 seconds.\n",
      "Train mask loss: \t 0.0209,     pcmra loss: \t 0.0,     \tdice loss: \t 0.15288.\n",
      "Eval  mask loss: \t 0.04776,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27104.\n",
      "New best train loss, saving model.\n",
      "Epoch 46, train loss: 0.020887, train pcmra loss: 0.0\n",
      "Epoch 47, train loss: 0.0204, train pcmra loss: 0.0\n",
      "Epoch 48, train loss: 0.019879, train pcmra loss: 0.0\n",
      "Epoch 49, train loss: 0.020693, train pcmra loss: 0.0\n",
      "Epoch 50, train loss: 0.020158, train pcmra loss: 0.0\n",
      "Epoch 50 took 43.27 seconds.\n",
      "Train mask loss: \t 0.01872,     pcmra loss: \t 0.0,     \tdice loss: \t 0.13958.\n",
      "Eval  mask loss: \t 0.04891,     pcmra loss: \t 0.0,     \tdice loss: \t 0.26091.\n",
      "New best train loss, saving model.\n",
      "Epoch 51, train loss: 0.019505, train pcmra loss: 0.0\n",
      "Epoch 52, train loss: 0.019511, train pcmra loss: 0.0\n",
      "Epoch 53, train loss: 0.019697, train pcmra loss: 0.0\n",
      "Epoch 54, train loss: 0.019285, train pcmra loss: 0.0\n",
      "Epoch 55, train loss: 0.019506, train pcmra loss: 0.0\n",
      "Epoch 55 took 42.48 seconds.\n",
      "Train mask loss: \t 0.02061,     pcmra loss: \t 0.0,     \tdice loss: \t 0.16025.\n",
      "Eval  mask loss: \t 0.05192,     pcmra loss: \t 0.0,     \tdice loss: \t 0.28696.\n",
      "New best train loss, saving model.\n",
      "Epoch 56, train loss: 0.01828, train pcmra loss: 0.0\n",
      "Epoch 57, train loss: 0.018923, train pcmra loss: 0.0\n",
      "Epoch 58, train loss: 0.019642, train pcmra loss: 0.0\n",
      "Epoch 59, train loss: 0.018409, train pcmra loss: 0.0\n",
      "Epoch 60, train loss: 0.018311, train pcmra loss: 0.0\n",
      "Epoch 60 took 42.58 seconds.\n",
      "Train mask loss: \t 0.01918,     pcmra loss: \t 0.0,     \tdice loss: \t 0.15264.\n",
      "Eval  mask loss: \t 0.05337,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27606.\n",
      "New best train loss, saving model.\n",
      "Epoch 61, train loss: 0.017817, train pcmra loss: 0.0\n",
      "Epoch 62, train loss: 0.017844, train pcmra loss: 0.0\n",
      "Epoch 63, train loss: 0.017726, train pcmra loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, train loss: 0.017906, train pcmra loss: 0.0\n",
      "Epoch 65, train loss: 0.017639, train pcmra loss: 0.0\n",
      "Epoch 65 took 42.99 seconds.\n",
      "Train mask loss: \t 0.01908,     pcmra loss: \t 0.0,     \tdice loss: \t 0.14696.\n",
      "Eval  mask loss: \t 0.05252,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27383.\n",
      "New best train loss, saving model.\n",
      "Epoch 66, train loss: 0.017902, train pcmra loss: 0.0\n",
      "Epoch 67, train loss: 0.01788, train pcmra loss: 0.0\n",
      "Epoch 68, train loss: 0.017267, train pcmra loss: 0.0\n",
      "Epoch 69, train loss: 0.01692, train pcmra loss: 0.0\n",
      "{'device': 'GPU', 'print_models': False, 'name': '', 'pretrained': None, 'pretrained_best': 'train', 'reconstruction': 'mask', 'share_mapping': False, 'pcmra_lambda': 10, 'mask_lambda': 1.0, 'dataset': 'small', 'rotate': True, 'translate': True, 'flip': False, 'norm_min_max': [0, 1], 'seed': 34, 'epochs': 70, 'batch_size': 8, 'eval_every': 5, 'shuffle': True, 'n_coords_sample': 20000, 'cnn_setup': 1, 'mapping_setup': 2, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30.0, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'scheduler_on': 'combined'}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 1596\n",
      "[['16-01-22_Jarik_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27 Valentine kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27_Claudia_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-03_Feiko_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Luuk_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Michelle_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-12_Marjolein_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-03-18_Ruud_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-04-13_Pim_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-05-25_Emile_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)']]\n",
      "Val subjects: 28\n",
      "Test subjects: 28\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "Epoch 0, train loss: 0.189017, train pcmra loss: 0.0\n",
      "Epoch 0 took 43.32 seconds.\n",
      "Train mask loss: \t 0.1117,     pcmra loss: \t 0.0,     \tdice loss: \t 1.0.\n",
      "Eval  mask loss: \t 0.11375,     pcmra loss: \t 0.0,     \tdice loss: \t 1.0.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 1, train loss: 0.092154, train pcmra loss: 0.0\n",
      "Epoch 2, train loss: 0.071319, train pcmra loss: 0.0\n",
      "Epoch 3, train loss: 0.061967, train pcmra loss: 0.0\n",
      "Epoch 4, train loss: 0.055392, train pcmra loss: 0.0\n",
      "Epoch 5, train loss: 0.051307, train pcmra loss: 0.0\n",
      "Epoch 5 took 42.86 seconds.\n",
      "Train mask loss: \t 0.0491,     pcmra loss: \t 0.0,     \tdice loss: \t 0.37958.\n",
      "Eval  mask loss: \t 0.0497,     pcmra loss: \t 0.0,     \tdice loss: \t 0.34762.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 6, train loss: 0.047563, train pcmra loss: 0.0\n",
      "Epoch 7, train loss: 0.046247, train pcmra loss: 0.0\n",
      "Epoch 8, train loss: 0.043291, train pcmra loss: 0.0\n",
      "Epoch 9, train loss: 0.041934, train pcmra loss: 0.0\n",
      "Epoch 10, train loss: 0.040138, train pcmra loss: 0.0\n",
      "Epoch 10 took 42.92 seconds.\n",
      "Train mask loss: \t 0.03873,     pcmra loss: \t 0.0,     \tdice loss: \t 0.30244.\n",
      "Eval  mask loss: \t 0.04679,     pcmra loss: \t 0.0,     \tdice loss: \t 0.32346.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 11, train loss: 0.038583, train pcmra loss: 0.0\n",
      "Epoch 12, train loss: 0.037234, train pcmra loss: 0.0\n",
      "Epoch 13, train loss: 0.036356, train pcmra loss: 0.0\n",
      "Epoch 14, train loss: 0.035107, train pcmra loss: 0.0\n",
      "Epoch 15, train loss: 0.034161, train pcmra loss: 0.0\n",
      "Epoch 15 took 42.53 seconds.\n",
      "Train mask loss: \t 0.03254,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25864.\n",
      "Eval  mask loss: \t 0.04466,     pcmra loss: \t 0.0,     \tdice loss: \t 0.30305.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 16, train loss: 0.03278, train pcmra loss: 0.0\n",
      "Epoch 17, train loss: 0.033343, train pcmra loss: 0.0\n",
      "Epoch 18, train loss: 0.032161, train pcmra loss: 0.0\n",
      "Epoch 19, train loss: 0.03084, train pcmra loss: 0.0\n",
      "Epoch 20, train loss: 0.030472, train pcmra loss: 0.0\n",
      "Epoch 20 took 42.8 seconds.\n",
      "Train mask loss: \t 0.02918,     pcmra loss: \t 0.0,     \tdice loss: \t 0.23148.\n",
      "Eval  mask loss: \t 0.04711,     pcmra loss: \t 0.0,     \tdice loss: \t 0.29597.\n",
      "New best train loss, saving model.\n",
      "Epoch 21, train loss: 0.030716, train pcmra loss: 0.0\n",
      "Epoch 22, train loss: 0.030522, train pcmra loss: 0.0\n",
      "Epoch 23, train loss: 0.029558, train pcmra loss: 0.0\n",
      "Epoch 24, train loss: 0.028606, train pcmra loss: 0.0\n",
      "Epoch 25, train loss: 0.027794, train pcmra loss: 0.0\n",
      "Epoch 25 took 42.37 seconds.\n",
      "Train mask loss: \t 0.02858,     pcmra loss: \t 0.0,     \tdice loss: \t 0.21297.\n",
      "Eval  mask loss: \t 0.04652,     pcmra loss: \t 0.0,     \tdice loss: \t 0.29337.\n",
      "New best train loss, saving model.\n",
      "Epoch 26, train loss: 0.027519, train pcmra loss: 0.0\n",
      "Epoch 27, train loss: 0.027294, train pcmra loss: 0.0\n",
      "Epoch 28, train loss: 0.026508, train pcmra loss: 0.0\n",
      "Epoch 29, train loss: 0.026081, train pcmra loss: 0.0\n",
      "Epoch 30, train loss: 0.026396, train pcmra loss: 0.0\n",
      "Epoch 30 took 43.6 seconds.\n",
      "Train mask loss: \t 0.025,     pcmra loss: \t 0.0,     \tdice loss: \t 0.18586.\n",
      "Eval  mask loss: \t 0.04452,     pcmra loss: \t 0.0,     \tdice loss: \t 0.26902.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 31, train loss: 0.02578, train pcmra loss: 0.0\n",
      "Epoch 32, train loss: 0.024821, train pcmra loss: 0.0\n",
      "Epoch 33, train loss: 0.024676, train pcmra loss: 0.0\n",
      "Epoch 34, train loss: 0.023748, train pcmra loss: 0.0\n",
      "Epoch 35, train loss: 0.023724, train pcmra loss: 0.0\n",
      "Epoch 35 took 42.5 seconds.\n",
      "Train mask loss: \t 0.02304,     pcmra loss: \t 0.0,     \tdice loss: \t 0.17571.\n",
      "Eval  mask loss: \t 0.0459,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27646.\n",
      "New best train loss, saving model.\n",
      "Epoch 36, train loss: 0.023338, train pcmra loss: 0.0\n",
      "Epoch 37, train loss: 0.023258, train pcmra loss: 0.0\n",
      "Epoch 38, train loss: 0.02269, train pcmra loss: 0.0\n",
      "Epoch 39, train loss: 0.022334, train pcmra loss: 0.0\n",
      "Epoch 40, train loss: 0.022264, train pcmra loss: 0.0\n",
      "Epoch 40 took 43.17 seconds.\n",
      "Train mask loss: \t 0.02096,     pcmra loss: \t 0.0,     \tdice loss: \t 0.16672.\n",
      "Eval  mask loss: \t 0.05015,     pcmra loss: \t 0.0,     \tdice loss: \t 0.28664.\n",
      "New best train loss, saving model.\n",
      "Epoch 41, train loss: 0.021974, train pcmra loss: 0.0\n",
      "Epoch 42, train loss: 0.021166, train pcmra loss: 0.0\n",
      "Epoch 43, train loss: 0.021409, train pcmra loss: 0.0\n",
      "Epoch 44, train loss: 0.021241, train pcmra loss: 0.0\n",
      "Epoch 45, train loss: 0.021172, train pcmra loss: 0.0\n",
      "Epoch 45 took 41.21 seconds.\n",
      "Train mask loss: \t 0.0193,     pcmra loss: \t 0.0,     \tdice loss: \t 0.15034.\n",
      "Eval  mask loss: \t 0.04902,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27146.\n",
      "New best train loss, saving model.\n",
      "Epoch 46, train loss: 0.020944, train pcmra loss: 0.0\n",
      "Epoch 47, train loss: 0.020846, train pcmra loss: 0.0\n",
      "Epoch 48, train loss: 0.020412, train pcmra loss: 0.0\n",
      "Epoch 49, train loss: 0.020139, train pcmra loss: 0.0\n",
      "Epoch 50, train loss: 0.020069, train pcmra loss: 0.0\n",
      "Epoch 50 took 42.53 seconds.\n",
      "Train mask loss: \t 0.02067,     pcmra loss: \t 0.0,     \tdice loss: \t 0.16324.\n",
      "Eval  mask loss: \t 0.049,     pcmra loss: \t 0.0,     \tdice loss: \t 0.28496.\n",
      "New best train loss, saving model.\n",
      "Epoch 51, train loss: 0.019902, train pcmra loss: 0.0\n",
      "Epoch 52, train loss: 0.019402, train pcmra loss: 0.0\n",
      "Epoch 53, train loss: 0.019314, train pcmra loss: 0.0\n",
      "Epoch 54, train loss: 0.019045, train pcmra loss: 0.0\n",
      "Epoch 55, train loss: 0.018974, train pcmra loss: 0.0\n",
      "Epoch 55 took 42.98 seconds.\n",
      "Train mask loss: \t 0.02056,     pcmra loss: \t 0.0,     \tdice loss: \t 0.16023.\n",
      "Eval  mask loss: \t 0.05263,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27831.\n",
      "New best train loss, saving model.\n",
      "Epoch 56, train loss: 0.019308, train pcmra loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, train loss: 0.019064, train pcmra loss: 0.0\n",
      "Epoch 58, train loss: 0.018184, train pcmra loss: 0.0\n",
      "Epoch 59, train loss: 0.018307, train pcmra loss: 0.0\n",
      "Epoch 60, train loss: 0.018685, train pcmra loss: 0.0\n",
      "Epoch 60 took 42.38 seconds.\n",
      "Train mask loss: \t 0.01808,     pcmra loss: \t 0.0,     \tdice loss: \t 0.13586.\n",
      "Eval  mask loss: \t 0.05307,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27466.\n",
      "New best train loss, saving model.\n",
      "Epoch 61, train loss: 0.018207, train pcmra loss: 0.0\n",
      "Epoch 62, train loss: 0.017931, train pcmra loss: 0.0\n",
      "Epoch 63, train loss: 0.017468, train pcmra loss: 0.0\n",
      "Epoch 64, train loss: 0.017955, train pcmra loss: 0.0\n",
      "Epoch 65, train loss: 0.017484, train pcmra loss: 0.0\n",
      "Epoch 65 took 42.4 seconds.\n",
      "Train mask loss: \t 0.01836,     pcmra loss: \t 0.0,     \tdice loss: \t 0.13553.\n",
      "Eval  mask loss: \t 0.05216,     pcmra loss: \t 0.0,     \tdice loss: \t 0.26113.\n",
      "New best train loss, saving model.\n",
      "Epoch 66, train loss: 0.018075, train pcmra loss: 0.0\n",
      "Epoch 67, train loss: 0.017907, train pcmra loss: 0.0\n",
      "Epoch 68, train loss: 0.017438, train pcmra loss: 0.0\n",
      "Epoch 69, train loss: 0.017792, train pcmra loss: 0.0\n",
      "{'device': 'GPU', 'print_models': False, 'name': '', 'pretrained': None, 'pretrained_best': 'train', 'reconstruction': 'mask', 'share_mapping': False, 'pcmra_lambda': 5, 'mask_lambda': 1.0, 'dataset': 'small', 'rotate': True, 'translate': True, 'flip': False, 'norm_min_max': [0, 1], 'seed': 34, 'epochs': 70, 'batch_size': 24, 'eval_every': 5, 'shuffle': True, 'n_coords_sample': 5000, 'cnn_setup': 1, 'mapping_setup': 2, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30.0, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'scheduler_on': 'combined'}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 1596\n",
      "[['16-01-22_Jarik_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27 Valentine kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27_Claudia_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-03_Feiko_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Luuk_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Michelle_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-12_Marjolein_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-03-18_Ruud_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-04-13_Pim_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-05-25_Emile_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)']]\n",
      "Val subjects: 28\n",
      "Test subjects: 28\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "Epoch 0, train loss: 0.296138, train pcmra loss: 0.0\n",
      "Epoch 0 took 36.22 seconds.\n",
      "Train mask loss: \t 0.18118,     pcmra loss: \t 0.0,     \tdice loss: \t 1.0.\n",
      "Eval  mask loss: \t 0.18392,     pcmra loss: \t 0.0,     \tdice loss: \t 1.0.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 1, train loss: 0.154512, train pcmra loss: 0.0\n",
      "Epoch 2, train loss: 0.117712, train pcmra loss: 0.0\n",
      "Epoch 3, train loss: 0.091572, train pcmra loss: 0.0\n",
      "Epoch 4, train loss: 0.077119, train pcmra loss: 0.0\n",
      "Epoch 5, train loss: 0.068021, train pcmra loss: 0.0\n",
      "Epoch 5 took 36.43 seconds.\n",
      "Train mask loss: \t 0.06667,     pcmra loss: \t 0.0,     \tdice loss: \t 0.4898.\n",
      "Eval  mask loss: \t 0.06837,     pcmra loss: \t 0.0,     \tdice loss: \t 0.48524.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 6, train loss: 0.061438, train pcmra loss: 0.0\n",
      "Epoch 7, train loss: 0.057346, train pcmra loss: 0.0\n",
      "Epoch 8, train loss: 0.052582, train pcmra loss: 0.0\n",
      "Epoch 9, train loss: 0.048433, train pcmra loss: 0.0\n",
      "Epoch 10, train loss: 0.0478, train pcmra loss: 0.0\n",
      "Epoch 10 took 36.3 seconds.\n",
      "Train mask loss: \t 0.04599,     pcmra loss: \t 0.0,     \tdice loss: \t 0.33216.\n",
      "Eval  mask loss: \t 0.05129,     pcmra loss: \t 0.0,     \tdice loss: \t 0.3499.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 11, train loss: 0.046387, train pcmra loss: 0.0\n",
      "Epoch 12, train loss: 0.043844, train pcmra loss: 0.0\n",
      "Epoch 13, train loss: 0.041261, train pcmra loss: 0.0\n",
      "Epoch 14, train loss: 0.038944, train pcmra loss: 0.0\n",
      "Epoch 15, train loss: 0.03803, train pcmra loss: 0.0\n",
      "Epoch 15 took 36.61 seconds.\n",
      "Train mask loss: \t 0.03808,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25741.\n",
      "Eval  mask loss: \t 0.04423,     pcmra loss: \t 0.0,     \tdice loss: \t 0.28224.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 16, train loss: 0.036567, train pcmra loss: 0.0\n",
      "Epoch 17, train loss: 0.035644, train pcmra loss: 0.0\n",
      "Epoch 18, train loss: 0.035035, train pcmra loss: 0.0\n",
      "Epoch 19, train loss: 0.034061, train pcmra loss: 0.0\n",
      "Epoch 20, train loss: 0.032589, train pcmra loss: 0.0\n",
      "Epoch 20 took 36.58 seconds.\n",
      "Train mask loss: \t 0.03195,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25031.\n",
      "Eval  mask loss: \t 0.04304,     pcmra loss: \t 0.0,     \tdice loss: \t 0.29078.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 21, train loss: 0.032754, train pcmra loss: 0.0\n",
      "Epoch 22, train loss: 0.030891, train pcmra loss: 0.0\n",
      "Epoch 23, train loss: 0.031076, train pcmra loss: 0.0\n",
      "Epoch 24, train loss: 0.030079, train pcmra loss: 0.0\n",
      "Epoch 25, train loss: 0.029442, train pcmra loss: 0.0\n",
      "Epoch 25 took 36.4 seconds.\n",
      "Train mask loss: \t 0.02798,     pcmra loss: \t 0.0,     \tdice loss: \t 0.20177.\n",
      "Eval  mask loss: \t 0.04117,     pcmra loss: \t 0.0,     \tdice loss: \t 0.26246.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 26, train loss: 0.028313, train pcmra loss: 0.0\n",
      "Epoch 27, train loss: 0.029752, train pcmra loss: 0.0\n",
      "Epoch 28, train loss: 0.0297, train pcmra loss: 0.0\n",
      "Epoch 29, train loss: 0.028617, train pcmra loss: 0.0\n",
      "Epoch 30, train loss: 0.027098, train pcmra loss: 0.0\n",
      "Epoch 30 took 36.3 seconds.\n",
      "Train mask loss: \t 0.02676,     pcmra loss: \t 0.0,     \tdice loss: \t 0.19374.\n",
      "Eval  mask loss: \t 0.04036,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25272.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 31, train loss: 0.027389, train pcmra loss: 0.0\n",
      "Epoch 32, train loss: 0.027184, train pcmra loss: 0.0\n",
      "Epoch 33, train loss: 0.025391, train pcmra loss: 0.0\n",
      "Epoch 34, train loss: 0.025374, train pcmra loss: 0.0\n",
      "Epoch 35, train loss: 0.026044, train pcmra loss: 0.0\n",
      "Epoch 35 took 36.38 seconds.\n",
      "Train mask loss: \t 0.02509,     pcmra loss: \t 0.0,     \tdice loss: \t 0.19471.\n",
      "Eval  mask loss: \t 0.04544,     pcmra loss: \t 0.0,     \tdice loss: \t 0.28568.\n",
      "New best train loss, saving model.\n",
      "Epoch 36, train loss: 0.025004, train pcmra loss: 0.0\n",
      "Epoch 37, train loss: 0.024857, train pcmra loss: 0.0\n",
      "Epoch 38, train loss: 0.023768, train pcmra loss: 0.0\n",
      "Epoch 39, train loss: 0.023899, train pcmra loss: 0.0\n",
      "Epoch 40, train loss: 0.024476, train pcmra loss: 0.0\n",
      "Epoch 40 took 36.44 seconds.\n",
      "Train mask loss: \t 0.02816,     pcmra loss: \t 0.0,     \tdice loss: \t 0.21207.\n",
      "Eval  mask loss: \t 0.05149,     pcmra loss: \t 0.0,     \tdice loss: \t 0.31829.\n",
      "New best train loss, saving model.\n",
      "Epoch 41, train loss: 0.024352, train pcmra loss: 0.0\n",
      "Epoch 42, train loss: 0.02299, train pcmra loss: 0.0\n",
      "Epoch 43, train loss: 0.022249, train pcmra loss: 0.0\n",
      "Epoch 44, train loss: 0.023434, train pcmra loss: 0.0\n",
      "Epoch 45, train loss: 0.022724, train pcmra loss: 0.0\n",
      "Epoch 45 took 36.3 seconds.\n",
      "Train mask loss: \t 0.02362,     pcmra loss: \t 0.0,     \tdice loss: \t 0.17172.\n",
      "Eval  mask loss: \t 0.04479,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25928.\n",
      "New best train loss, saving model.\n",
      "Epoch 46, train loss: 0.022495, train pcmra loss: 0.0\n",
      "Epoch 47, train loss: 0.022422, train pcmra loss: 0.0\n",
      "Epoch 48, train loss: 0.022204, train pcmra loss: 0.0\n",
      "Epoch 49, train loss: 0.022006, train pcmra loss: 0.0\n",
      "Epoch 50, train loss: 0.022171, train pcmra loss: 0.0\n",
      "Epoch 50 took 36.47 seconds.\n",
      "Train mask loss: \t 0.02046,     pcmra loss: \t 0.0,     \tdice loss: \t 0.14761.\n",
      "Eval  mask loss: \t 0.04373,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25074.\n",
      "New best train loss, saving model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, train loss: 0.02108, train pcmra loss: 0.0\n",
      "Epoch 52, train loss: 0.021521, train pcmra loss: 0.0\n",
      "Epoch 53, train loss: 0.020259, train pcmra loss: 0.0\n",
      "Epoch 54, train loss: 0.02083, train pcmra loss: 0.0\n",
      "Epoch 55, train loss: 0.021342, train pcmra loss: 0.0\n",
      "Epoch 55 took 35.96 seconds.\n",
      "Train mask loss: \t 0.02151,     pcmra loss: \t 0.0,     \tdice loss: \t 0.16457.\n",
      "Eval  mask loss: \t 0.04651,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27606.\n",
      "New best train loss, saving model.\n",
      "Epoch 56, train loss: 0.021353, train pcmra loss: 0.0\n",
      "Epoch 57, train loss: 0.020227, train pcmra loss: 0.0\n",
      "Epoch 58, train loss: 0.019823, train pcmra loss: 0.0\n",
      "Epoch 59, train loss: 0.02015, train pcmra loss: 0.0\n",
      "Epoch 60, train loss: 0.020381, train pcmra loss: 0.0\n",
      "Epoch 60 took 36.35 seconds.\n",
      "Train mask loss: \t 0.0219,     pcmra loss: \t 0.0,     \tdice loss: \t 0.16924.\n",
      "Eval  mask loss: \t 0.04392,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25271.\n",
      "New best train loss, saving model.\n",
      "Epoch 61, train loss: 0.020387, train pcmra loss: 0.0\n",
      "Epoch 62, train loss: 0.020788, train pcmra loss: 0.0\n",
      "Epoch 63, train loss: 0.019799, train pcmra loss: 0.0\n",
      "Epoch 64, train loss: 0.018929, train pcmra loss: 0.0\n",
      "Epoch 65, train loss: 0.018854, train pcmra loss: 0.0\n",
      "Epoch 65 took 36.14 seconds.\n",
      "Train mask loss: \t 0.01866,     pcmra loss: \t 0.0,     \tdice loss: \t 0.14098.\n",
      "Eval  mask loss: \t 0.04509,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25483.\n",
      "New best train loss, saving model.\n",
      "Epoch 66, train loss: 0.019412, train pcmra loss: 0.0\n",
      "Epoch 67, train loss: 0.02011, train pcmra loss: 0.0\n",
      "Epoch 68, train loss: 0.019967, train pcmra loss: 0.0\n",
      "Epoch 69, train loss: 0.019071, train pcmra loss: 0.0\n",
      "{'device': 'GPU', 'print_models': False, 'name': '', 'pretrained': None, 'pretrained_best': 'train', 'reconstruction': 'mask', 'share_mapping': False, 'pcmra_lambda': 10, 'mask_lambda': 1.0, 'dataset': 'small', 'rotate': True, 'translate': True, 'flip': False, 'norm_min_max': [0, 1], 'seed': 34, 'epochs': 70, 'batch_size': 24, 'eval_every': 5, 'shuffle': True, 'n_coords_sample': 5000, 'cnn_setup': 1, 'mapping_setup': 2, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30.0, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'scheduler_on': 'combined'}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 1596\n",
      "[['16-01-22_Jarik_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27 Valentine kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-01-27_Claudia_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-03_Feiko_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Luuk_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-10_Michelle_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-02-12_Marjolein_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-03-18_Ruud_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-04-13_Pim_kt-pca_done2', 'Aorta Volunteers', 'scaled_rot 0 (-, -)'], ['16-05-25_Emile_kt-pca_done', 'Aorta Volunteers', 'scaled_rot 0 (-, -)']]\n",
      "Val subjects: 28\n",
      "Test subjects: 28\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "Epoch 0, train loss: 0.305357, train pcmra loss: 0.0\n",
      "Epoch 0 took 36.22 seconds.\n",
      "Train mask loss: \t 0.18612,     pcmra loss: \t 0.0,     \tdice loss: \t 1.0.\n",
      "Eval  mask loss: \t 0.1875,     pcmra loss: \t 0.0,     \tdice loss: \t 1.0.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 1, train loss: 0.156577, train pcmra loss: 0.0\n",
      "Epoch 2, train loss: 0.11987, train pcmra loss: 0.0\n",
      "Epoch 3, train loss: 0.094303, train pcmra loss: 0.0\n",
      "Epoch 4, train loss: 0.077297, train pcmra loss: 0.0\n",
      "Epoch 5, train loss: 0.068236, train pcmra loss: 0.0\n",
      "Epoch 5 took 36.33 seconds.\n",
      "Train mask loss: \t 0.06684,     pcmra loss: \t 0.0,     \tdice loss: \t 0.56993.\n",
      "Eval  mask loss: \t 0.07363,     pcmra loss: \t 0.0,     \tdice loss: \t 0.58759.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 6, train loss: 0.061684, train pcmra loss: 0.0\n",
      "Epoch 7, train loss: 0.055405, train pcmra loss: 0.0\n",
      "Epoch 8, train loss: 0.051658, train pcmra loss: 0.0\n",
      "Epoch 9, train loss: 0.049311, train pcmra loss: 0.0\n",
      "Epoch 10, train loss: 0.046257, train pcmra loss: 0.0\n",
      "Epoch 10 took 36.5 seconds.\n",
      "Train mask loss: \t 0.04459,     pcmra loss: \t 0.0,     \tdice loss: \t 0.32021.\n",
      "Eval  mask loss: \t 0.04792,     pcmra loss: \t 0.0,     \tdice loss: \t 0.32.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 11, train loss: 0.044139, train pcmra loss: 0.0\n",
      "Epoch 12, train loss: 0.042459, train pcmra loss: 0.0\n",
      "Epoch 13, train loss: 0.0396, train pcmra loss: 0.0\n",
      "Epoch 14, train loss: 0.03878, train pcmra loss: 0.0\n",
      "Epoch 15, train loss: 0.038739, train pcmra loss: 0.0\n",
      "Epoch 15 took 36.34 seconds.\n",
      "Train mask loss: \t 0.04322,     pcmra loss: \t 0.0,     \tdice loss: \t 0.32582.\n",
      "Eval  mask loss: \t 0.0495,     pcmra loss: \t 0.0,     \tdice loss: \t 0.34826.\n",
      "New best train loss, saving model.\n",
      "Epoch 16, train loss: 0.037028, train pcmra loss: 0.0\n",
      "Epoch 17, train loss: 0.035092, train pcmra loss: 0.0\n",
      "Epoch 18, train loss: 0.034816, train pcmra loss: 0.0\n",
      "Epoch 19, train loss: 0.033484, train pcmra loss: 0.0\n",
      "Epoch 20, train loss: 0.033879, train pcmra loss: 0.0\n",
      "Epoch 20 took 36.28 seconds.\n",
      "Train mask loss: \t 0.03159,     pcmra loss: \t 0.0,     \tdice loss: \t 0.24224.\n",
      "Eval  mask loss: \t 0.04152,     pcmra loss: \t 0.0,     \tdice loss: \t 0.28426.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 21, train loss: 0.032142, train pcmra loss: 0.0\n",
      "Epoch 22, train loss: 0.031478, train pcmra loss: 0.0\n",
      "Epoch 23, train loss: 0.0314, train pcmra loss: 0.0\n",
      "Epoch 24, train loss: 0.029767, train pcmra loss: 0.0\n",
      "Epoch 25, train loss: 0.030072, train pcmra loss: 0.0\n",
      "Epoch 25 took 36.49 seconds.\n",
      "Train mask loss: \t 0.03011,     pcmra loss: \t 0.0,     \tdice loss: \t 0.21008.\n",
      "Eval  mask loss: \t 0.04105,     pcmra loss: \t 0.0,     \tdice loss: \t 0.26226.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 26, train loss: 0.029877, train pcmra loss: 0.0\n",
      "Epoch 27, train loss: 0.028779, train pcmra loss: 0.0\n",
      "Epoch 28, train loss: 0.028566, train pcmra loss: 0.0\n",
      "Epoch 29, train loss: 0.028317, train pcmra loss: 0.0\n",
      "Epoch 30, train loss: 0.026922, train pcmra loss: 0.0\n",
      "Epoch 30 took 36.41 seconds.\n",
      "Train mask loss: \t 0.02656,     pcmra loss: \t 0.0,     \tdice loss: \t 0.19028.\n",
      "Eval  mask loss: \t 0.03891,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25193.\n",
      "New best train loss, saving model.\n",
      "New best val loss, saving model.\n",
      "Epoch 31, train loss: 0.027558, train pcmra loss: 0.0\n",
      "Epoch 32, train loss: 0.026312, train pcmra loss: 0.0\n",
      "Epoch 33, train loss: 0.026828, train pcmra loss: 0.0\n",
      "Epoch 34, train loss: 0.026357, train pcmra loss: 0.0\n",
      "Epoch 35, train loss: 0.025119, train pcmra loss: 0.0\n",
      "Epoch 35 took 36.35 seconds.\n",
      "Train mask loss: \t 0.02444,     pcmra loss: \t 0.0,     \tdice loss: \t 0.18433.\n",
      "Eval  mask loss: \t 0.04055,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25687.\n",
      "New best train loss, saving model.\n",
      "Epoch 36, train loss: 0.025557, train pcmra loss: 0.0\n",
      "Epoch 37, train loss: 0.024252, train pcmra loss: 0.0\n",
      "Epoch 38, train loss: 0.025042, train pcmra loss: 0.0\n",
      "Epoch 39, train loss: 0.02473, train pcmra loss: 0.0\n",
      "Epoch 40, train loss: 0.023609, train pcmra loss: 0.0\n",
      "Epoch 40 took 36.44 seconds.\n",
      "Train mask loss: \t 0.02394,     pcmra loss: \t 0.0,     \tdice loss: \t 0.18047.\n",
      "Eval  mask loss: \t 0.04155,     pcmra loss: \t 0.0,     \tdice loss: \t 0.27394.\n",
      "New best train loss, saving model.\n",
      "Epoch 41, train loss: 0.024072, train pcmra loss: 0.0\n",
      "Epoch 42, train loss: 0.02358, train pcmra loss: 0.0\n",
      "Epoch 43, train loss: 0.023156, train pcmra loss: 0.0\n",
      "Epoch 44, train loss: 0.02314, train pcmra loss: 0.0\n",
      "Epoch 45, train loss: 0.022607, train pcmra loss: 0.0\n",
      "Epoch 45 took 36.48 seconds.\n",
      "Train mask loss: \t 0.02261,     pcmra loss: \t 0.0,     \tdice loss: \t 0.18059.\n",
      "Eval  mask loss: \t 0.04051,     pcmra loss: \t 0.0,     \tdice loss: \t 0.26033.\n",
      "New best train loss, saving model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, train loss: 0.022817, train pcmra loss: 0.0\n",
      "Epoch 47, train loss: 0.022304, train pcmra loss: 0.0\n",
      "Epoch 48, train loss: 0.022241, train pcmra loss: 0.0\n",
      "Epoch 49, train loss: 0.022533, train pcmra loss: 0.0\n",
      "Epoch 50, train loss: 0.022207, train pcmra loss: 0.0\n",
      "Epoch 50 took 36.39 seconds.\n",
      "Train mask loss: \t 0.02283,     pcmra loss: \t 0.0,     \tdice loss: \t 0.17448.\n",
      "Eval  mask loss: \t 0.04405,     pcmra loss: \t 0.0,     \tdice loss: \t 0.26968.\n",
      "New best train loss, saving model.\n",
      "Epoch 51, train loss: 0.022177, train pcmra loss: 0.0\n",
      "Epoch 52, train loss: 0.021445, train pcmra loss: 0.0\n",
      "Epoch 53, train loss: 0.021176, train pcmra loss: 0.0\n",
      "Epoch 54, train loss: 0.022031, train pcmra loss: 0.0\n",
      "Epoch 55, train loss: 0.021044, train pcmra loss: 0.0\n",
      "Epoch 55 took 36.46 seconds.\n",
      "Train mask loss: \t 0.02052,     pcmra loss: \t 0.0,     \tdice loss: \t 0.15709.\n",
      "Eval  mask loss: \t 0.04441,     pcmra loss: \t 0.0,     \tdice loss: \t 0.26368.\n",
      "New best train loss, saving model.\n",
      "Epoch 56, train loss: 0.020969, train pcmra loss: 0.0\n",
      "Epoch 57, train loss: 0.021073, train pcmra loss: 0.0\n",
      "Epoch 58, train loss: 0.020588, train pcmra loss: 0.0\n",
      "Epoch 59, train loss: 0.020597, train pcmra loss: 0.0\n",
      "Epoch 60, train loss: 0.020172, train pcmra loss: 0.0\n",
      "Epoch 60 took 36.31 seconds.\n",
      "Train mask loss: \t 0.02064,     pcmra loss: \t 0.0,     \tdice loss: \t 0.1547.\n",
      "Eval  mask loss: \t 0.04204,     pcmra loss: \t 0.0,     \tdice loss: \t 0.24326.\n",
      "New best train loss, saving model.\n",
      "Epoch 61, train loss: 0.020219, train pcmra loss: 0.0\n",
      "Epoch 62, train loss: 0.019621, train pcmra loss: 0.0\n",
      "Epoch 63, train loss: 0.02032, train pcmra loss: 0.0\n",
      "Epoch 64, train loss: 0.021607, train pcmra loss: 0.0\n",
      "Epoch 65, train loss: 0.019341, train pcmra loss: 0.0\n",
      "Epoch 65 took 36.18 seconds.\n",
      "Train mask loss: \t 0.01913,     pcmra loss: \t 0.0,     \tdice loss: \t 0.14959.\n",
      "Eval  mask loss: \t 0.04489,     pcmra loss: \t 0.0,     \tdice loss: \t 0.25567.\n",
      "New best train loss, saving model.\n",
      "Epoch 66, train loss: 0.019431, train pcmra loss: 0.0\n",
      "Epoch 67, train loss: 0.020142, train pcmra loss: 0.0\n",
      "Epoch 68, train loss: 0.019945, train pcmra loss: 0.0\n",
      "Epoch 69, train loss: 0.019519, train pcmra loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()    \n",
    "\n",
    "# ARGS.n_coords_sample=-1\n",
    "# ARGS.batch_size = 4\n",
    "\n",
    "versions = [(20000, 8), (5000, 24)]\n",
    "\n",
    "ARGS = init_ARGS()\n",
    "\n",
    "\n",
    "for ARGS.n_coords_sample, ARGS.batch_size in versions: \n",
    "    for ARGS.pcmra_lambda in [5, 10]:\n",
    "        ARGS.epochs = 70\n",
    "        ARGS.print_models = False\n",
    "        ARGS.rotate, ARGS.translate, ARGS.flip = True, True, False\n",
    "\n",
    "\n",
    "        ARGS.scheduler_on = \"combined\"\n",
    "        ARGS.reconstruction = \"mask\"\n",
    "\n",
    "        ARGS.share_mapping = False\n",
    "\n",
    "        ARGS.cnn_setup = 1\n",
    "        ARGS.mapping_setup = 2\n",
    "\n",
    "        print(vars(ARGS))\n",
    "\n",
    "        train()  \n",
    "\n",
    "        torch.cuda.empty_cache()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ARGS = init_ARGS()\n",
    "\n",
    "# ARGS.print_models = False\n",
    "# ARGS.epochs = 250\n",
    "# ARGS.eval_every = 20\n",
    "\n",
    "# cnn_mapping_combis = [[1, 1], [1, 2], [2, 3], [2, 4]]\n",
    "# transformations = [(False, False, False)]\n",
    "# share_mapping = [False, True]\n",
    "# reconstruction = [\"pcmra\", \"both\", \"mask\"]\n",
    "# lambdas = [(1, 1), (1, 10)]\n",
    "# omega_0s = [5, 30, 100]\n",
    "\n",
    "# for ARGS.cnn_setup, ARGS.mapping_setup in cnn_mapping_combis: \n",
    "#     for ARGS.rotate, ARGS.translate, ARGS.flip in transformations: \n",
    "#         for ARGS.share_mapping in share_mapping: \n",
    "#             for ARGS.reconstruction in reconstruction: \n",
    "#                 for ARGS.mask_lambda, ARGS.pcmra_lambda in lambdas: \n",
    "#                     for ARGS.first_omega_0 in omega_0s: \n",
    "#                         print(vars(ARGS))\n",
    "#                         train()  \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ARGS = init_ARGS()\n",
    "\n",
    "# ARGS.print_models = False\n",
    "# ARGS.epochs = 51\n",
    "\n",
    "# transformations = [(True, True, False), (True, True, True)]\n",
    "# cnn_mapping_combis = [[1, 2], [2, 3], [2, 4]]\n",
    "# share_mapping = [False, True]\n",
    "# reconstruction = [\"pcmra\", \"both\", \"mask\"]\n",
    "# lambdas = [(1, 1), (1, 10)]\n",
    "# omega_0s = [30]\n",
    "\n",
    "# for ARGS.rotate, ARGS.translate, ARGS.flip in transformations: \n",
    "#     for ARGS.cnn_setup, ARGS.mapping_setup in cnn_mapping_combis: \n",
    "#         for ARGS.share_mapping in share_mapping: \n",
    "#             for ARGS.reconstruction in reconstruction: \n",
    "#                 for ARGS.mask_lambda, ARGS.pcmra_lambda in lambdas: \n",
    "#                     for ARGS.first_omega_0 in omega_0s: \n",
    "#                         print(vars(ARGS))\n",
    "#                         train()  \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--device DEVICE]\n",
      "                             [--print_models PRINT_MODELS] [--name NAME]\n",
      "                             [--pretrained PRETRAINED]\n",
      "                             [--pretrained_best PRETRAINED_BEST]\n",
      "                             [--reconstruction RECONSTRUCTION]\n",
      "                             [--share_mapping SHARE_MAPPING]\n",
      "                             [--pcmra_lambda PCMRA_LAMBDA]\n",
      "                             [--mask_lambda MASK_LAMBDA] [--dataset DATASET]\n",
      "                             [--rotate ROTATE] [--translate TRANSLATE]\n",
      "                             [--flip FLIP] [--norm_min_max NORM_MIN_MAX]\n",
      "                             [--seed SEED] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--eval_every EVAL_EVERY] [--shuffle SHUFFLE]\n",
      "                             [--n_coords_sample N_COORDS_SAMPLE]\n",
      "                             [--cnn_setup CNN_SETUP]\n",
      "                             [--mapping_setup MAPPING_SETUP]\n",
      "                             [--dim_hidden DIM_HIDDEN]\n",
      "                             [--siren_hidden_layers SIREN_HIDDEN_LAYERS]\n",
      "                             [--first_omega_0 FIRST_OMEGA_0]\n",
      "                             [--hidden_omega_0 HIDDEN_OMEGA_0]\n",
      "                             [--pcmra_first_omega_0 PCMRA_FIRST_OMEGA_0]\n",
      "                             [--pcmra_hidden_omega_0 PCMRA_HIDDEN_OMEGA_0]\n",
      "                             [--cnn_lr CNN_LR] [--cnn_wd CNN_WD]\n",
      "                             [--mapping_lr MAPPING_LR]\n",
      "                             [--pcmra_mapping_lr PCMRA_MAPPING_LR]\n",
      "                             [--siren_lr SIREN_LR] [--siren_wd SIREN_WD]\n",
      "                             [--pcmra_siren_lr PCMRA_SIREN_LR]\n",
      "                             [--pcmra_siren_wd PCMRA_SIREN_WD]\n",
      "                             [--scheduler_on SCHEDULER_ON]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/ptenkaate/.local/share/jupyter/runtime/kernel-9e3db1bd-c4fa-4d27-be43-bc6b05418d51.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ptenkaate/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    PARSER = argparse.ArgumentParser()\n",
    "\n",
    "    # Arguments for training\n",
    "    PARSER.add_argument('--device', type=str, default=\"GPU\", \n",
    "                        help='Device that should be used.')\n",
    "\n",
    "    PARSER.add_argument('--print_models', type=str, default=\"GPU\", \n",
    "                        help='Print the models after initialization or not.')\n",
    "\n",
    "    PARSER.add_argument('--name', type=str, default=\"\", \n",
    "                        help='Name of the folder where the output should be saved.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained', type=str, default=None, \n",
    "                        help='Folder name of pretrained model that should be loaded.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_best', type=str, default=\"train\", \n",
    "                        help='Pretrained model with lowest [train, val] loss.')\n",
    "    \n",
    "    \n",
    "    # PCMRA and Mask reconstruction\n",
    "    PARSER.add_argument('--reconstruction', type=str, default=\"pcmra\", \n",
    "                        help='Do we want the CNN trained on reconstruction of pcmra/mask/both.')\n",
    "    \n",
    "    PARSER.add_argument('--share_mapping', type=bool, default=False, \n",
    "                        help='Do we want to share the mapping from latent repr to gamma and beta?')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_lambda', type=float, default=1., \n",
    "                        help='Multiplier for pcmra reconstruction loss')\n",
    "    \n",
    "    PARSER.add_argument('--mask_lambda', type=float, default=1., \n",
    "                        help='Multiplier for mask reconstruction loss')\n",
    "    \n",
    "    \n",
    "    # data\n",
    "    PARSER.add_argument('--dataset', type=str, default=\"small\", \n",
    "                        help='The dataset which we train on.')\n",
    "    \n",
    "    PARSER.add_argument('--rotate', type=bool, default=True, \n",
    "                        help='Rotations of the same image')\n",
    "    \n",
    "    PARSER.add_argument('--translate', type=bool, default=True, \n",
    "                        help='Translations of the same image')\n",
    "    \n",
    "    PARSER.add_argument('--flip', type=bool, default=True, \n",
    "                        help='Flips of the same image')\n",
    "    \n",
    "    PARSER.add_argument('--norm_min_max', type=list, default=[0, 1], \n",
    "                        help='List with min and max for normalizing input.')\n",
    "    \n",
    "    PARSER.add_argument('--seed', type=int, default=34, \n",
    "                        help='Seed for initializig dataloader')\n",
    "    \n",
    "    \n",
    "    # train variables\n",
    "    PARSER.add_argument('--epochs', type=int, default=51, \n",
    "                        help='Number of epochs.')\n",
    "    \n",
    "    PARSER.add_argument('--batch_size', type=int, default=24, \n",
    "                        help='Number of epochs.')\n",
    "        \n",
    "    PARSER.add_argument('--eval_every', type=int, default=5, \n",
    "                        help='Set the # epochs after which evaluation should be done.')\n",
    "    \n",
    "    PARSER.add_argument('--shuffle', type=bool, default=True, \n",
    "                        help='Shuffle the train dataloader?')\n",
    "    \n",
    "    PARSER.add_argument('--n_coords_sample', type=int, default=5000, \n",
    "                        help='Number of coordinates that should be sampled for each subject.')\n",
    "    \n",
    "    \n",
    "    # CNN\n",
    "    PARSER.add_argument('--cnn_setup', type=int, default=1, \n",
    "                        help='Setup of the CNN.')\n",
    "\n",
    "    \n",
    "    # Mapping\n",
    "    PARSER.add_argument('--mapping_setup', type=int, default=1, \n",
    "                        help='Setup of the Mapping network.')\n",
    "\n",
    "    \n",
    "    # SIREN\n",
    "    PARSER.add_argument('--dim_hidden', type=int, default=256, \n",
    "                        help='Dimension of hidden SIREN layers.')\n",
    "    \n",
    "    PARSER.add_argument('--siren_hidden_layers', type=int, default=3, \n",
    "                        help='Number of hidden SIREN layers.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--first_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of first layer.')\n",
    "    \n",
    "    PARSER.add_argument('--hidden_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of hidden layer.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_first_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of first layer of PCMRA siren.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_hidden_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of hidden layer of PCMRA siren.')\n",
    "    \n",
    "    \n",
    "    # optimizers\n",
    "    PARSER.add_argument('--cnn_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of cnn optim.')\n",
    "\n",
    "    PARSER.add_argument('--cnn_wd', type=float, default=0, \n",
    "                        help='Weight decay of cnn optim.')\n",
    "\n",
    "    \n",
    "    PARSER.add_argument('--mapping_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_mapping_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "    \n",
    "\n",
    "    PARSER.add_argument('--siren_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "\n",
    "    PARSER.add_argument('--siren_wd', type=float, default=0, \n",
    "                        help='Weight decay of siren optim.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_siren_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of PCMRA siren optim.')    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_siren_wd', type=float, default=0, \n",
    "                        help='Weight decay of PCMRA siren optim.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--scheduler_on', type=str, default=\"combined\", \n",
    "                        help='Schedule lr on pcmra/mask/combined loss.')\n",
    "    \n",
    " \n",
    "    ARGS = PARSER.parse_args()\n",
    "    \n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitc17f53f707db4b89be7c32a22adf91a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
