{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "import numpy as np\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                 kernel_size, stride, padding, \n",
    "                 activation=\"relu\", max_pool=None, layer_norm=None, batch_norm=False):\n",
    "        \n",
    "        super(ConvLayer, self).__init__()\n",
    "        \n",
    "        net = [nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, \n",
    "                         stride=stride, padding=padding)]\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            net.append(nn.ReLU())\n",
    "        \n",
    "        elif activation == \"leakyrelu\": \n",
    "            net.append(nn.LeakyReLU())\n",
    "            \n",
    "        if layer_norm: \n",
    "            net.append(nn.LayerNorm(layer_norm)) # add layer normalization\n",
    "        \n",
    "        if batch_norm: \n",
    "            net.append(nn.BatchNorm3d(out_channels)) # add batch normalization\n",
    "        \n",
    "        if max_pool: \n",
    "            net.append(nn.MaxPool3d(max_pool)) # add max_pooling\n",
    "        \n",
    "        self.model = nn.Sequential(*net)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out = self.model(input)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\" Reshapes a 4d matrix to a 2d matrix. \"\"\"\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "        \n",
    "\n",
    "class ReshapeTensor(nn.Module):\n",
    "    def __init__(self, size): \n",
    "        super(ReshapeTensor, self).__init__()\n",
    "        self.size = size\n",
    "                \n",
    "    def forward(self, input):\n",
    "        return input.reshape([input.shape[0]] + self.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "############ cnn_setup -1 ##############\n",
    "######################################## \n",
    "\n",
    "class LargeCNN1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LargeCNN1, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   8,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(8,   8,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 128, 128)),\n",
    "            \n",
    "            ConvLayer(8,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=(1, 2, 2), padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "########################################\n",
    "############ cnn_setup -3 ##############\n",
    "######################################## \n",
    "\n",
    "class LargeCNN3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LargeCNN3, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   4,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(4,   4,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 128, 128)),\n",
    "            \n",
    "            ConvLayer(4,   8,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(8,  8,  kernel_size=5, stride=(1, 2, 2), padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            \n",
    "            ConvLayer(8,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "\n",
    "########################################\n",
    "############ cnn_setup -4 ##############\n",
    "######################################## \n",
    "\n",
    "class LargeCNN4(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LargeCNN4, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(1, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.interpolate(x, size=(12, 32, 32), mode='trilinear')    \n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "########################################\n",
    "############ cnn_setup -5 ##############\n",
    "######################################## \n",
    "\n",
    "class LargeCNN5(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LargeCNN5, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 128, 128)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=(1, 2, 2), padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(64,  128,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(128, 128,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(128, 256, kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(256, 256, kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "\n",
    "########################################\n",
    "############ cnn_setup -6 ##############\n",
    "######################################## \n",
    "\n",
    "class LargeCNN6(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LargeCNN6, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   8,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(8,   8,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 128, 128)),\n",
    "            \n",
    "            ConvLayer(8,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=(1, 2, 2), padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########## mapping_setup -1 #############\n",
    "########################################\n",
    "\n",
    "class LargeMapping1(nn.Module):\n",
    "\n",
    "    def __init__(self, ARGS):\n",
    "        super(LargeMapping1, self).__init__()\n",
    "        \n",
    "        self.n_gammas = ARGS.siren_hidden_layers + 1\n",
    "        self.dim_hidden = ARGS.dim_hidden\n",
    "        \n",
    "        self.gammas = nn.ModuleList()\n",
    "        for i in range(self.n_gammas):\n",
    "            self.gammas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(6144, 1024),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(1024, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, self.dim_hidden), \n",
    "                                       )\n",
    "                         )\n",
    "        \n",
    "        \n",
    "        self.betas = nn.ModuleList()\n",
    "        for i in range(self.n_gammas):\n",
    "            self.betas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(6144, 1024),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(1024, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, self.dim_hidden), \n",
    "                                       )\n",
    "                         )\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.empty(0).to(x.device)\n",
    "        \n",
    "        for gamma in self.gammas: \n",
    "            out = torch.cat((out, gamma(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        for beta in self.betas: \n",
    "            out = torch.cat((out, beta(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        return out[:, :self.n_gammas, :], out[:, self.n_gammas:, :]\n",
    "    \n",
    "\n",
    "########################################\n",
    "########## mapping_setup -2 #############\n",
    "########################################\n",
    "\n",
    "class LargeMapping2(nn.Module):\n",
    "\n",
    "    def __init__(self, ARGS):\n",
    "        super(LargeMapping2, self).__init__()\n",
    "        \n",
    "        self.n_gammas = ARGS.siren_hidden_layers + 1\n",
    "        self.dim_hidden = ARGS.dim_hidden\n",
    "        \n",
    "        self.gammas = nn.ModuleList()\n",
    "        for i in range(self.n_gammas):\n",
    "            self.gammas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(12288, 1024),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(1024, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, self.dim_hidden), \n",
    "                                       )\n",
    "                         )\n",
    "        \n",
    "        \n",
    "        self.betas = nn.ModuleList()\n",
    "        for i in range(self.n_gammas):\n",
    "            self.betas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(12288, 1024),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(1024, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, self.dim_hidden), \n",
    "                                       )\n",
    "                         )\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.empty(0).to(x.device)\n",
    "        \n",
    "        for gamma in self.gammas: \n",
    "            out = torch.cat((out, gamma(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        for beta in self.betas: \n",
    "            out = torch.cat((out, beta(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        return out[:, :self.n_gammas, :], out[:, self.n_gammas:, :]\n",
    "    \n",
    "\n",
    "########################################\n",
    "########## mapping_setup -5 #############\n",
    "########################################\n",
    "\n",
    "class LargeMapping5(nn.Module):\n",
    "\n",
    "    def __init__(self, ARGS):\n",
    "        super(LargeMapping5, self).__init__()\n",
    "        \n",
    "        self.n_gammas = ARGS.siren_hidden_layers + 1\n",
    "        self.dim_hidden = ARGS.dim_hidden\n",
    "        \n",
    "        self.gammas = nn.ModuleList()\n",
    "        for i in range(self.n_gammas):\n",
    "            self.gammas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(12288, 2048),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(2048, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, self.dim_hidden), \n",
    "                                       )\n",
    "                         )\n",
    "        \n",
    "        \n",
    "        self.betas = nn.ModuleList()\n",
    "        for i in range(self.n_gammas):\n",
    "            self.betas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(12288, 2048),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(2048, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, self.dim_hidden), \n",
    "                                       )\n",
    "                         )\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.empty(0).to(x.device)\n",
    "        \n",
    "        for gamma in self.gammas: \n",
    "            out = torch.cat((out, gamma(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        for beta in self.betas: \n",
    "            out = torch.cat((out, beta(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        return out[:, :self.n_gammas, :], out[:, self.n_gammas:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combi 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN1, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(24, 64, 64)),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(12, 32, 32)),\n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(12, 32, 32)),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(6, 16, 16)),\n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(6, 16, 16)),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(3, 8, 8)),\n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(3, 8, 8)),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(2, 4, 4)),\n",
    "            \n",
    "            Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mapping1, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(in_features=4096, out_features=2048, bias=True),\n",
    "                                   nn.LeakyReLU(.2),\n",
    "                                   \n",
    "                                   nn.Linear(in_features=2048, out_features=1024, bias=True),\n",
    "                                   nn.LeakyReLU(.2),\n",
    "                                   \n",
    "                                   nn.Linear(1024, 512), \n",
    "                                   ReshapeTensor([2, 256]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)  \n",
    "        \n",
    "        return out[:, 0, :], out[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mapping2, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(in_features=4096, out_features=2048, bias=True),\n",
    "                                   nn.LeakyReLU(.2),\n",
    "                                   \n",
    "                                   nn.Linear(in_features=2048, out_features=1024, bias=True),\n",
    "                                   nn.LeakyReLU(.2),\n",
    "                                   \n",
    "                                   nn.Linear(1024, 2048), \n",
    "                                   ReshapeTensor([8, 256]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)  \n",
    "                \n",
    "        return out[:, :4, :], out[:, 4:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combi 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN2, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(24, 64, 64)),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(12, 32, 32)),\n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(12, 32, 32)),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(6, 16, 16)),\n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(6, 16, 16)),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(3, 8, 8)),\n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(3, 8, 8)),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(2, 4, 4)),\n",
    "            \n",
    "            Flatten(),\n",
    "            \n",
    "            nn.Linear(in_features=4096, out_features=2048, bias=True),\n",
    "            nn.LeakyReLU(.2),\n",
    "\n",
    "            nn.Linear(in_features=2048, out_features=1024, bias=True),\n",
    "            nn.LeakyReLU(.2),\n",
    "         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mapping3, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(1024, 512), \n",
    "                                   ReshapeTensor([2, 256]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)  \n",
    "        \n",
    "        return out[:, 0, :], out[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Mapping4(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mapping4, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(1024, 2048), \n",
    "                                   ReshapeTensor([8, 256]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)  \n",
    "        \n",
    "        return out[:, :4, :], out[:, 4:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(            \n",
    "            ConvLayer(1,    16,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(16,   16,  kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True, max_pool=(1, 2, 2)),\n",
    "            \n",
    "            ConvLayer(16,   32,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(32,   32,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True),\n",
    "            \n",
    "            ConvLayer(32,   64,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(64,   64,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True),\n",
    "            \n",
    "            ConvLayer(64,   128,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(128,  128,  kernel_size=3, stride=2, padding=1, activation=\"relu\"), \n",
    "            \n",
    "            ConvLayer(128,   512,  kernel_size=(3, 4, 4), stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "\n",
    "class Encoder_1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Encoder_1, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(            \n",
    "            ConvLayer(1,    16,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(16,   16,  kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True, max_pool=(1, 2, 2)),\n",
    "            \n",
    "            ConvLayer(16,   32,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(32,   32,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True),\n",
    "            \n",
    "            ConvLayer(32,   64,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(64,   64,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True),\n",
    "            \n",
    "            ConvLayer(64,   128,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(128,  128,  kernel_size=3, stride=2, padding=1, activation=\"relu\",\n",
    "                      batch_norm=True),\n",
    "            \n",
    "            ConvLayer(128,   512,  kernel_size=(3, 4, 4), stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "\n",
    "class Encoder_2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Encoder_2, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(            \n",
    "            ConvLayer(1,    16,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(16,   16,  kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True, max_pool=(1, 2, 2)),\n",
    "            \n",
    "            ConvLayer(16,   32,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(32,   32,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True),\n",
    "            \n",
    "            ConvLayer(32,   64,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(64,   64,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True),\n",
    "            \n",
    "            ConvLayer(64,   128,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(128,  128,  kernel_size=3, stride=2, padding=1), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "\n",
    "class Encoder_Mapping(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder_Mapping, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(Flatten(), \n",
    "                                   nn.Linear(512, 512),\n",
    "                                   nn.LeakyReLU(.2),\n",
    "                                   nn.Linear(512, 512), \n",
    "                                   nn.LeakyReLU(.2),\n",
    "                                   nn.Linear(512, 512), \n",
    "                                   nn.LeakyReLU(.2),\n",
    "                                   nn.Linear(512, 2048), \n",
    "                                   ReshapeTensor([8, 256]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)  \n",
    "        \n",
    "        return out[:, :4, :], out[:, 4:, :]\n",
    "    \n",
    "class Encoder_Mapping_1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder_Mapping_1, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(Flatten(), \n",
    "                                   nn.Linear(512, 512),\n",
    "                                   nn.LeakyReLU(.2),\n",
    "                                   nn.Linear(512, 512), \n",
    "                                   nn.LeakyReLU(.2),\n",
    "                                   nn.Linear(512, 512), \n",
    "                                   nn.LeakyReLU(.2),\n",
    "                                   nn.Linear(512, 512), \n",
    "                                   ReshapeTensor([2, 256]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)  \n",
    "        \n",
    "        return out[:, 0, :], out[:, 1, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output 128, 3, 4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "############ cnn_setup 5 ###############\n",
    "########################################\n",
    "\n",
    "class CNN3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN3, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 64, 64)),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(3, 4, 4)),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "\n",
    "\n",
    "########################################\n",
    "############ cnn_setup 13 ##############\n",
    "########################################\n",
    "\n",
    "class CNN11(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN11, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 64, 64)),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, \n",
    "                      layer_norm=(3, 4, 4)),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "\n",
    "########################################\n",
    "############ cnn_setup 6 ###############\n",
    "########################################\n",
    "class CNN4(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN4, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(3, 4, 4)),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    \n",
    "########################################\n",
    "############ cnn_setup 7 ###############\n",
    "########################################    \n",
    "class CNN5(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN5, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 64, 64)),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "\n",
    "    \n",
    "########################################\n",
    "############ cnn_setup 8 ###############\n",
    "######################################## \n",
    "\n",
    "class CNN6(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN6, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "                      layer_norm=(24, 64, 64)),\n",
    "            ConvLayer(16,  16,  kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            ConvLayer(32,  32,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            ConvLayer(64,  64,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            ConvLayer(128, 128, kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      layer_norm=(3, 4, 4)),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "\n",
    "    \n",
    "########################################\n",
    "############ cnn_setup 9 ###############\n",
    "######################################## \n",
    "\n",
    "class CNN7(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN7, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(128, 128, kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      layer_norm=(3, 4, 4)),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "########################################\n",
    "############ cnn_setup 10 ##############\n",
    "######################################## \n",
    "\n",
    "class CNN8(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN8, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 64, 64)),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=(1, 2, 2), padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(3, 4, 4)),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "\n",
    "########################################\n",
    "############ cnn_setup 14 ##############\n",
    "######################################## \n",
    "\n",
    "class CNN12(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN12, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    \n",
    "########################################\n",
    "############ cnn_setup 15 ##############\n",
    "######################################## \n",
    "\n",
    "class CNN13(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN13, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "########################################\n",
    "############ cnn_setup 16 ##############\n",
    "######################################## \n",
    "\n",
    "class CNN14(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN14, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "\n",
    "########################################\n",
    "############ cnn_setup 17 ##############\n",
    "######################################## \n",
    "\n",
    "class CNN15(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN15, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(2, 2, 2), layer_norm=(24, 32, 32)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    \n",
    "########################################\n",
    "############ cnn_setup 18 ##############\n",
    "######################################## \n",
    "\n",
    "class CNN16(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN16, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=(1, 2, 2), padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "############ cnn_setup -2 ##############\n",
    "######################################## \n",
    "\n",
    "class LargeCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LargeCNN, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   8,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(8,   8,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 128, 128)),\n",
    "            \n",
    "            ConvLayer(8,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output 512, 1, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "############ cnn_setup 11 ##############\n",
    "######################################## \n",
    "\n",
    "class CNN9(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN9, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 64, 64)),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(3, 4, 4)),\n",
    "            \n",
    "            ConvLayer(128, 512, kernel_size=(3, 4, 4), stride=1, padding=0, activation=\"relu\", \n",
    "                      layer_norm=(1, 1, 1)),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "########################################\n",
    "############ cnn_setup 12 ##############\n",
    "######################################## \n",
    "\n",
    "class CNN10(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN10, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 64, 64)),\n",
    "            ConvLayer(16,  16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      max_pool=(1, 2, 2), layer_norm=(24, 64, 64)),\n",
    "            \n",
    "            ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(24, 32, 32)),\n",
    "            ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            \n",
    "            ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(12, 16, 16)),\n",
    "            ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            \n",
    "            ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(6, 8, 8)),\n",
    "            ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "                      layer_norm=(3, 4, 4)),\n",
    "            \n",
    "            ConvLayer(128, 512, kernel_size=(3, 4, 4), stride=1, padding=0,  activation=\"relu\"),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input 512, 1, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########## mapping_setup 6 #############\n",
    "########################################\n",
    "\n",
    "class Encoder_Mapping_2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder_Mapping_2, self).__init__()\n",
    "        \n",
    "        self.gammas = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.gammas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(512, 512),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, 256), \n",
    "                                       )\n",
    "                         )\n",
    "        \n",
    "        self.betas = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.betas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(512, 512),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, 256), \n",
    "                                       )\n",
    "                         )\n",
    "             \n",
    "    def forward(self, x):\n",
    "        out = torch.empty(0).to(x.device)\n",
    "        \n",
    "        for gamma in self.gammas: \n",
    "            out = torch.cat((out, gamma(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        for beta in self.betas: \n",
    "            out = torch.cat((out, beta(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        return out[:, :4, :], out[:, 4:, :]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input 128, 3, 4, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########## mapping_setup 7 #############\n",
    "########################################\n",
    "\n",
    "class Encoder_Mapping_3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder_Mapping_3, self).__init__()\n",
    "        \n",
    "        self.gammas = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.gammas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(6144, 1024),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(1024, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, 256), \n",
    "                                       )\n",
    "                         )\n",
    "        \n",
    "        \n",
    "        self.betas = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.betas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(6144, 1024),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(1024, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, 256), \n",
    "                                       )\n",
    "                         )\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.empty(0).to(x.device)\n",
    "        \n",
    "        for gamma in self.gammas: \n",
    "            out = torch.cat((out, gamma(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        for beta in self.betas: \n",
    "            out = torch.cat((out, beta(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        return out[:, :4, :], out[:, 4:, :]\n",
    "    \n",
    "    \n",
    "########################################\n",
    "########## mapping_setup 8 #############\n",
    "########################################\n",
    "\n",
    "class Encoder_Mapping_4(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder_Mapping_4, self).__init__()\n",
    "        \n",
    "        self.gammas = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.gammas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(6144, 2048),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(2048, 1024),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(1024, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, 256), \n",
    "                                       )\n",
    "                         )\n",
    "        \n",
    "        \n",
    "        self.betas = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.betas.append(nn.Sequential(Flatten(), \n",
    "                                      nn.Linear(6144, 2048),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(2048, 1024),\n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(1024, 512), \n",
    "                                      nn.LeakyReLU(.2),\n",
    "                                      nn.Linear(512, 256), \n",
    "                                       )\n",
    "                         )\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.empty(0).to(x.device)\n",
    "        \n",
    "        for gamma in self.gammas: \n",
    "            out = torch.cat((out, gamma(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        for beta in self.betas: \n",
    "            out = torch.cat((out, beta(x).unsqueeze(1)), 1)\n",
    "        \n",
    "        return out[:, :4, :], out[:, 4:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping = Encoder_Mapping_4().cuda()\n",
    "# # input_shape = (24, 512, 1, 1, 1)\n",
    "# input_shape = (24, 128, 3, 4, 4)\n",
    "# inp = torch.randn(input_shape).cuda()\n",
    "\n",
    "# summary(mapping, input_size=input_shape, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Imported CNN and Mapping functions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
