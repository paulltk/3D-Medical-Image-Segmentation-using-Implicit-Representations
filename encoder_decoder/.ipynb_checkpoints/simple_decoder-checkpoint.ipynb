{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/data_classes/new_dataset.ipynb to script\n",
      "[NbConvertApp] Writing 5780 bytes to py_files/new_dataset.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/model_classes/cnn_model.ipynb to script\n",
      "[NbConvertApp] Writing 8572 bytes to py_files/cnn_model.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/model_classes/pigan_model.ipynb to script\n",
      "[NbConvertApp] Writing 3587 bytes to py_files/pigan_model.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/pi-gan/pi_gan_functions.ipynb to script\n",
      "[NbConvertApp] Writing 21735 bytes to py_files/pi_gan_functions.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/pi-gan_sequential/seq_pi_gan_functions.ipynb to script\n",
      "[NbConvertApp] Writing 21854 bytes to py_files/seq_pi_gan_functions.py\n",
      "Saved py files.\n"
     ]
    }
   ],
   "source": [
    "%run /home/ptenkaate/scratch/Master-Thesis/convert_ipynb_to_py_files.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported CNN and Mapping functions.\n",
      "Imported PI-Gan model.\n",
      "Loaded all helper functions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ptenkaate/.local/lib/python3.6/site-packages/matplotlib/backends/qt_editor/figureoptions.py:11: MatplotlibDeprecationWarning: \n",
      "The support for Qt4  was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "  from matplotlib.backends.qt_compat import QtGui\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math \n",
    "import skimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from py_files.new_dataset import *\n",
    "\n",
    "from py_files.cnn_model import *\n",
    "from py_files.pigan_model import *\n",
    "\n",
    "from py_files.seq_pi_gan_functions import *\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  --                        --\n",
       "├─Sequential: 1-1                        [32, 512, 1, 1, 1]        --\n",
       "│    └─ConvLayer: 2-1                    [32, 16, 24, 64, 64]      448\n",
       "│    └─ConvLayer: 2-2                    [32, 16, 24, 32, 32]      6,960\n",
       "│    └─ConvLayer: 2-3                    [32, 32, 24, 32, 32]      13,856\n",
       "│    └─ConvLayer: 2-4                    [32, 32, 12, 16, 16]      27,744\n",
       "│    └─ConvLayer: 2-5                    [32, 64, 12, 16, 16]      55,360\n",
       "│    └─ConvLayer: 2-6                    [32, 64, 6, 8, 8]         110,784\n",
       "│    └─ConvLayer: 2-7                    [32, 128, 6, 8, 8]        221,312\n",
       "│    └─ConvLayer: 2-8                    [32, 128, 3, 4, 4]        442,496\n",
       "│    └─ConvLayer: 2-9                    [32, 512, 1, 1, 1]        3,146,240\n",
       "==========================================================================================\n",
       "Total params: 4,025,200\n",
       "Trainable params: 4,025,200\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 175.22\n",
       "==========================================================================================\n",
       "Input size (MB): 12.58\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 16.10\n",
       "Estimated Total Size (MB): 28.68\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                 kernel_size, stride, padding, \n",
    "                 activation=\"relu\", max_pool=None, layer_norm=None, batch_norm=False):\n",
    "        \n",
    "        super(ConvLayer, self).__init__()\n",
    "        \n",
    "        net = [nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, \n",
    "                         stride=stride, padding=padding)]\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            net.append(nn.ReLU())\n",
    "        \n",
    "        elif activation == \"leakyrelu\": \n",
    "            net.append(nn.LeakyReLU())\n",
    "            \n",
    "        if layer_norm: \n",
    "            net.append(nn.LayerNorm(layer_norm)) # add layer normalization\n",
    "        \n",
    "        if batch_norm: \n",
    "            net.append(nn.BatchNorm3d(out_channels)) # add layer normalization\n",
    "        \n",
    "        if max_pool: \n",
    "            net.append(nn.MaxPool3d(max_pool)) # add max_pooling\n",
    "        \n",
    "        self.model = nn.Sequential(*net)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out = self.model(input)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "#             ######## val loss 0.00072\n",
    "#             ConvLayer(1,   32,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "#             ConvLayer(32,   32,  kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "#                       batch_norm=True, max_pool=(1, 2, 2)),\n",
    "            \n",
    "#             ConvLayer(32,   64,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "#             ConvLayer(64,   64,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "#                       batch_norm=True),\n",
    "            \n",
    "#             ConvLayer(64,   128,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "#             ConvLayer(128,   128,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "#                       batch_norm=True),\n",
    "            \n",
    "#             ConvLayer(128,   256,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "#             ConvLayer(256,   256,  kernel_size=3, stride=2, padding=1)\n",
    "#             ######## val loss 0.00072\n",
    "            \n",
    "            ConvLayer(1,   16,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(16,   16,  kernel_size=3, stride=1, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True, max_pool=(1, 2, 2)),\n",
    "            \n",
    "            ConvLayer(16,   32,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(32,   32,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True),\n",
    "            \n",
    "            ConvLayer(32,   64,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(64,   64,  kernel_size=3, stride=2, padding=1, activation=\"relu\", \n",
    "                      batch_norm=True),\n",
    "            \n",
    "            ConvLayer(64,   128,  kernel_size=3, stride=1, padding=1, activation=\"relu\"),\n",
    "            ConvLayer(128,   128,  kernel_size=3, stride=2, padding=1, ),\n",
    "            \n",
    "            ConvLayer(128,   512,  kernel_size=(3, 4, 4), stride=1, padding=0)\n",
    "#             ConvLayer(128,   128,  kernel_size=(3, 3, 3), stride=1, padding=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             ConvLayer(1,   32,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "#             ConvLayer(32,   32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", \n",
    "#                       batch_norm=True, max_pool=(1, 2, 2)),\n",
    "            \n",
    "#             ConvLayer(32,   64,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "#             ConvLayer(64,   64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "#                       batch_norm=True),\n",
    "            \n",
    "#             ConvLayer(64,   128,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "#             ConvLayer(128,   128,  kernel_size=5, stride=2, padding=2, activation=\"relu\", \n",
    "#                       batch_norm=True),\n",
    "            \n",
    "#             ConvLayer(128,   256,  kernel_size=5, stride=1, padding=2, activation=\"relu\"),\n",
    "#             ConvLayer(256,   256,  kernel_size=5, stride=2, padding=2)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             ConvLayer(1,   16,  kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(24, 64, 64)),\n",
    "#             ConvLayer(16,  16,  kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(12, 32, 32)),\n",
    "#             ConvLayer(16,  32,  kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(12, 32, 32)),\n",
    "#             ConvLayer(32,  32,  kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(6, 16, 16)),\n",
    "#             ConvLayer(32,  64,  kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(6, 16, 16)),\n",
    "#             ConvLayer(64,  64,  kernel_size=5, stride=2, padding=2, activation=\"relu\", layer_norm=(3, 8, 8)),\n",
    "#             ConvLayer(64,  128, kernel_size=5, stride=1, padding=2, activation=\"relu\", layer_norm=(3, 8, 8)),\n",
    "#             ConvLayer(128, 128, kernel_size=5, stride=2, padding=2, activation=None, layer_norm=None),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "\n",
    "input_size = (32, 1, 24, 64, 64)\n",
    "encoder = Encoder()\n",
    "summary(encoder, input_size=input_size, depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 512, 1, 1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  --                        --\n",
       "├─Sequential: 1-1                        [32, 1, 24, 64, 64]       --\n",
       "│    └─ConvTranspose3d: 2-1              [32, 128, 3, 4, 4]        3,145,856\n",
       "│    └─ReLU: 2-2                         [32, 128, 3, 4, 4]        --\n",
       "│    └─ConvTranspose3d: 2-3              [32, 64, 6, 8, 8]         221,248\n",
       "│    └─ReLU: 2-4                         [32, 64, 6, 8, 8]         --\n",
       "│    └─ConvTranspose3d: 2-5              [32, 32, 12, 16, 16]      55,328\n",
       "│    └─ReLU: 2-6                         [32, 32, 12, 16, 16]      --\n",
       "│    └─ConvTranspose3d: 2-7              [32, 16, 24, 32, 32]      13,840\n",
       "│    └─ReLU: 2-8                         [32, 16, 24, 32, 32]      --\n",
       "│    └─ConvTranspose3d: 2-9              [32, 1, 24, 64, 64]       433\n",
       "==========================================================================================\n",
       "Total params: 3,436,705\n",
       "Trainable params: 3,436,705\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 25.33\n",
       "==========================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 158.86\n",
       "Params size (MB): 13.75\n",
       "Estimated Total Size (MB): 172.67\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "#             ######## val loss 0.00072\n",
    "#             nn.ConvTranspose3d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose3d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose3d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose3d(32, 1, 3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1)),\n",
    "#             ######## val loss 0.00072\n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose3d(512, 128, (3, 4, 4), stride=2, padding=0, output_padding=0),\n",
    "#             nn.ConvTranspose3d(128, 128, (3, 3, 3), stride=1, padding=0, output_padding=0),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(16, 1, 3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1)),\n",
    "            \n",
    "#             nn.ConvTranspose3d(128, 64, 5, stride=2, padding=2, output_padding=(0, 1, 1)),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose3d(64, 32, 5, stride=2, padding=2, output_padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose3d(32, 16, 5, stride=2, padding=2, output_padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose3d(16, 1, 5, stride=2, padding=2, output_padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "output_size = encoder(torch.randn(input_size).cuda()).shape\n",
    "print(tuple(output_size))\n",
    "decoder = Decoder()\n",
    "summary(decoder, input_size=output_size, depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ARGS class initialized.\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 84\n",
      "Val subjects: 28\n",
      "Test subjects: 28\n",
      "('16-03-18_Sanjay2_kt-pca_done2', '16-05-11 Sandra_kt-pca_done2', '16-06-24_Ot_kt-pca_done1', '17-02-08_VaderUiterwijk', '16-04-20_105_done', '16-06-29_115_done1', '16-06-29_116_done1', '16-08-17_117_done1', '16-08-19_119_done1', '16-08-26_121_done1', 'RESV_025', 'RESV_103', 'RESV_104', 'RESV_301', 'RESV_306', '16-04-15_205_done', '16-04-29_214_done', '16-05-18_215_done', '16-05-20_217_done', '16-06-03_219_done', '16-06-15_202_done', '16-07-06_216_done1', '16-07-22_226_done1', '16-08-12_236_done1')\n"
     ]
    }
   ],
   "source": [
    "ARGS = init_ARGS()\n",
    "\n",
    "ARGS.flip = True, \n",
    "ARGS.crop = True \n",
    "ARGS.rotate = True\n",
    "ARGS.stretch = True\n",
    "ARGS.translate = True\n",
    "\n",
    "ARGS.epochs = 200\n",
    "\n",
    "ARGS.batch_size = 24\n",
    "ARGS.dataset = \"small\"\n",
    "\n",
    "##### path to wich the model should be saved #####\n",
    "path = get_folder(ARGS)\n",
    "\n",
    "##### save ARGS #####\n",
    "with open(f\"{path}/ARGS.txt\", \"w\") as f:\n",
    "    print(vars(ARGS), file=f)\n",
    "\n",
    "##### data preparation #####\n",
    "train_dl, val_dl, test_dl = initialize_dataloaders(ARGS) \n",
    "\n",
    "print(next(iter(test_dl))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# slice_len = 64\n",
    "\n",
    "\n",
    "# ARGS.flip = True, \n",
    "# ARGS.crop = True \n",
    "# ARGS.rotate = True\n",
    "# ARGS.stretch = True\n",
    "# ARGS.translate = True\n",
    "\n",
    "# for batch in train_dl: \n",
    "#     break\n",
    "    \n",
    "\n",
    "# t = time.time()\n",
    "\n",
    "\n",
    "# batch = transform_batch(batch, ARGS)\n",
    "\n",
    "# print(time.time() - t)\n",
    "\n",
    "# idx, subj, proj, pcmras, masks = batch\n",
    "\n",
    "# titles = np.array([[f\"{i} {s} {p}\"] * pcmra.shape[1] for i, s, p, pcmra in zip(idx, subj, proj, pcmras)]).flatten()\n",
    "\n",
    "# s_pcmras = pcmras.contiguous().view(-1, slice_len, slice_len).cpu().detach().permute(1, 2, 0)\n",
    "# s_masks = masks.contiguous().view(-1, slice_len, slice_len).cpu().detach().permute(1, 2, 0)\n",
    "# s_both = s_pcmras + s_masks\n",
    "\n",
    "# window = Show_images(titles, (s_pcmras.numpy(), \"pcmras\"), (s_masks.numpy(), \"masks\"), (s_both.numpy(), \"both\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_run = \"pi-gan 17-05-2021 14:40:52 \"\n",
    "\n",
    "encoder = Encoder().cuda()\n",
    "decoder = Decoder().cuda()\n",
    "\n",
    "# encoder.load_state_dict(torch.load(f\"saved_runs/{saved_run}/cnn_train.pt\"))\n",
    "# decoder.load_state_dict(torch.load(f\"saved_runs/{saved_run}/decoder_train.pt\"))\n",
    "\n",
    "e_optim = torch.optim.Adam(lr=1e-4, params=encoder.parameters())\n",
    "d_optim = torch.optim.Adam(lr=1e-4, params=decoder.parameters())\n",
    "\n",
    "# encoder.load_state_dict(torch.load(f\"saved_runs/{saved_run}/cnn_train.pt\"))\n",
    "# decoder.load_state_dict(torch.load(f\"saved_runs/{saved_run}/decoder_train.pt\"))\n",
    "\n",
    "\n",
    "##### loss function #####\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "losses = np.empty((0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 took 1.35 seconds.\n",
      "train loss 0.0009114517451962456\n",
      "val loss 0.0013110591098666191\n",
      "Epoch 20 took 1.47 seconds.\n",
      "train loss 0.000880210121977143\n",
      "val loss 0.0012889749486930668\n",
      "Epoch 40 took 1.28 seconds.\n",
      "train loss 0.0008998968551168218\n",
      "val loss 0.001283119956497103\n",
      "Epoch 60 took 1.45 seconds.\n",
      "train loss 0.0008709115936653689\n",
      "val loss 0.0012869283673353493\n",
      "Epoch 80 took 1.29 seconds.\n",
      "train loss 0.0008746554231038317\n",
      "val loss 0.0012754518538713455\n",
      "Epoch 100 took 1.49 seconds.\n",
      "train loss 0.00090795784490183\n",
      "val loss 0.001270233653485775\n",
      "Epoch 120 took 1.49 seconds.\n",
      "train loss 0.000845207687234506\n",
      "val loss 0.0012653495650738478\n",
      "Epoch 140 took 1.43 seconds.\n",
      "train loss 0.000899824924999848\n",
      "val loss 0.0012654326274059713\n",
      "Epoch 160 took 1.46 seconds.\n",
      "train loss 0.0008582500886404887\n",
      "val loss 0.001254416594747454\n",
      "Epoch 180 took 1.42 seconds.\n",
      "train loss 0.0008624179026810452\n",
      "val loss 0.0012661151704378426\n",
      "Epoch 200 took 1.41 seconds.\n",
      "train loss 0.0008677841251483187\n",
      "val loss 0.001258713484276086\n",
      "Epoch 220 took 1.09 seconds.\n",
      "train loss 0.0008687567315064371\n",
      "val loss 0.001273750385735184\n",
      "Epoch 240 took 1.1 seconds.\n",
      "train loss 0.0008584138995502144\n",
      "val loss 0.0012589263496920466\n",
      "Epoch 260 took 1.08 seconds.\n",
      "train loss 0.0008246960205724463\n",
      "val loss 0.0012618369073607028\n",
      "Epoch 280 took 1.08 seconds.\n",
      "train loss 0.0008311989658977836\n",
      "val loss 0.0012710971059277654\n",
      "Epoch 300 took 1.1 seconds.\n",
      "train loss 0.0008182948076864704\n",
      "val loss 0.0012760431272909045\n",
      "Epoch 320 took 1.05 seconds.\n",
      "train loss 0.0008778092014836147\n",
      "val loss 0.0012714030453935266\n",
      "Epoch 340 took 1.05 seconds.\n",
      "train loss 0.0008547722391085699\n",
      "val loss 0.0012602456263266504\n",
      "Epoch 360 took 1.1 seconds.\n",
      "train loss 0.0008682289771968499\n",
      "val loss 0.0012437875848263502\n",
      "Epoch 380 took 1.4 seconds.\n",
      "train loss 0.0008203393372241408\n",
      "val loss 0.0012575862347148359\n",
      "Epoch 400 took 1.43 seconds.\n",
      "train loss 0.0007971919840201735\n",
      "val loss 0.00127125607104972\n",
      "Epoch 420 took 1.45 seconds.\n",
      "train loss 0.0008757909818086773\n",
      "val loss 0.0012611935380846262\n",
      "Epoch 440 took 1.39 seconds.\n",
      "train loss 0.0008428823930444196\n",
      "val loss 0.0012394544901326299\n",
      "Epoch 460 took 1.36 seconds.\n",
      "train loss 0.0008435658819507807\n",
      "val loss 0.0012393007054924965\n",
      "Epoch 480 took 1.39 seconds.\n",
      "train loss 0.0008087318274192512\n",
      "val loss 0.0012378287501633167\n",
      "Epoch 500 took 1.43 seconds.\n",
      "train loss 0.0008624926413176581\n",
      "val loss 0.001246521482244134\n",
      "Epoch 520 took 1.41 seconds.\n",
      "train loss 0.00082309429126326\n",
      "val loss 0.0012471505906432867\n",
      "Epoch 540 took 1.39 seconds.\n",
      "train loss 0.0008042091794777662\n",
      "val loss 0.0012695243349298835\n",
      "Epoch 560 took 1.05 seconds.\n",
      "train loss 0.0008635323465568945\n",
      "val loss 0.0012506881612353027\n",
      "Epoch 580 took 1.05 seconds.\n",
      "train loss 0.0007792510587023571\n",
      "val loss 0.0012438218691386282\n",
      "Epoch 600 took 1.05 seconds.\n",
      "train loss 0.0008067252347245812\n",
      "val loss 0.0012391908094286919\n",
      "Epoch 620 took 1.05 seconds.\n",
      "train loss 0.0008585575415054336\n",
      "val loss 0.001233940594829619\n",
      "Epoch 640 took 1.05 seconds.\n",
      "train loss 0.0009204431698890403\n",
      "val loss 0.0012740310048684478\n",
      "Epoch 660 took 1.05 seconds.\n",
      "train loss 0.0008269799436675385\n",
      "val loss 0.0012467845808714628\n",
      "Epoch 680 took 1.06 seconds.\n",
      "train loss 0.0008266627992270514\n",
      "val loss 0.0012377442326396704\n",
      "Epoch 700 took 1.05 seconds.\n",
      "train loss 0.0008277792512672022\n",
      "val loss 0.0012452269438654184\n",
      "Epoch 720 took 1.05 seconds.\n",
      "train loss 0.0008323595393449068\n",
      "val loss 0.0012475709663704038\n",
      "Epoch 740 took 1.05 seconds.\n",
      "train loss 0.0008404272375628352\n",
      "val loss 0.00123183912364766\n",
      "Epoch 760 took 1.05 seconds.\n",
      "train loss 0.0008264835487352684\n",
      "val loss 0.0012306328280828893\n",
      "Epoch 780 took 1.05 seconds.\n",
      "train loss 0.0008460298704449087\n",
      "val loss 0.0012617565225809813\n",
      "Epoch 800 took 1.07 seconds.\n",
      "train loss 0.0008367842092411593\n",
      "val loss 0.0012300317757762969\n",
      "Epoch 820 took 1.07 seconds.\n",
      "train loss 0.0008836540946504101\n",
      "val loss 0.001256696821656078\n",
      "Epoch 840 took 1.09 seconds.\n",
      "train loss 0.0008093240467133\n",
      "val loss 0.0012305999989621341\n",
      "Epoch 860 took 1.05 seconds.\n",
      "train loss 0.0007680970011278987\n",
      "val loss 0.0012304074480198324\n",
      "Epoch 880 took 1.05 seconds.\n",
      "train loss 0.000776226501329802\n",
      "val loss 0.001221105339936912\n",
      "Epoch 900 took 1.06 seconds.\n",
      "train loss 0.0007962098170537502\n",
      "val loss 0.0012391157215461135\n",
      "Epoch 920 took 1.08 seconds.\n",
      "train loss 0.0008412027527811006\n",
      "val loss 0.0012217988260090351\n",
      "Epoch 940 took 1.09 seconds.\n",
      "train loss 0.0008884727285476401\n",
      "val loss 0.0012463417369872332\n",
      "Epoch 960 took 1.09 seconds.\n",
      "train loss 0.0008654595731059089\n",
      "val loss 0.0012262434465810657\n",
      "Epoch 980 took 1.09 seconds.\n",
      "train loss 0.0008139228448271751\n",
      "val loss 0.0012513346155174077\n",
      "Epoch 1000 took 1.12 seconds.\n",
      "train loss 0.0007930360006866977\n",
      "val loss 0.0012168033863417804\n",
      "Epoch 1020 took 1.11 seconds.\n",
      "train loss 0.0008562582079321146\n",
      "val loss 0.0012433589436113834\n",
      "Epoch 1040 took 1.09 seconds.\n",
      "train loss 0.000808210505056195\n",
      "val loss 0.0012281196541152894\n",
      "Epoch 1060 took 1.08 seconds.\n",
      "train loss 0.0007470528216799721\n",
      "val loss 0.0012458145502023399\n",
      "Epoch 1080 took 1.12 seconds.\n",
      "train loss 0.0007995013147592545\n",
      "val loss 0.0012124376371502876\n",
      "Epoch 1100 took 1.05 seconds.\n",
      "train loss 0.00076384907879401\n",
      "val loss 0.0012144679785706103\n",
      "Epoch 1120 took 1.05 seconds.\n",
      "train loss 0.000818310902104713\n",
      "val loss 0.001214199757669121\n",
      "Epoch 1140 took 1.07 seconds.\n",
      "train loss 0.0007700660498812795\n",
      "val loss 0.0012115659774281085\n",
      "Epoch 1160 took 1.07 seconds.\n",
      "train loss 0.0008145000028889626\n",
      "val loss 0.001215033233165741\n",
      "Epoch 1180 took 1.07 seconds.\n",
      "train loss 0.0009312983165727928\n",
      "val loss 0.0012146674562245607\n",
      "Epoch 1200 took 1.06 seconds.\n",
      "train loss 0.0008124270389089361\n",
      "val loss 0.0012270243605598807\n",
      "Epoch 1220 took 1.05 seconds.\n",
      "train loss 0.0007617667142767459\n",
      "val loss 0.0012307982542552054\n",
      "Epoch 1240 took 1.05 seconds.\n",
      "train loss 0.0008069990872172639\n",
      "val loss 0.0012102242326363921\n",
      "Epoch 1260 took 1.06 seconds.\n",
      "train loss 0.0007901475328253582\n",
      "val loss 0.001203831227030605\n",
      "Epoch 1280 took 1.07 seconds.\n",
      "train loss 0.0007650761544937268\n",
      "val loss 0.0012346411822363734\n",
      "Epoch 1300 took 1.09 seconds.\n",
      "train loss 0.0007923484226921573\n",
      "val loss 0.0011998240952380002\n",
      "Epoch 1320 took 1.05 seconds.\n",
      "train loss 0.0007751011580694467\n",
      "val loss 0.0011996413231827319\n",
      "Epoch 1340 took 1.1 seconds.\n",
      "train loss 0.0007821358740329742\n",
      "val loss 0.0012039594002999365\n",
      "Epoch 1360 took 1.06 seconds.\n",
      "train loss 0.0007634689682163298\n",
      "val loss 0.001194480457343161\n",
      "Epoch 1380 took 1.08 seconds.\n",
      "train loss 0.0008047369337873533\n",
      "val loss 0.001239319797605276\n",
      "Epoch 1400 took 1.1 seconds.\n",
      "train loss 0.0007667712052352726\n",
      "val loss 0.0012082290486432612\n",
      "Epoch 1420 took 1.05 seconds.\n",
      "train loss 0.0007893688598414883\n",
      "val loss 0.0012039901339448988\n",
      "Epoch 1440 took 1.06 seconds.\n",
      "train loss 0.0007024955993983895\n",
      "val loss 0.0012022301671095192\n",
      "Epoch 1460 took 1.05 seconds.\n",
      "train loss 0.0007772452081553638\n",
      "val loss 0.0012041004956699908\n",
      "Epoch 1480 took 1.05 seconds.\n",
      "train loss 0.0008539722621208057\n",
      "val loss 0.0012185684172436595\n",
      "Epoch 1500 took 1.09 seconds.\n",
      "train loss 0.0008531694911653176\n",
      "val loss 0.001212585426401347\n",
      "Epoch 1520 took 1.07 seconds.\n",
      "train loss 0.0007740010623820126\n",
      "val loss 0.0012091773678548634\n",
      "Epoch 1540 took 1.09 seconds.\n",
      "train loss 0.000789936282671988\n",
      "val loss 0.001204059342853725\n",
      "Epoch 1560 took 1.12 seconds.\n",
      "train loss 0.0007283013983396813\n",
      "val loss 0.0012131643015891314\n",
      "Epoch 1580 took 1.09 seconds.\n",
      "train loss 0.0007613992493133992\n",
      "val loss 0.0011933418572880328\n",
      "Epoch 1600 took 1.09 seconds.\n",
      "train loss 0.0007862777856644243\n",
      "val loss 0.0011950595653615892\n",
      "Epoch 1620 took 1.09 seconds.\n",
      "train loss 0.0007930866413516924\n",
      "val loss 0.0012423141160979867\n",
      "Epoch 1640 took 1.05 seconds.\n",
      "train loss 0.0007349504448939115\n",
      "val loss 0.0012016540858894587\n",
      "Epoch 1660 took 1.1 seconds.\n",
      "train loss 0.0007174666679929942\n",
      "val loss 0.0011869726586155593\n",
      "Epoch 1680 took 1.07 seconds.\n",
      "train loss 0.0007243580330396071\n",
      "val loss 0.001189128146506846\n",
      "Epoch 1700 took 1.09 seconds.\n",
      "train loss 0.000743469296139665\n",
      "val loss 0.001193055883049965\n",
      "Epoch 1720 took 1.11 seconds.\n",
      "train loss 0.0007736504776403308\n",
      "val loss 0.0011890923487953842\n",
      "Epoch 1740 took 1.1 seconds.\n",
      "train loss 0.0007950732397148386\n",
      "val loss 0.0011954981600865722\n",
      "Epoch 1760 took 1.11 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.0007245516899274662\n",
      "val loss 0.0011912455083802342\n",
      "Epoch 1780 took 1.12 seconds.\n",
      "train loss 0.000737740847398527\n",
      "val loss 0.00119654624722898\n",
      "Epoch 1800 took 1.07 seconds.\n",
      "train loss 0.0007714774983469397\n",
      "val loss 0.001189257891383022\n",
      "Epoch 1820 took 1.07 seconds.\n",
      "train loss 0.0007981982489582151\n",
      "val loss 0.0011870345915667713\n",
      "Epoch 1840 took 1.07 seconds.\n",
      "train loss 0.0007489428098779172\n",
      "val loss 0.0011838816571980715\n",
      "Epoch 1860 took 1.07 seconds.\n",
      "train loss 0.0007097396883182228\n",
      "val loss 0.0011923405108973384\n",
      "Epoch 1880 took 1.07 seconds.\n",
      "train loss 0.000790736434282735\n",
      "val loss 0.0011961497948504984\n",
      "Epoch 1900 took 1.07 seconds.\n",
      "train loss 0.0007407545344904065\n",
      "val loss 0.001184987893793732\n",
      "Epoch 1920 took 1.05 seconds.\n",
      "train loss 0.0007706378673901781\n",
      "val loss 0.001200356346089393\n",
      "Epoch 1940 took 1.05 seconds.\n",
      "train loss 0.0007850801630411297\n",
      "val loss 0.0012005132157355547\n",
      "Epoch 1960 took 1.05 seconds.\n",
      "train loss 0.0007494584278902039\n",
      "val loss 0.0011739840847440064\n",
      "Epoch 1980 took 1.08 seconds.\n",
      "train loss 0.0007807432411937043\n",
      "val loss 0.0011780531494878232\n",
      "Epoch 2000 took 1.06 seconds.\n",
      "train loss 0.0007060600182740018\n",
      "val loss 0.0011962918797507882\n",
      "Epoch 2020 took 1.11 seconds.\n",
      "train loss 0.000766808123444207\n",
      "val loss 0.0011924566351808608\n",
      "Epoch 2040 took 1.06 seconds.\n",
      "train loss 0.0007012535352259874\n",
      "val loss 0.001179651590064168\n",
      "Epoch 2060 took 1.07 seconds.\n",
      "train loss 0.0007306483603315428\n",
      "val loss 0.001180689549073577\n",
      "Epoch 2080 took 1.06 seconds.\n",
      "train loss 0.0007273624942172319\n",
      "val loss 0.0012021278380416334\n",
      "Epoch 2100 took 1.09 seconds.\n",
      "train loss 0.0007378779991995543\n",
      "val loss 0.0011748276883736253\n",
      "Epoch 2120 took 1.06 seconds.\n",
      "train loss 0.0007268343179021031\n",
      "val loss 0.0011773325968533754\n",
      "Epoch 2140 took 1.05 seconds.\n",
      "train loss 0.0007667656318517402\n",
      "val loss 0.0011786696850322187\n",
      "Epoch 2160 took 1.06 seconds.\n",
      "train loss 0.0007561530510429293\n",
      "val loss 0.0011830938747152686\n",
      "Epoch 2180 took 1.08 seconds.\n",
      "train loss 0.0007437107851728797\n",
      "val loss 0.00117368035716936\n",
      "Epoch 2200 took 1.06 seconds.\n",
      "train loss 0.0007044601952657104\n",
      "val loss 0.0011823548702523112\n",
      "Epoch 2220 took 1.07 seconds.\n",
      "train loss 0.0007304805476451293\n",
      "val loss 0.001174920762423426\n",
      "Epoch 2240 took 1.08 seconds.\n",
      "train loss 0.0007609965832671151\n",
      "val loss 0.0011836567427963018\n",
      "Epoch 2260 took 1.05 seconds.\n",
      "train loss 0.0006990112015046179\n",
      "val loss 0.0011769791599363089\n",
      "Epoch 2280 took 1.1 seconds.\n",
      "train loss 0.0006845832685939968\n",
      "val loss 0.0011712951818481088\n",
      "Epoch 2300 took 1.05 seconds.\n",
      "train loss 0.0007234402291942388\n",
      "val loss 0.0011814276804216206\n",
      "Epoch 2320 took 1.06 seconds.\n",
      "train loss 0.0007208327733678743\n",
      "val loss 0.001172957185190171\n",
      "Epoch 2340 took 1.09 seconds.\n",
      "train loss 0.0007189708558144048\n",
      "val loss 0.001178514095954597\n",
      "Epoch 2360 took 1.05 seconds.\n",
      "train loss 0.0007742616580799222\n",
      "val loss 0.0011787328985519707\n",
      "Epoch 2380 took 1.09 seconds.\n",
      "train loss 0.0007399385503958911\n",
      "val loss 0.0011739826877601445\n",
      "Epoch 2400 took 1.05 seconds.\n",
      "train loss 0.0007425633084494621\n",
      "val loss 0.001175918965600431\n",
      "Epoch 2420 took 1.09 seconds.\n",
      "train loss 0.0007292544905794784\n",
      "val loss 0.0011701141484081745\n",
      "Epoch 2440 took 1.07 seconds.\n",
      "train loss 0.0007217099919216707\n",
      "val loss 0.0011832506279461086\n",
      "Epoch 2460 took 1.06 seconds.\n",
      "train loss 0.0007775658887112513\n",
      "val loss 0.0011735724401660264\n",
      "Epoch 2480 took 1.05 seconds.\n",
      "train loss 0.0007426949596265331\n",
      "val loss 0.0011758613400161266\n",
      "Epoch 2500 took 1.08 seconds.\n",
      "train loss 0.0007666305464226753\n",
      "val loss 0.0011749385157600045\n",
      "Epoch 2520 took 1.09 seconds.\n",
      "train loss 0.0006925335474079475\n",
      "val loss 0.0011736329179257154\n",
      "Epoch 2540 took 1.09 seconds.\n",
      "train loss 0.0007363331242231652\n",
      "val loss 0.0011682023759931326\n",
      "Epoch 2560 took 1.07 seconds.\n",
      "train loss 0.000746222518500872\n",
      "val loss 0.0011707462253980339\n",
      "Epoch 2580 took 1.08 seconds.\n",
      "train loss 0.0007450065750163049\n",
      "val loss 0.0011731518898159266\n",
      "Epoch 2600 took 1.07 seconds.\n",
      "train loss 0.000728083134163171\n",
      "val loss 0.0011832566233351827\n",
      "Epoch 2620 took 1.17 seconds.\n",
      "train loss 0.0006982881895964965\n",
      "val loss 0.001160837709903717\n",
      "Epoch 2640 took 1.11 seconds.\n",
      "train loss 0.0007918997580418363\n",
      "val loss 0.0011660520103760064\n",
      "Epoch 2660 took 1.09 seconds.\n",
      "train loss 0.0007900133932707831\n",
      "val loss 0.0011702849296852946\n",
      "Epoch 2680 took 1.09 seconds.\n",
      "train loss 0.0006950975366635248\n",
      "val loss 0.0011553379008546472\n",
      "Epoch 2700 took 1.09 seconds.\n",
      "train loss 0.0007137526845326647\n",
      "val loss 0.0011650511878542602\n",
      "Epoch 2720 took 1.12 seconds.\n",
      "train loss 0.0007413895073113963\n",
      "val loss 0.0011667878716252744\n",
      "Epoch 2740 took 1.05 seconds.\n",
      "train loss 0.0007009754481259733\n",
      "val loss 0.0011773069854825735\n",
      "Epoch 2760 took 1.08 seconds.\n",
      "train loss 0.0007597704388899729\n",
      "val loss 0.001166387286502868\n",
      "Epoch 2780 took 1.05 seconds.\n",
      "train loss 0.0008186191989807412\n",
      "val loss 0.0011712496052496135\n",
      "Epoch 2800 took 1.06 seconds.\n",
      "train loss 0.0007064763340167701\n",
      "val loss 0.0011616643168963492\n",
      "Epoch 2820 took 1.06 seconds.\n",
      "train loss 0.0008196189010050148\n",
      "val loss 0.0012411093339323997\n",
      "Epoch 2840 took 1.07 seconds.\n",
      "train loss 0.0007376478897640482\n",
      "val loss 0.00116678886115551\n",
      "Epoch 2860 took 1.05 seconds.\n",
      "train loss 0.0007207248854683712\n",
      "val loss 0.0011620265431702137\n",
      "Epoch 2880 took 1.05 seconds.\n",
      "train loss 0.0007004377548582852\n",
      "val loss 0.00117223069537431\n",
      "Epoch 2900 took 1.07 seconds.\n",
      "train loss 0.0007223999127745628\n",
      "val loss 0.0011590112117119133\n",
      "Epoch 2920 took 1.05 seconds.\n",
      "train loss 0.000672105437843129\n",
      "val loss 0.001166679197922349\n",
      "Epoch 2940 took 1.05 seconds.\n",
      "train loss 0.0006763051205780357\n",
      "val loss 0.0011479163658805192\n",
      "Epoch 2960 took 1.07 seconds.\n",
      "train loss 0.0007018169271759689\n",
      "val loss 0.0011488560121506453\n",
      "Epoch 2980 took 1.07 seconds.\n",
      "train loss 0.0007225366280181333\n",
      "val loss 0.0011720691109076142\n",
      "Epoch 3000 took 1.06 seconds.\n",
      "train loss 0.0007267108449013904\n",
      "val loss 0.0011503578280098736\n",
      "Epoch 3020 took 1.08 seconds.\n",
      "train loss 0.0008174505637725815\n",
      "val loss 0.001174009288661182\n",
      "Epoch 3040 took 1.07 seconds.\n",
      "train loss 0.000740520321414806\n",
      "val loss 0.0011567982728593051\n",
      "Epoch 3060 took 1.08 seconds.\n",
      "train loss 0.0007011807902017608\n",
      "val loss 0.0011485093273222446\n",
      "Epoch 3080 took 1.07 seconds.\n",
      "train loss 0.0006999263350735418\n",
      "val loss 0.0011642391327768564\n",
      "Epoch 3100 took 1.09 seconds.\n",
      "train loss 0.0007358237635344267\n",
      "val loss 0.0011979808914475143\n",
      "Epoch 3120 took 1.09 seconds.\n",
      "train loss 0.0007246001478051767\n",
      "val loss 0.0011680786265060306\n",
      "Epoch 3140 took 1.07 seconds.\n",
      "train loss 0.0006890746008139104\n",
      "val loss 0.0011595427640713751\n",
      "Epoch 3160 took 1.07 seconds.\n",
      "train loss 0.0007110837032087147\n",
      "val loss 0.0011473346967250109\n",
      "Epoch 3180 took 1.07 seconds.\n",
      "train loss 0.0006959205056773499\n",
      "val loss 0.0011710455873981118\n",
      "Epoch 3200 took 1.07 seconds.\n",
      "train loss 0.0007381414907285944\n",
      "val loss 0.001174167322460562\n",
      "Epoch 3220 took 1.07 seconds.\n",
      "train loss 0.0006861812871647999\n",
      "val loss 0.001145700633060187\n",
      "Epoch 3240 took 1.07 seconds.\n",
      "train loss 0.0006714630435453728\n",
      "val loss 0.001148920156992972\n",
      "Epoch 3260 took 1.1 seconds.\n",
      "train loss 0.0007353823748417199\n",
      "val loss 0.0011560156126506627\n",
      "Epoch 3280 took 1.09 seconds.\n",
      "train loss 0.0006922287429915741\n",
      "val loss 0.0011494819773361087\n",
      "Epoch 3300 took 1.07 seconds.\n",
      "train loss 0.0007270031637744978\n",
      "val loss 0.001142058812547475\n",
      "Epoch 3320 took 1.05 seconds.\n",
      "train loss 0.0007265730091603473\n",
      "val loss 0.0011518042301759124\n",
      "Epoch 3340 took 1.05 seconds.\n",
      "train loss 0.0006919695297256112\n",
      "val loss 0.0011433165054768324\n",
      "Epoch 3360 took 1.11 seconds.\n",
      "train loss 0.0007335888512898237\n",
      "val loss 0.0011750960838980973\n",
      "Epoch 3380 took 1.05 seconds.\n",
      "train loss 0.0007246317982207984\n",
      "val loss 0.0011526026646606624\n",
      "Epoch 3400 took 1.05 seconds.\n",
      "train loss 0.0007008562533883378\n",
      "val loss 0.0011442980612628162\n",
      "Epoch 3420 took 1.05 seconds.\n",
      "train loss 0.0007013428839854896\n",
      "val loss 0.0011451772297732532\n",
      "Epoch 3440 took 1.05 seconds.\n",
      "train loss 0.0006618580082431436\n",
      "val loss 0.001149485120549798\n",
      "Epoch 3460 took 1.12 seconds.\n",
      "train loss 0.0007378396694548428\n",
      "val loss 0.0011840105871669948\n",
      "Epoch 3480 took 1.1 seconds.\n",
      "train loss 0.0007264251325977966\n",
      "val loss 0.0012000969145447016\n",
      "Epoch 3500 took 1.09 seconds.\n",
      "train loss 0.0007128930592443794\n",
      "val loss 0.00114048074465245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3520 took 1.14 seconds.\n",
      "train loss 0.0006883355526952073\n",
      "val loss 0.0011421875096857548\n",
      "Epoch 3540 took 1.12 seconds.\n",
      "train loss 0.0007314381509786472\n",
      "val loss 0.0011403932585380971\n",
      "Epoch 3560 took 1.1 seconds.\n",
      "train loss 0.0007428561366396025\n",
      "val loss 0.0011479266686365008\n",
      "Epoch 3580 took 1.09 seconds.\n",
      "train loss 0.0006834773521404713\n",
      "val loss 0.0011460913810878992\n",
      "Epoch 3600 took 1.07 seconds.\n",
      "train loss 0.0006878200947539881\n",
      "val loss 0.0011411922750994563\n",
      "Epoch 3620 took 1.07 seconds.\n",
      "train loss 0.0007282634469447657\n",
      "val loss 0.001156762067694217\n",
      "Epoch 3640 took 1.07 seconds.\n",
      "train loss 0.0007264877494890243\n",
      "val loss 0.0011530313058756292\n",
      "Epoch 3660 took 1.08 seconds.\n",
      "train loss 0.0007550118170911446\n",
      "val loss 0.001215898199006915\n",
      "Epoch 3680 took 1.07 seconds.\n",
      "train loss 0.0007383480842690915\n",
      "val loss 0.0011457113432697952\n",
      "Epoch 3700 took 1.05 seconds.\n",
      "train loss 0.0006966376968193799\n",
      "val loss 0.0011392016313038766\n",
      "Epoch 3720 took 1.07 seconds.\n",
      "train loss 0.0007235304219648242\n",
      "val loss 0.0011389357969164848\n",
      "Epoch 3740 took 1.05 seconds.\n",
      "train loss 0.0007039552874630317\n",
      "val loss 0.0011454799096100032\n",
      "Epoch 3760 took 1.05 seconds.\n",
      "train loss 0.0007070908905006945\n",
      "val loss 0.0011366682010702789\n",
      "Epoch 3780 took 1.05 seconds.\n",
      "train loss 0.0006244292890187353\n",
      "val loss 0.0011311936541460454\n",
      "Epoch 3800 took 1.07 seconds.\n",
      "train loss 0.000686723506078124\n",
      "val loss 0.0011351865832693875\n",
      "Epoch 3820 took 1.05 seconds.\n",
      "train loss 0.0007023463695077226\n",
      "val loss 0.0011339300544932485\n",
      "Epoch 3840 took 1.05 seconds.\n",
      "train loss 0.0007189250027295202\n",
      "val loss 0.0011332292342558503\n",
      "Epoch 3860 took 1.06 seconds.\n",
      "train loss 0.0006672673334833235\n",
      "val loss 0.0011361845536157489\n",
      "Epoch 3880 took 1.07 seconds.\n",
      "train loss 0.000668421620503068\n",
      "val loss 0.0011339474585838616\n",
      "Epoch 3900 took 1.07 seconds.\n",
      "train loss 0.0007348438230110332\n",
      "val loss 0.0011316874879412353\n",
      "Epoch 3920 took 1.05 seconds.\n",
      "train loss 0.0006686972774332389\n",
      "val loss 0.0011407265556044877\n",
      "Epoch 3940 took 1.08 seconds.\n",
      "train loss 0.0007116081542335451\n",
      "val loss 0.0011363803059794009\n",
      "Epoch 3960 took 1.08 seconds.\n",
      "train loss 0.0006372253556037322\n",
      "val loss 0.0011324032675474882\n",
      "Epoch 3980 took 1.1 seconds.\n",
      "train loss 0.0006941108877072111\n",
      "val loss 0.001126438262872398\n",
      "Epoch 4000 took 1.05 seconds.\n",
      "train loss 0.000692670582793653\n",
      "val loss 0.0011407595011405647\n",
      "Epoch 4020 took 1.08 seconds.\n",
      "train loss 0.0007261260761879385\n",
      "val loss 0.0011388235143385828\n",
      "Epoch 4040 took 1.08 seconds.\n",
      "train loss 0.0007359408919000998\n",
      "val loss 0.001152391661889851\n",
      "Epoch 4060 took 1.07 seconds.\n",
      "train loss 0.0006888502830406651\n",
      "val loss 0.0011618295684456825\n",
      "Epoch 4080 took 1.05 seconds.\n",
      "train loss 0.0006550049583893269\n",
      "val loss 0.001131989760324359\n",
      "Epoch 4100 took 1.05 seconds.\n",
      "train loss 0.0007090947910910472\n",
      "val loss 0.0011456568608991802\n",
      "Epoch 4120 took 1.07 seconds.\n",
      "train loss 0.0006861513975309208\n",
      "val loss 0.0011343771475367248\n",
      "Epoch 4140 took 1.05 seconds.\n",
      "train loss 0.0007604161219205707\n",
      "val loss 0.0011311969137750566\n",
      "Epoch 4160 took 1.07 seconds.\n",
      "train loss 0.0007178169616963714\n",
      "val loss 0.001134730235207826\n",
      "Epoch 4180 took 1.06 seconds.\n",
      "train loss 0.0007080330542521551\n",
      "val loss 0.0011201173765584826\n",
      "Epoch 4200 took 1.07 seconds.\n",
      "train loss 0.0006898641295265406\n",
      "val loss 0.0011468526208773255\n",
      "Epoch 4220 took 1.09 seconds.\n",
      "train loss 0.0006746421713614836\n",
      "val loss 0.0011366967228241265\n",
      "Epoch 4240 took 1.06 seconds.\n",
      "train loss 0.0007011704728938639\n",
      "val loss 0.0011379019706510007\n",
      "Epoch 4260 took 1.05 seconds.\n",
      "train loss 0.0007071051077218726\n",
      "val loss 0.0011230645468458533\n",
      "Epoch 4280 took 1.05 seconds.\n",
      "train loss 0.0006670193251920864\n",
      "val loss 0.0011234713019803166\n",
      "Epoch 4300 took 1.07 seconds.\n",
      "train loss 0.0006815937522333115\n",
      "val loss 0.0011382807279005647\n",
      "Epoch 4320 took 1.06 seconds.\n",
      "train loss 0.0007099893409758806\n",
      "val loss 0.0011343294172547758\n",
      "Epoch 4340 took 1.06 seconds.\n",
      "train loss 0.000676170238875784\n",
      "val loss 0.0011331089190207422\n",
      "Epoch 4360 took 1.08 seconds.\n",
      "train loss 0.000683865713654086\n",
      "val loss 0.0011284833308309317\n",
      "Epoch 4380 took 1.05 seconds.\n",
      "train loss 0.0006916142156114802\n",
      "val loss 0.0011294333962723613\n",
      "Epoch 4400 took 1.05 seconds.\n",
      "train loss 0.0007066999969538301\n",
      "val loss 0.0011445761192589998\n",
      "Epoch 4420 took 1.07 seconds.\n",
      "train loss 0.0006513583430205472\n",
      "val loss 0.0011279003228992224\n",
      "Epoch 4440 took 1.09 seconds.\n",
      "train loss 0.0006771823245799169\n",
      "val loss 0.0011272075935266912\n",
      "Epoch 4460 took 1.06 seconds.\n",
      "train loss 0.0007048416009638458\n",
      "val loss 0.0011292605777271092\n",
      "Epoch 4480 took 1.07 seconds.\n",
      "train loss 0.0006694272306049243\n",
      "val loss 0.0011452282778918743\n",
      "Epoch 4500 took 1.09 seconds.\n",
      "train loss 0.0007146138959797099\n",
      "val loss 0.0011211734963580966\n",
      "Epoch 4520 took 1.05 seconds.\n",
      "train loss 0.0007076869369484484\n",
      "val loss 0.0011427035205997527\n",
      "Epoch 4540 took 1.05 seconds.\n",
      "train loss 0.0007206761074485257\n",
      "val loss 0.0011736434535123408\n",
      "Epoch 4560 took 1.06 seconds.\n",
      "train loss 0.0007063296070555225\n",
      "val loss 0.0011445789132267237\n",
      "Epoch 4580 took 1.07 seconds.\n",
      "train loss 0.0006728895823471248\n",
      "val loss 0.0011231866083107889\n",
      "Epoch 4600 took 1.05 seconds.\n",
      "train loss 0.00067414045042824\n",
      "val loss 0.0011338635231368244\n",
      "Epoch 4620 took 1.07 seconds.\n",
      "train loss 0.0006850581121398136\n",
      "val loss 0.0011187384952791035\n",
      "Epoch 4640 took 1.09 seconds.\n",
      "train loss 0.0006393451185431331\n",
      "val loss 0.0011175640393048525\n",
      "Epoch 4660 took 1.06 seconds.\n",
      "train loss 0.0006963506166357547\n",
      "val loss 0.0011210383963771164\n",
      "Epoch 4680 took 1.07 seconds.\n",
      "train loss 0.0006289735756581649\n",
      "val loss 0.0011095835361629725\n",
      "Epoch 4700 took 1.05 seconds.\n",
      "train loss 0.0006858133419882506\n",
      "val loss 0.001125316193792969\n",
      "Epoch 4720 took 1.05 seconds.\n",
      "train loss 0.0006759181851521134\n",
      "val loss 0.0011180348228663206\n",
      "Epoch 4740 took 1.08 seconds.\n",
      "train loss 0.0006952079565962777\n",
      "val loss 0.0011365690734237432\n",
      "Epoch 4760 took 1.06 seconds.\n",
      "train loss 0.0006881846202304587\n",
      "val loss 0.0011444572592154145\n",
      "Epoch 4780 took 1.08 seconds.\n",
      "train loss 0.0006513092375826091\n",
      "val loss 0.0011345838429406285\n",
      "Epoch 4800 took 1.05 seconds.\n",
      "train loss 0.0006378243706421927\n",
      "val loss 0.0011248168884776533\n",
      "Epoch 4820 took 1.07 seconds.\n",
      "train loss 0.0007174203637987375\n",
      "val loss 0.001173421973362565\n",
      "Epoch 4840 took 1.08 seconds.\n",
      "train loss 0.0006837373221060261\n",
      "val loss 0.0011139828711748123\n",
      "Epoch 4860 took 1.07 seconds.\n",
      "train loss 0.0006325138529064134\n",
      "val loss 0.001124929403886199\n",
      "Epoch 4880 took 1.09 seconds.\n",
      "train loss 0.0006799670372856781\n",
      "val loss 0.0011185254552401602\n",
      "Epoch 4900 took 1.05 seconds.\n",
      "train loss 0.0006807328172726557\n",
      "val loss 0.0011370007996447384\n",
      "Epoch 4920 took 1.07 seconds.\n",
      "train loss 0.0006895411934237927\n",
      "val loss 0.0011144448653794825\n",
      "Epoch 4940 took 1.05 seconds.\n",
      "train loss 0.0007002696802373976\n",
      "val loss 0.001116075145546347\n",
      "Epoch 4960 took 1.05 seconds.\n",
      "train loss 0.0006773559143766761\n",
      "val loss 0.0011189517099410295\n",
      "Epoch 4980 took 1.07 seconds.\n",
      "train loss 0.0006789972685510293\n",
      "val loss 0.0011191589874215424\n",
      "Epoch 5000 took 1.05 seconds.\n",
      "train loss 0.0006276970671024173\n",
      "val loss 0.0011212185490876436\n",
      "Epoch 5020 took 1.07 seconds.\n",
      "train loss 0.0006648005728493445\n",
      "val loss 0.0011147612240165472\n",
      "Epoch 5040 took 1.07 seconds.\n",
      "train loss 0.0006888934003654867\n",
      "val loss 0.0011240466265007854\n",
      "Epoch 5060 took 1.05 seconds.\n",
      "train loss 0.0007446553790941834\n",
      "val loss 0.001162748783826828\n",
      "Epoch 5080 took 1.05 seconds.\n",
      "train loss 0.0007281621365109459\n",
      "val loss 0.0011234523844905198\n",
      "Epoch 5100 took 1.07 seconds.\n",
      "train loss 0.0006673586613032967\n",
      "val loss 0.0011077297967858613\n",
      "Epoch 5120 took 1.05 seconds.\n",
      "train loss 0.0006350116600515321\n",
      "val loss 0.0011179728899151087\n",
      "Epoch 5140 took 1.08 seconds.\n",
      "train loss 0.0006512717081932351\n",
      "val loss 0.0011252029216848314\n",
      "Epoch 5160 took 1.12 seconds.\n",
      "train loss 0.0006270794983720407\n",
      "val loss 0.0011099042021669447\n",
      "Epoch 5180 took 1.06 seconds.\n",
      "train loss 0.0006923483888385817\n",
      "val loss 0.001108666299842298\n",
      "Epoch 5200 took 1.07 seconds.\n",
      "train loss 0.0007026448001852259\n",
      "val loss 0.0011196759878657758\n",
      "Epoch 5220 took 1.05 seconds.\n",
      "train loss 0.0006700504454784095\n",
      "val loss 0.0011183152091689408\n",
      "Epoch 5240 took 1.05 seconds.\n",
      "train loss 0.000640284750261344\n",
      "val loss 0.001114364538807422\n",
      "Epoch 5260 took 1.07 seconds.\n",
      "train loss 0.0007254569354699925\n",
      "val loss 0.001123276015277952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5280 took 1.05 seconds.\n",
      "train loss 0.0007514924509450793\n",
      "val loss 0.0011473253252916038\n",
      "Epoch 5300 took 1.05 seconds.\n",
      "train loss 0.0007271577924257144\n",
      "val loss 0.0011118752881884575\n",
      "Epoch 5320 took 1.05 seconds.\n",
      "train loss 0.0006603908404940739\n",
      "val loss 0.0011142135481350124\n",
      "Epoch 5340 took 1.07 seconds.\n",
      "train loss 0.0006455008551711217\n",
      "val loss 0.0011095418594777584\n",
      "Epoch 5360 took 1.05 seconds.\n",
      "train loss 0.0006664934189757332\n",
      "val loss 0.001100734167266637\n",
      "Epoch 5380 took 1.05 seconds.\n",
      "train loss 0.0006934589182492346\n",
      "val loss 0.0011204842594452202\n",
      "Epoch 5400 took 1.07 seconds.\n",
      "train loss 0.000682816345943138\n",
      "val loss 0.0011057579540647566\n",
      "Epoch 5420 took 1.08 seconds.\n",
      "train loss 0.0006415887182811275\n",
      "val loss 0.0011010408634319901\n",
      "Epoch 5440 took 1.07 seconds.\n",
      "train loss 0.000673928385367617\n",
      "val loss 0.0011065586586482823\n",
      "Epoch 5460 took 1.05 seconds.\n",
      "train loss 0.0006795533554395661\n",
      "val loss 0.0011183118331246078\n",
      "Epoch 5480 took 1.05 seconds.\n",
      "train loss 0.0006479210715042427\n",
      "val loss 0.001120111148338765\n",
      "Epoch 5500 took 1.08 seconds.\n",
      "train loss 0.000604228291194886\n",
      "val loss 0.0010993201867677271\n",
      "Epoch 5520 took 1.09 seconds.\n",
      "train loss 0.0006471120868809521\n",
      "val loss 0.0011132345534861088\n",
      "Epoch 5540 took 1.07 seconds.\n",
      "train loss 0.0006890296353958547\n",
      "val loss 0.001108801458030939\n",
      "Epoch 5560 took 1.07 seconds.\n",
      "train loss 0.0007001952617429197\n",
      "val loss 0.0011081155971623957\n",
      "Epoch 5580 took 1.05 seconds.\n",
      "train loss 0.0007098321075318381\n",
      "val loss 0.0011540037812665105\n",
      "Epoch 5600 took 1.07 seconds.\n",
      "train loss 0.0006922080647200346\n",
      "val loss 0.0011086944723501801\n",
      "Epoch 5620 took 1.08 seconds.\n",
      "train loss 0.0006530851678689942\n",
      "val loss 0.0011067526647821069\n",
      "Epoch 5640 took 1.07 seconds.\n",
      "train loss 0.0006728285952704027\n",
      "val loss 0.0011090144980698824\n",
      "Epoch 5660 took 1.05 seconds.\n",
      "train loss 0.00064352028130088\n",
      "val loss 0.0011133954394608736\n",
      "Epoch 5680 took 1.06 seconds.\n",
      "train loss 0.000647724446025677\n",
      "val loss 0.0010998145444318652\n",
      "Epoch 5700 took 1.06 seconds.\n",
      "train loss 0.000661828598822467\n",
      "val loss 0.001113568665459752\n",
      "Epoch 5720 took 1.07 seconds.\n",
      "train loss 0.0006316976359812543\n",
      "val loss 0.0011118620168417692\n",
      "Epoch 5740 took 1.05 seconds.\n",
      "train loss 0.0006652251468040049\n",
      "val loss 0.0011097299284301698\n",
      "Epoch 5760 took 1.07 seconds.\n",
      "train loss 0.0006210428400663659\n",
      "val loss 0.0011023239349015057\n",
      "Epoch 5780 took 1.05 seconds.\n",
      "train loss 0.0006631354481214657\n",
      "val loss 0.001101927598938346\n",
      "Epoch 5800 took 1.05 seconds.\n",
      "train loss 0.0006192935106810182\n",
      "val loss 0.001096160733141005\n",
      "Epoch 5820 took 1.06 seconds.\n",
      "train loss 0.0007773068500682712\n",
      "val loss 0.001154972822405398\n",
      "Epoch 5840 took 1.08 seconds.\n",
      "train loss 0.0006808287726016715\n",
      "val loss 0.0011051959008909762\n",
      "Epoch 5860 took 1.07 seconds.\n",
      "train loss 0.0006483503966592252\n",
      "val loss 0.001100428809877485\n",
      "Epoch 5880 took 1.07 seconds.\n",
      "train loss 0.0006852355727460235\n",
      "val loss 0.001109113625716418\n",
      "Epoch 5900 took 1.05 seconds.\n",
      "train loss 0.0006419070559786633\n",
      "val loss 0.0011009983718395233\n",
      "Epoch 5920 took 1.07 seconds.\n",
      "train loss 0.0005994760722387582\n",
      "val loss 0.0010992320021614432\n",
      "Epoch 5940 took 1.07 seconds.\n",
      "train loss 0.0006427674816222861\n",
      "val loss 0.0010973963653668761\n",
      "Epoch 5960 took 1.05 seconds.\n",
      "train loss 0.0006322948465822265\n",
      "val loss 0.0011135600507259369\n",
      "Epoch 5980 took 1.05 seconds.\n",
      "train loss 0.0006453735404647887\n",
      "val loss 0.0011150491191074252\n",
      "Epoch 6000 took 1.07 seconds.\n",
      "train loss 0.0006531356630148366\n",
      "val loss 0.0011070186155848205\n",
      "Epoch 6020 took 1.12 seconds.\n",
      "train loss 0.0006593925354536623\n",
      "val loss 0.001101663161534816\n",
      "Epoch 6040 took 1.08 seconds.\n",
      "train loss 0.0006529874517582357\n",
      "val loss 0.0010982041712850332\n",
      "Epoch 6060 took 1.11 seconds.\n",
      "train loss 0.0006753202906111255\n",
      "val loss 0.0010976506164297462\n",
      "Epoch 6080 took 1.05 seconds.\n",
      "train loss 0.0006871214864077047\n",
      "val loss 0.0011055302456952631\n",
      "Epoch 6100 took 1.05 seconds.\n",
      "train loss 0.0006554613210028037\n",
      "val loss 0.001095870102290064\n",
      "Epoch 6120 took 1.09 seconds.\n",
      "train loss 0.0006240442453417927\n",
      "val loss 0.0010961077641695738\n",
      "Epoch 6140 took 1.06 seconds.\n",
      "train loss 0.0006676443881588057\n",
      "val loss 0.0010911212884820998\n",
      "Epoch 6160 took 1.06 seconds.\n",
      "train loss 0.0006174419540911913\n",
      "val loss 0.001095934712793678\n",
      "Epoch 6180 took 1.07 seconds.\n",
      "train loss 0.0006415815296350047\n",
      "val loss 0.001093963219318539\n",
      "Epoch 6200 took 1.05 seconds.\n",
      "train loss 0.0006733607733622193\n",
      "val loss 0.0011027001892216504\n",
      "Epoch 6220 took 1.09 seconds.\n",
      "train loss 0.0006268876168178394\n",
      "val loss 0.0010902203503064811\n",
      "Epoch 6240 took 1.07 seconds.\n",
      "train loss 0.0006556300795637071\n",
      "val loss 0.0011026638676412404\n",
      "Epoch 6260 took 1.08 seconds.\n",
      "train loss 0.0005979771740385331\n",
      "val loss 0.001112858997657895\n",
      "Epoch 6280 took 1.09 seconds.\n",
      "train loss 0.0006733518675900996\n",
      "val loss 0.0010996423079632223\n",
      "Epoch 6300 took 1.07 seconds.\n",
      "train loss 0.0006278797081904486\n",
      "val loss 0.0011077583185397089\n",
      "Epoch 6320 took 1.08 seconds.\n",
      "train loss 0.0006817755347583443\n",
      "val loss 0.001108965603634715\n",
      "Epoch 6340 took 1.07 seconds.\n",
      "train loss 0.00063647139177192\n",
      "val loss 0.0010951171861961484\n",
      "Epoch 6360 took 1.07 seconds.\n",
      "train loss 0.0006530667742481455\n",
      "val loss 0.001094955310691148\n",
      "Epoch 6380 took 1.08 seconds.\n",
      "train loss 0.0006722989055560902\n",
      "val loss 0.0010896382736973464\n",
      "Epoch 6400 took 1.05 seconds.\n",
      "train loss 0.0006543941126437858\n",
      "val loss 0.0010960254003293812\n",
      "Epoch 6420 took 1.05 seconds.\n",
      "train loss 0.0006205254321685061\n",
      "val loss 0.001094238949008286\n",
      "Epoch 6440 took 1.08 seconds.\n",
      "train loss 0.0006424279708880931\n",
      "val loss 0.0011073530768044293\n",
      "Epoch 6460 took 1.07 seconds.\n",
      "train loss 0.0006172618304844946\n",
      "val loss 0.0010883171926252544\n",
      "Epoch 6480 took 1.1 seconds.\n",
      "train loss 0.0006509108934551477\n",
      "val loss 0.0010947439586743712\n",
      "Epoch 6500 took 1.05 seconds.\n",
      "train loss 0.0006449642532970756\n",
      "val loss 0.0010875379666686058\n",
      "Epoch 6520 took 1.05 seconds.\n",
      "train loss 0.0006194580782903358\n",
      "val loss 0.0010942522203549743\n",
      "Epoch 6540 took 1.07 seconds.\n",
      "train loss 0.0006191607535583898\n",
      "val loss 0.0010942200315184891\n",
      "Epoch 6560 took 1.05 seconds.\n",
      "train loss 0.0006160868797451258\n",
      "val loss 0.0010901010828092694\n",
      "Epoch 6580 took 1.05 seconds.\n",
      "train loss 0.0006556992302648723\n",
      "val loss 0.0010930206044577062\n",
      "Epoch 6600 took 1.08 seconds.\n",
      "train loss 0.0006521797768073156\n",
      "val loss 0.0011251538526266813\n",
      "Epoch 6620 took 1.11 seconds.\n",
      "train loss 0.0005980746282148175\n",
      "val loss 0.0010886575910262764\n",
      "Epoch 6640 took 1.07 seconds.\n",
      "train loss 0.0006395847740350291\n",
      "val loss 0.001094819454010576\n",
      "Epoch 6660 took 1.05 seconds.\n",
      "train loss 0.0006528761878143996\n",
      "val loss 0.0011107110767625272\n",
      "Epoch 6680 took 1.07 seconds.\n",
      "train loss 0.0006560152251040563\n",
      "val loss 0.0010911396238952875\n",
      "Epoch 6700 took 1.05 seconds.\n",
      "train loss 0.000610029193921946\n",
      "val loss 0.0010885907104238868\n",
      "Epoch 6720 took 1.08 seconds.\n",
      "train loss 0.0005762887303717434\n",
      "val loss 0.00108607072615996\n",
      "Epoch 6740 took 1.07 seconds.\n",
      "train loss 0.0005937085225014016\n",
      "val loss 0.0010901722707785666\n",
      "Epoch 6760 took 1.05 seconds.\n",
      "train loss 0.000616739023826085\n",
      "val loss 0.0010835910215973854\n",
      "Epoch 6780 took 1.08 seconds.\n",
      "train loss 0.0006248220161069185\n",
      "val loss 0.001090215751901269\n",
      "Epoch 6800 took 1.07 seconds.\n",
      "train loss 0.0006190184649312869\n",
      "val loss 0.0010917718173004687\n",
      "Epoch 6820 took 1.07 seconds.\n",
      "train loss 0.0006112016853876412\n",
      "val loss 0.0010865405783988535\n",
      "Epoch 6840 took 1.05 seconds.\n",
      "train loss 0.0006121054320828989\n",
      "val loss 0.0010882950737141073\n",
      "Epoch 6860 took 1.05 seconds.\n",
      "train loss 0.0006749369640601799\n",
      "val loss 0.001097826927434653\n",
      "Epoch 6880 took 1.06 seconds.\n",
      "train loss 0.0006631001888308674\n",
      "val loss 0.0011012053582817316\n",
      "Epoch 6900 took 1.05 seconds.\n",
      "train loss 0.000589084331295453\n",
      "val loss 0.0010910356068052351\n",
      "Epoch 6920 took 1.1 seconds.\n",
      "train loss 0.000656489806715399\n",
      "val loss 0.0010901882196776569\n",
      "Epoch 6940 took 1.07 seconds.\n",
      "train loss 0.0005936714733252302\n",
      "val loss 0.0010859076865017414\n",
      "Epoch 6960 took 1.05 seconds.\n",
      "train loss 0.0006124850042397156\n",
      "val loss 0.0010905840899795294\n",
      "Epoch 6980 took 1.05 seconds.\n",
      "train loss 0.0006562084599863738\n",
      "val loss 0.001088551478460431\n",
      "Epoch 7000 took 1.07 seconds.\n",
      "train loss 0.0006132936250651255\n",
      "val loss 0.0010888021788559854\n",
      "Epoch 7020 took 1.05 seconds.\n",
      "train loss 0.0006048290379112586\n",
      "val loss 0.0011089487234130502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7040 took 1.06 seconds.\n",
      "train loss 0.0006121883634477854\n",
      "val loss 0.001085763389710337\n",
      "Epoch 7060 took 1.1 seconds.\n",
      "train loss 0.0006747481238562614\n",
      "val loss 0.0010813424014486372\n",
      "Epoch 7080 took 1.05 seconds.\n",
      "train loss 0.0006232602900126949\n",
      "val loss 0.0011131411301903427\n",
      "Epoch 7100 took 1.05 seconds.\n",
      "train loss 0.0006214078748598695\n",
      "val loss 0.0010896835010498762\n",
      "Epoch 7120 took 1.07 seconds.\n",
      "train loss 0.0006139847537269816\n",
      "val loss 0.0010756682022474706\n",
      "Epoch 7140 took 1.07 seconds.\n",
      "train loss 0.0006132762209745124\n",
      "val loss 0.0010799083393067122\n",
      "Epoch 7160 took 1.05 seconds.\n",
      "train loss 0.00062891400011722\n",
      "val loss 0.0010803912300616503\n",
      "Epoch 7180 took 1.07 seconds.\n",
      "train loss 0.000610044357017614\n",
      "val loss 0.0010963499662466347\n",
      "Epoch 7200 took 1.06 seconds.\n",
      "train loss 0.0007087874255375937\n",
      "val loss 0.0010839287424460053\n",
      "Epoch 7220 took 1.05 seconds.\n",
      "train loss 0.0006421275465982035\n",
      "val loss 0.0010961948428303003\n",
      "Epoch 7240 took 1.05 seconds.\n",
      "train loss 0.0006215228495420888\n",
      "val loss 0.0010855059372261167\n",
      "Epoch 7260 took 1.07 seconds.\n",
      "train loss 0.0005981816357234493\n",
      "val loss 0.0010739496210590005\n",
      "Epoch 7280 took 1.05 seconds.\n",
      "train loss 0.0006018654239596799\n",
      "val loss 0.0010816242429427803\n",
      "Epoch 7300 took 1.05 seconds.\n",
      "train loss 0.0006645214452873915\n",
      "val loss 0.0010908518452197313\n",
      "Epoch 7320 took 1.09 seconds.\n",
      "train loss 0.0006103398918639868\n",
      "val loss 0.0010918286861851811\n",
      "Epoch 7340 took 1.09 seconds.\n",
      "train loss 0.0006166216626297683\n",
      "val loss 0.0010757115669548512\n",
      "Epoch 7360 took 1.05 seconds.\n",
      "train loss 0.000581251188123133\n",
      "val loss 0.0010887068347074091\n",
      "Epoch 7380 took 1.07 seconds.\n",
      "train loss 0.0006601146596949548\n",
      "val loss 0.0010743181337602437\n",
      "Epoch 7400 took 1.05 seconds.\n",
      "train loss 0.0006589759286725894\n",
      "val loss 0.0011143896845169365\n",
      "Epoch 7420 took 1.05 seconds.\n",
      "train loss 0.0006837080291006714\n",
      "val loss 0.001084695744793862\n",
      "Epoch 7440 took 1.07 seconds.\n",
      "train loss 0.0005898795643588528\n",
      "val loss 0.0010764608159661293\n",
      "Epoch 7460 took 1.05 seconds.\n",
      "train loss 0.0006231697334442288\n",
      "val loss 0.0010879532783292234\n",
      "Epoch 7480 took 1.1 seconds.\n",
      "train loss 0.0006130469992058352\n",
      "val loss 0.0010825342033058405\n",
      "Epoch 7500 took 1.07 seconds.\n",
      "train loss 0.0006292499310802668\n",
      "val loss 0.0010857237502932549\n",
      "Epoch 7520 took 1.07 seconds.\n",
      "train loss 0.0006077964353607967\n",
      "val loss 0.0010853474377654493\n",
      "Epoch 7540 took 1.05 seconds.\n",
      "train loss 0.0006403568695532158\n",
      "val loss 0.0010845388751477003\n",
      "Epoch 7560 took 1.05 seconds.\n",
      "train loss 0.0006340474501485005\n",
      "val loss 0.0010776411509141326\n",
      "Epoch 7580 took 1.07 seconds.\n",
      "train loss 0.000621331186266616\n",
      "val loss 0.0010772121022455394\n",
      "Epoch 7600 took 1.14 seconds.\n",
      "train loss 0.0006735443457728252\n",
      "val loss 0.0010783183970488608\n",
      "Epoch 7620 took 1.05 seconds.\n",
      "train loss 0.0006405884923879057\n",
      "val loss 0.0010725774336606264\n",
      "Epoch 7640 took 1.07 seconds.\n",
      "train loss 0.0006194644520292059\n",
      "val loss 0.0010792887187562883\n",
      "Epoch 7660 took 1.11 seconds.\n",
      "train loss 0.0006393358926288784\n",
      "val loss 0.001097360800486058\n",
      "Epoch 7680 took 1.07 seconds.\n",
      "train loss 0.0006065072520868853\n",
      "val loss 0.001072207116521895\n",
      "Epoch 7700 took 1.14 seconds.\n",
      "train loss 0.0006063310283934698\n",
      "val loss 0.001075347128789872\n",
      "Epoch 7720 took 1.06 seconds.\n",
      "train loss 0.0006005409231875092\n",
      "val loss 0.0010798074654303491\n",
      "Epoch 7740 took 1.08 seconds.\n",
      "train loss 0.0006392562936525792\n",
      "val loss 0.0010769542423076928\n",
      "Epoch 7760 took 1.06 seconds.\n",
      "train loss 0.0006019187567289919\n",
      "val loss 0.0010767338098958135\n",
      "Epoch 7780 took 1.07 seconds.\n",
      "train loss 0.0005846606654813513\n",
      "val loss 0.0010703210136853158\n",
      "Epoch 7800 took 1.12 seconds.\n",
      "train loss 0.0006349205214064568\n",
      "val loss 0.0010703618172556162\n",
      "Epoch 7820 took 1.06 seconds.\n",
      "train loss 0.000605746841756627\n",
      "val loss 0.001076430722605437\n",
      "Epoch 7840 took 1.06 seconds.\n",
      "train loss 0.0006245500699151307\n",
      "val loss 0.0010905060917139053\n",
      "Epoch 7860 took 1.05 seconds.\n",
      "train loss 0.0005650547100231051\n",
      "val loss 0.0010769162909127772\n",
      "Epoch 7880 took 1.05 seconds.\n",
      "train loss 0.0006802431744290516\n",
      "val loss 0.001075121050234884\n",
      "Epoch 7900 took 1.07 seconds.\n",
      "train loss 0.0006258404973777942\n",
      "val loss 0.0010729774367064238\n",
      "Epoch 7920 took 1.07 seconds.\n",
      "train loss 0.0006876761035528034\n",
      "val loss 0.001073530816938728\n",
      "Epoch 7940 took 1.05 seconds.\n",
      "train loss 0.0006193990120664239\n",
      "val loss 0.0010778360301628709\n",
      "Epoch 7960 took 1.05 seconds.\n",
      "train loss 0.0006413287192117423\n",
      "val loss 0.0010859098983928561\n",
      "Epoch 7980 took 1.06 seconds.\n",
      "train loss 0.0005679681489709765\n",
      "val loss 0.0010743520106188953\n",
      "Epoch 8000 took 1.06 seconds.\n",
      "train loss 0.0005745174858020619\n",
      "val loss 0.0010828302474692464\n",
      "Epoch 8020 took 1.05 seconds.\n",
      "train loss 0.0006239260401343927\n",
      "val loss 0.001070679514668882\n",
      "Epoch 8040 took 1.09 seconds.\n",
      "train loss 0.0006688827415928245\n",
      "val loss 0.00110762903932482\n",
      "Epoch 8060 took 1.07 seconds.\n",
      "train loss 0.0005852108006365597\n",
      "val loss 0.0010735993273556232\n",
      "Epoch 8080 took 1.06 seconds.\n",
      "train loss 0.0006558571476489305\n",
      "val loss 0.001088320859707892\n",
      "Epoch 8100 took 1.05 seconds.\n",
      "train loss 0.0006032313467585482\n",
      "val loss 0.0010741017758846283\n",
      "Epoch 8120 took 1.07 seconds.\n",
      "train loss 0.0005757620965596288\n",
      "val loss 0.001072065148036927\n",
      "Epoch 8140 took 1.1 seconds.\n",
      "train loss 0.0006237323832465336\n",
      "val loss 0.001067939039785415\n",
      "Epoch 8160 took 1.07 seconds.\n",
      "train loss 0.0006647851987509057\n",
      "val loss 0.0011049318709410727\n",
      "Epoch 8180 took 1.07 seconds.\n",
      "train loss 0.0005998885608278215\n",
      "val loss 0.0010706096654757857\n",
      "Epoch 8200 took 1.07 seconds.\n",
      "train loss 0.0006101653270889074\n",
      "val loss 0.001072568353265524\n",
      "Epoch 8220 took 1.05 seconds.\n",
      "train loss 0.0006455652473960072\n",
      "val loss 0.0010652674245648086\n",
      "Epoch 8240 took 1.05 seconds.\n",
      "train loss 0.0006012744997860864\n",
      "val loss 0.001066050783265382\n",
      "Epoch 8260 took 1.07 seconds.\n",
      "train loss 0.0006040840380592272\n",
      "val loss 0.001069987309165299\n",
      "Epoch 8280 took 1.07 seconds.\n",
      "train loss 0.0006115083233453333\n",
      "val loss 0.0010659487452358007\n",
      "Epoch 8300 took 1.08 seconds.\n",
      "train loss 0.0006306476279860362\n",
      "val loss 0.0010719019919633865\n",
      "Epoch 8320 took 1.09 seconds.\n",
      "train loss 0.0006225724646355957\n",
      "val loss 0.0010644043213687837\n",
      "Epoch 8340 took 1.08 seconds.\n",
      "train loss 0.0006154072034405544\n",
      "val loss 0.001064768002834171\n",
      "Epoch 8360 took 1.08 seconds.\n",
      "train loss 0.0005954589869361371\n",
      "val loss 0.001067912788130343\n",
      "Epoch 8380 took 1.08 seconds.\n",
      "train loss 0.0005807773923152126\n",
      "val loss 0.0010696887620724738\n",
      "Epoch 8400 took 1.09 seconds.\n",
      "train loss 0.0005962567956885323\n",
      "val loss 0.001073489140253514\n",
      "Epoch 8420 took 1.08 seconds.\n",
      "train loss 0.0006258055364014581\n",
      "val loss 0.0010776923154480755\n",
      "Epoch 8440 took 1.07 seconds.\n",
      "train loss 0.0005927468228037469\n",
      "val loss 0.0010797504801303148\n",
      "Epoch 8460 took 1.07 seconds.\n",
      "train loss 0.0005949807818979025\n",
      "val loss 0.0010656158556230366\n",
      "Epoch 8480 took 1.07 seconds.\n",
      "train loss 0.0006461816083174199\n",
      "val loss 0.0010650308686308563\n",
      "Epoch 8500 took 1.06 seconds.\n",
      "train loss 0.0006075558776501566\n",
      "val loss 0.0010650932090356946\n",
      "Epoch 8520 took 1.07 seconds.\n",
      "train loss 0.0005856914795003831\n",
      "val loss 0.00107384700095281\n",
      "Epoch 8540 took 1.07 seconds.\n",
      "train loss 0.0006774211360607296\n",
      "val loss 0.0011388033744879067\n",
      "Epoch 8560 took 1.05 seconds.\n",
      "train loss 0.000592248237808235\n",
      "val loss 0.0010776808485388756\n",
      "Epoch 8580 took 1.07 seconds.\n",
      "train loss 0.0006291740282904357\n",
      "val loss 0.001059824600815773\n",
      "Epoch 8600 took 1.09 seconds.\n",
      "train loss 0.0005957739049335942\n",
      "val loss 0.0010593496263027191\n",
      "Epoch 8620 took 1.08 seconds.\n",
      "train loss 0.0006121289916336536\n",
      "val loss 0.0010605037095956504\n",
      "Epoch 8640 took 1.07 seconds.\n",
      "train loss 0.0005995591054670513\n",
      "val loss 0.0010672941571101546\n",
      "Epoch 8660 took 1.05 seconds.\n",
      "train loss 0.0006267200951697305\n",
      "val loss 0.0010577788925729692\n",
      "Epoch 8680 took 1.07 seconds.\n",
      "train loss 0.0006120371108409017\n",
      "val loss 0.0010770161170512438\n",
      "Epoch 8700 took 1.09 seconds.\n",
      "train loss 0.0006286741845542565\n",
      "val loss 0.0010719982674345374\n",
      "Epoch 8720 took 1.06 seconds.\n",
      "train loss 0.0005878899100935087\n",
      "val loss 0.001065534946974367\n",
      "Epoch 8740 took 1.05 seconds.\n",
      "train loss 0.0005596078772214241\n",
      "val loss 0.0010594092309474945\n",
      "Epoch 8760 took 1.05 seconds.\n",
      "train loss 0.0005747190734837204\n",
      "val loss 0.001059585134498775\n",
      "Epoch 8780 took 1.09 seconds.\n",
      "train loss 0.0005821237500640564\n",
      "val loss 0.0010656336671672761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8800 took 1.05 seconds.\n",
      "train loss 0.0006179859192343429\n",
      "val loss 0.001069660997018218\n",
      "Epoch 8820 took 1.43 seconds.\n",
      "train loss 0.0006305983988568187\n",
      "val loss 0.0010534990578889847\n",
      "Epoch 8840 took 1.44 seconds.\n",
      "train loss 0.0005912801279919222\n",
      "val loss 0.00106275879079476\n",
      "Epoch 8860 took 1.31 seconds.\n",
      "train loss 0.0006342903507174924\n",
      "val loss 0.001066597702447325\n",
      "Epoch 8880 took 1.46 seconds.\n",
      "train loss 0.0005817422570544295\n",
      "val loss 0.0010612980113364756\n",
      "Epoch 8900 took 1.27 seconds.\n",
      "train loss 0.0006278980436036363\n",
      "val loss 0.0010631591430865228\n",
      "Epoch 8920 took 1.46 seconds.\n",
      "train loss 0.0005731580313295126\n",
      "val loss 0.0010645700967870653\n",
      "Epoch 8940 took 1.27 seconds.\n",
      "train loss 0.0005800060171168298\n",
      "val loss 0.0010615091887302697\n",
      "Epoch 8960 took 1.45 seconds.\n",
      "train loss 0.000634929645457305\n",
      "val loss 0.0010579688823781908\n",
      "Epoch 8980 took 1.27 seconds.\n",
      "train loss 0.0005645956262014806\n",
      "val loss 0.001068964775186032\n",
      "Epoch 9000 took 1.49 seconds.\n",
      "train loss 0.0006545851356349885\n",
      "val loss 0.0010626261355355382\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b76a8ed22eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mep_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0me_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0md_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# summary(encoder, input_size=(32, 1, 24, 64, 64), depth=2)\n",
    "# summary(decoder, input_size=(32, 64, 3, 64, 64), depth=2)\n",
    "\n",
    "\n",
    "# for g in e_optim.param_groups:\n",
    "#     g['lr'] = 1e-5\n",
    "\n",
    "# for g in d_optim.param_groups:\n",
    "#     g['lr'] = 1e-5\n",
    "\n",
    "ARGS.epochs = 10000\n",
    "\n",
    "for ep in range(ARGS.epochs):\n",
    "#     print(\"Epoch\", ep)\n",
    "\n",
    "    ep_loss = []\n",
    "\n",
    "    t = time.time() \n",
    "    \n",
    "    for batch in train_dl:\n",
    "                    \n",
    "        batch = transform_batch(batch, ARGS)            \n",
    "        _, _, _, pcmra, coords, pcmra_array, mask_array = get_siren_batch(batch)\n",
    "        \n",
    "        \n",
    "        out = decoder(encoder(pcmra))\n",
    "\n",
    "        loss = criterion(out, pcmra)\n",
    "        loss.backward()\n",
    "\n",
    "        ep_loss.append(loss.item())\n",
    "\n",
    "        e_optim.step(); d_optim.step()  \n",
    "\n",
    "        e_optim.zero_grad(); d_optim.zero_grad()  \n",
    "        \n",
    "        \n",
    "    if ep % 20 == 0: \n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "\n",
    "            print(f\"Epoch {ep} took {round(time.time() - t, 2)} seconds.\")\n",
    "\n",
    "            train_loss = []\n",
    "\n",
    "            b = 0\n",
    "            for batch in train_dl:\n",
    "\n",
    "                batch = transform_batch(batch, ARGS)\n",
    "\n",
    "                _, _, _, pcmra, coords, pcmra_array, mask_array = get_siren_batch(batch)\n",
    "\n",
    "                out = encoder(pcmra)\n",
    "                out = decoder(out)\n",
    "\n",
    "                loss = criterion(out, pcmra)\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                b += 1\n",
    "\n",
    "                if b == 10: \n",
    "                    break\n",
    "\n",
    "            print(\"train loss\", np.array(train_loss).mean())\n",
    "\n",
    "            val_loss = []\n",
    "\n",
    "            for batch in val_dl:\n",
    "\n",
    "                _, _, _, pcmra, coords, pcmra_array, mask_array = get_siren_batch(batch)\n",
    "\n",
    "                out = encoder(pcmra)\n",
    "                out = decoder(out)\n",
    "\n",
    "                loss = criterion(out, pcmra)\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "            print(\"val loss\", np.array(val_loss).mean())\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            \n",
    "            losses = np.append(losses, [[ep, np.array(train_loss).mean(), np.array(train_loss).std(), \n",
    "                                         np.array(val_loss).mean(), np.array(val_loss).std()]], axis=0)\n",
    "            \n",
    "            \n",
    "            np.save(f\"{path}/losses.npy\", losses)\n",
    "    \n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_through_output(dataloader, shape=(64, 64, 24), transform=True):\n",
    "    pcmras = outs = torch.Tensor([])\n",
    "    \n",
    "    titles = []\n",
    "\n",
    "    ep_loss = []\n",
    "\n",
    "    for batch in dataloader: \n",
    "\n",
    "        if transform:\n",
    "            batch = transform_batch(batch, ARGS)\n",
    "\n",
    "        _, _, _, pcmra, coords, pcmra_array, mask_array = get_siren_batch(batch)\n",
    "        \n",
    "        out = encoder(pcmra)\n",
    "        out = decoder(out)\n",
    "\n",
    "        loss = criterion(pcmra, out) \n",
    "        ep_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        pcmras = torch.cat((pcmras, pcmra.contiguous().view(-1, 64, 64).cpu().detach().permute(1, 2, 0)), 2)\n",
    "        outs = torch.cat((outs, out.contiguous().view(-1, 64, 64).cpu().detach().permute(1, 2, 0)), 2)\n",
    "        \n",
    "    \n",
    "    print(np.array(ep_loss).mean())\n",
    "    window = Show_images(\"Comparison\", (pcmras.numpy(), \"pcmras\"), \n",
    "                                 (outs.numpy(), \"output\"))\n",
    "\n",
    "    return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010884649236686528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<py_files.seq_pi_gan_functions.Show_images at 0x7f5efc180908>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scroll_through_output(test_dl, shape=(64, 64, 24), transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), f\"{path}/cnn_train.pt\")\n",
    "torch.save(e_optim.state_dict(), f\"{path}/cnn_optim_train.pt\")\n",
    "\n",
    "torch.save(decoder.state_dict(), f\"{path}/decoder_train.pt\")\n",
    "torch.save(e_optim.state_dict(), f\"{path}/decoder_optim_train.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitc17f53f707db4b89be7c32a22adf91a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
