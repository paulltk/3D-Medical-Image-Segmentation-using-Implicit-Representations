{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run convert_ipynb_to_py_files.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported CNN model.\n",
      "Imported PI-Gan model.\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Loaded all helper functions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math \n",
    "import skimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# from data_classes.py_files.custom_datasets import *\n",
    "from data_classes.py_files.new_dataset import *\n",
    "\n",
    "from model_classes.py_files.cnn_model import *\n",
    "from model_classes.py_files.pigan_model import *\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "DEVICE = set_device()\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Using device for training:', DEVICE)\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():  \n",
    "    \n",
    "    ##### path to wich the model should be saved #####\n",
    "    path = get_folder(ARGS)\n",
    "    \n",
    "    ##### save ARGS #####\n",
    "    with open(f\"{path}/ARGS.txt\", \"w\") as f:\n",
    "        print(vars(ARGS), file=f)\n",
    "        \n",
    "    ##### data preparation #####\n",
    "    train_dl, val_dl, test_dl = initialize_dataloaders(ARGS)  \n",
    "    \n",
    "#       train_dl, val_dl, test_dl = initialize_dataloaders([\"Aorta Volunteers\", \"Aorta BaV\",\n",
    "#                                                \"Aorta Resvcue\", \"Aorta CoA\"], ARGS)  \n",
    "    \n",
    "    \n",
    "    ##### initialize models and optimizers #####\n",
    "    models, optims = load_models_and_optims(ARGS)\n",
    "\n",
    "    ##### load pretrained model #####\n",
    "    if ARGS.pretrained: \n",
    "        print(f\"Loading pretrained model from '{ARGS.pretrained}'.\")\n",
    "        load_models(ARGS.pretrained, ARGS.pretrained_best, \n",
    "                    models, optims)\n",
    "    \n",
    "    ##### loss function #####\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    ##### epoch, train loss mean, train loss std, #####\n",
    "    ##### val loss mean, val loss std #####\n",
    "    losses = np.empty((0, 5))\n",
    "    dice_losses = np.empty((0, 5))\n",
    "\n",
    "    batch_count = 0     \n",
    "    \n",
    "    for ep in range(ARGS.epochs):\n",
    "    \n",
    "        t = time.time() \n",
    "\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "\n",
    "        t_loss_mean, t_loss_std, batch_count = train_epoch(train_dl, models, optims,\n",
    "                                                           criterion, batch_count, ARGS)\n",
    "        \n",
    "        print(f\"Epoch {ep}, train loss: {t_loss_mean}\")\n",
    "        \n",
    "        if ep % ARGS.eval_every == 0: \n",
    "\n",
    "            print(f\"Epoch {ep} took {round(time.time() - t)} seconds.\")\n",
    "            \n",
    "            t_loss_mean, t_loss_std, t_dice_mean, t_dice_std = val_epoch(train_dl, models, criterion)\n",
    "            v_loss_mean, v_loss_std, v_dice_mean, v_dice_std = val_epoch(val_dl, models, criterion)\n",
    "            \n",
    "            losses = np.append(losses, [[ep ,t_loss_mean, t_loss_std, \n",
    "                                         v_loss_mean, v_loss_std]], axis=0)\n",
    "            \n",
    "            dice_losses = np.append(dice_losses, [[ep ,t_dice_mean, t_dice_std, \n",
    "                                         v_dice_mean, v_dice_std]], axis=0)\n",
    "            \n",
    "            save_info(path, losses, dice_losses, models, optims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as .ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ARGS class initialized.\n"
     ]
    }
   ],
   "source": [
    "ARGS = init_ARGS()\n",
    "ARGS.acc_steps = 64\n",
    "ARGS.epochs = 50\n",
    "ARGS.seed = 1\n",
    "ARGS.eval_every = 5\n",
    "\n",
    "ARGS.cnn_setup = 17\n",
    "\n",
    "ARGS.rotated = True\n",
    "\n",
    "train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    PARSER = argparse.ArgumentParser()\n",
    "\n",
    "    # Arguments for training\n",
    "    PARSER.add_argument('--name', type=str, default=\"\", \n",
    "                        help='Name of the folder where the output should be saved.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained', type=str, default=None, \n",
    "                        help='Folder name of pretrained model that should be loaded.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_best', type=str, default=\"train\", \n",
    "                        help='Pretrained model with lowest [train, val] loss.')\n",
    "    \n",
    "    # data\n",
    "    PARSER.add_argument('--dataset', type=str, default=\"small\", \n",
    "                        help='The dataset which we train on.')\n",
    "    \n",
    "    PARSER.add_argument('--rotated', type=bool, default=True, \n",
    "                        help='Rotations of the same image')\n",
    "    \n",
    "    PARSER.add_argument('--norm_min_max', type=list, default=[0, 1], \n",
    "                        help='List with min and max for normalizing input.')\n",
    "    \n",
    "    PARSER.add_argument('--seed', type=int, default=34, \n",
    "                        help='List with min and max for normalizing input.')\n",
    "    \n",
    "    # train variables\n",
    "    PARSER.add_argument('--epochs', type=int, default=50, \n",
    "                        help='Number of epochs.')\n",
    "    \n",
    "    PARSER.add_argument('--acc_steps', type=int, default=64, \n",
    "                        help='Number of subjects that the gradient is \\\n",
    "                        accumulated over before taking an optim step.')\n",
    "    \n",
    "    PARSER.add_argument('--eval_every', type=int, default=5, \n",
    "                        help='Set the # epochs after which evaluation should be done.')\n",
    "    \n",
    "    PARSER.add_argument('--shuffle', type=bool, default=True, \n",
    "                        help='Shuffle the train dataloader?')\n",
    "    \n",
    "    PARSER.add_argument('--n_coords_sample', type=int, default=5000, \n",
    "                        help='Number of coordinates that should be sampled for each subject.')\n",
    "    \n",
    "    \n",
    "    # CNN\n",
    "    PARSER.add_argument('--cnn_setup', type=int, default=17, \n",
    "                        help='Setup of the CNN.')\n",
    "    \n",
    "    # SIREN\n",
    "    PARSER.add_argument('--dim_hidden', type=int, default=256, \n",
    "                        help='Dimension of hidden SIREN layers.')\n",
    "    \n",
    "    PARSER.add_argument('--siren_hidden_layers', type=int, default=3, \n",
    "                        help='Number of hidden SIREN layers.')\n",
    "    \n",
    "    PARSER.add_argument('--first_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of first layer.')\n",
    "    \n",
    "    PARSER.add_argument('--hidden_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of hidden layer.')\n",
    "    \n",
    "    \n",
    "    # optimizers\n",
    "    PARSER.add_argument('--cnn_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of cnn optim.')\n",
    "\n",
    "    PARSER.add_argument('--siren_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "\n",
    "    PARSER.add_argument('--mapping_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of mapping optim.')\n",
    "\n",
    "    PARSER.add_argument('--cnn_wd', type=float, default=0, \n",
    "                        help='Weight decay of cnn optim.')\n",
    "\n",
    "    PARSER.add_argument('--siren_wd', type=float, default=0, \n",
    "                        help='Weight decay of siren optim.')\n",
    "    \n",
    "    PARSER.add_argument('--mapping_wd', type=float, default=0, \n",
    "                        help='Weight decay of mapping optim.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     PARSER.add_argument('--z_dim', type=int, default=256, \n",
    "#                         help='Size of the latent pcmra representation.')\n",
    "    \n",
    "    \n",
    "#     # MAPPING network\n",
    "#     PARSER.add_argument('--with_mapping', type=bool, default=False, \n",
    "#                         help='Use the mapping network to produce a gamma and beta.')  \n",
    "    \n",
    "    \n",
    "    ARGS = PARSER.parse_args()\n",
    "    \n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitc17f53f707db4b89be7c32a22adf91a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
