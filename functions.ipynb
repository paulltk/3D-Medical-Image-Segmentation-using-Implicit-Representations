{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported CNN model.\n",
      "Imported PI-Gan model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import math \n",
    "import skimage\n",
    "import pickle\n",
    "import ast\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mplPath\n",
    "import pylab as pl\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from threading import Timer\n",
    "from PIL import Image\n",
    "\n",
    "# from data_classes.py_files.custom_datasets import *\n",
    "from data_classes.py_files.new_dataset import *\n",
    "\n",
    "from model_classes.py_files.cnn_model import *\n",
    "from model_classes.py_files.pigan_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARGS class for .ipynb files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class init_ARGS(object): \n",
    "    def __init__(self): \n",
    "        self.name = \"\"\n",
    "        self.pretrained = None\n",
    "        self.pretrained_best = \"train\"\n",
    "        self.dataset = \"small\"\n",
    "        self.norm_min_max = [0, 1]\n",
    "        self.seed = 34\n",
    "        self.epochs = 500\n",
    "        self.acc_steps = 10\n",
    "        self.eval_every = 10\n",
    "        self.shuffle = True\n",
    "        self.n_coords_sample = 5000\n",
    "        self.cnn_setup = 1\n",
    "        self.dim_hidden = 256\n",
    "        self.siren_hidden_layers = 3\n",
    "        self.first_omega_0 = 30.\n",
    "        self.hidden_omega_0 = 30.\n",
    "        self.cnn_lr = 1e-4\n",
    "        self.siren_lr = 1e-4\n",
    "        self.mapping_lr = 1e-4\n",
    "        self.cnn_wd = 0\n",
    "        self.siren_wd = 0\n",
    "        self.mapping_wd = 0\n",
    "        self.rotated = True\n",
    "\n",
    "\n",
    "        print(\"WARNING: ARGS class initialized.\")\n",
    "\n",
    "    def set_args(self, dictionary):\n",
    "        for k, v in dictionary.items():\n",
    "            setattr(self, k, v)\n",
    "          \n",
    "        \n",
    "def load_args(run):\n",
    "    run_path = os.path.join(\"saved_runs\", run, \"ARGS.txt\")\n",
    "\n",
    "    with  open(run_path, \"r\") as f:\n",
    "        contents = f.read()\n",
    "        args_dict = ast.literal_eval(contents)\n",
    "    \n",
    "    ARGS = init_ARGS()\n",
    "    \n",
    "    old_args = vars(ARGS)\n",
    "    \n",
    "    for k, v in args_dict.items(): \n",
    "        if k in old_args.keys(): \n",
    "            if old_args[k] != v: \n",
    "                print(f\"Changed param \\t{k}: {v}.\") \n",
    "        else:\n",
    "            print(f\"New param \\t{k}: {v}.\")\n",
    "            \n",
    "    ARGS.set_args(args_dict)\n",
    "    \n",
    "    return ARGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set torch device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "def set_device():\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return DEVICE \n",
    "\n",
    "DEVICE = set_device()\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Using device for training:', DEVICE)\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model saving functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder(ARGS): \n",
    "    now = datetime.now()\n",
    "    dt = now.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    path = f\"saved_runs/pi-gan {dt} {ARGS.name}\"\n",
    "    \n",
    "    Path(f\"{path}\").mkdir(parents=True, exist_ok=True)   \n",
    "\n",
    "    return path\n",
    "    \n",
    "\n",
    "def plot_graph(path, x, ys_and_labels, axes=(\"Epochs\", \"BCELoss\"), fig_name=\"loss_plot\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    for y, label in ys_and_labels: \n",
    "        ax.plot(x[1:], y[1:], label=label)\n",
    "\n",
    "    plt.xlabel(axes[0])\n",
    "    plt.ylabel(axes[1])\n",
    "    legend = ax.legend(loc='upper right')\n",
    "    \n",
    "    plt.savefig(f\"{path}/{fig_name}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "def save_info(path, losses, dice_losses, models, optims): \n",
    "    \n",
    "    np.save(f\"{path}/losses.npy\", losses)\n",
    "    np.save(f\"{path}/dice_losses.npy\", dice_losses)\n",
    "    \n",
    "    eps = losses[:, 0]\n",
    "    train_losses = losses[:, 1]\n",
    "    val_losses = losses[:, 3]\n",
    "    train_d_losses = dice_losses[:, 1]\n",
    "    val_d_losses = dice_losses[:, 3]\n",
    "    \n",
    "    print(f\"Train loss: \\t {round(train_losses[-1], 5)}, \\tdice loss: \\t {round(train_d_losses[-1], 5)}.\")\n",
    "    print(f\"Val loss: \\t {round(val_losses[-1], 5)}, \\tdice loss: \\t {round(val_d_losses[-1], 5)}.\")\n",
    "\n",
    "    if train_losses[-1] == train_losses.min(): \n",
    "        print(f\"New best train loss, saving model.\")\n",
    "\n",
    "        for model in models.keys():\n",
    "            torch.save(models[model].state_dict(), f\"{path}/{model}_train.pt\")\n",
    "            torch.save(optims[model].state_dict(), f\"{path}/{model}_optim_train.pt\")\n",
    "        \n",
    "    \n",
    "    if val_losses[-1] == val_losses.min(): \n",
    "        print(f\"New best val loss, saving model.\")\n",
    "\n",
    "        for model in models.keys():\n",
    "            torch.save(models[model].state_dict(), f\"{path}/{model}_val.pt\")\n",
    "            torch.save(optims[model].state_dict(), f\"{path}/{model}_optim_val.pt\")\n",
    "\n",
    "    plot_graph(path, eps, [(train_losses, \"Train loss\"), (val_losses, \"Eval loss\")], \n",
    "               axes=(\"Epochs\", \"BCELoss\"), fig_name=\"loss_plot\")\n",
    "    \n",
    "    plot_graph(path, eps, [(train_d_losses, \"Train dice loss\"), (val_d_losses, \"Eval dice loss\")], \n",
    "               axes=(\"Epochs\", \"BCELoss\"), fig_name=\"dice_loss_plot\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_dataloaders(projects, ARGS):\n",
    "#     assert(ARGS.dataset in [\"full\", \"small\"])\n",
    "\n",
    "#     data = PrepareData3D(projects, seed=ARGS.seed, image_size=ARGS.dataset, norm_min_max=ARGS.norm_min_max)\n",
    "\n",
    "#     train_ds = SirenDataset(data.train, DEVICE) \n",
    "#     train_dl = DataLoader(train_ds, batch_size=1, num_workers=0, shuffle=ARGS.shuffle)\n",
    "#     print(\"Train subjects:\", train_ds.__len__())\n",
    "\n",
    "#     val_ds = SirenDataset(data.val, DEVICE) \n",
    "#     val_dl = DataLoader(val_ds, batch_size=1, num_workers=0, shuffle=False)\n",
    "#     print(\"Validation subjects:\", val_ds.__len__())\n",
    "    \n",
    "#     test_ds = SirenDataset(data.test, DEVICE) \n",
    "#     test_dl = DataLoader(test_ds, batch_size=1, num_workers=0, shuffle=False)\n",
    "#     print(\"Test subjects:\", test_ds.__len__())\n",
    "    \n",
    "#     return train_dl, val_dl, test_dl\n",
    "    \n",
    "def initialize_dataloaders(ARGS):\n",
    "    \n",
    "    assert(ARGS.dataset in [\"full\", \"small\"])\n",
    "    \n",
    "    root = \"/home/ptenkaate/scratch/Master-Thesis/Dataset/\"\n",
    "    if ARGS.dataset == \"small\":\n",
    "        root += \"scaled_normalized\"\n",
    "    else: \n",
    "        root += \"original_normalized\"\n",
    "    \n",
    "    if ARGS.rotated: \n",
    "        root += \"_rotated\"\n",
    "    \n",
    "    subjects = [file.split(\"__\")[:2] for file in  os.listdir(root)]\n",
    "    subjects = np.array([list(subj) for subj in list(set(map(tuple, subjects)))])\n",
    "\n",
    "    idx = list(range(subjects.shape[0]))\n",
    "    split1, split2 = int(len(idx) * 0.6), int(len(idx) * 0.8)\n",
    "\n",
    "    random.shuffle(idx) # shuffles indices\n",
    "    train_idx, val_idx, test_idx = idx[:split1], idx[split1:split2], idx[split2:] # incides per data subset\n",
    "\n",
    "    train_subjects, val_subjects, test_subjects =  subjects[train_idx], subjects[val_idx], subjects[test_idx]\n",
    "\n",
    "    train_ds = SirenDataset(root, train_subjects, DEVICE)\n",
    "    train_dl = DataLoader(train_ds, batch_size=1, num_workers=0, shuffle=ARGS.shuffle)\n",
    "    print(\"Train subjects:\", train_dl.__len__())\n",
    "    \n",
    "    val_ds =  SirenDataset(root, val_subjects, DEVICE, dataset=\"val\")\n",
    "    val_dl = DataLoader(val_ds, batch_size=1, num_workers=0, shuffle=False)\n",
    "    print(\"Val subjects:\", val_dl.__len__())\n",
    "    \n",
    "    test_ds =  SirenDataset(root, val_subjects, DEVICE, dataset=\"test\")\n",
    "    test_dl = DataLoader(train_ds, batch_size=1, num_workers=0, shuffle=False)\n",
    "    print(\"Test subjects:\", test_ds.__len__())\n",
    "\n",
    "    return train_dl, val_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_and_optims(ARGS):\n",
    "        models = {}\n",
    "        optims = {}\n",
    "        \n",
    "        models[\"cnn\"] = load_cnn(ARGS).cuda()\n",
    "        optims[\"cnn\"] = torch.optim.Adam(lr=ARGS.cnn_lr, params=models[\"cnn\"].parameters(), \n",
    "                                         weight_decay=ARGS.cnn_wd)\n",
    "        \n",
    "#         if ARGS.with_mapping:\n",
    "#             models[\"mapping\"] = MappingNetwork(ARGS).cuda()\n",
    "#             optims[\"mapping\"] = torch.optim.Adam(lr=ARGS.mapping_lr, params=models[\"mapping\"].parameters(), \n",
    "#                                                  weight_decay=ARGS.mapping_wd)\n",
    "        \n",
    "        models[\"siren\"] = Siren(ARGS, in_features=3, out_features=1).cuda()\n",
    "        optims[\"siren\"] = torch.optim.Adam(lr=ARGS.siren_lr, params=models[\"siren\"].parameters(), \n",
    "                                           weight_decay=ARGS.siren_wd)\n",
    "        \n",
    "        for model, struct in models.items(): \n",
    "            print(struct)\n",
    "            \n",
    "        return models, optims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Random coords subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_coords(*arrays, n=1000): \n",
    "    \n",
    "    mx = arrays[0].shape[1]\n",
    "    rand_idx = random.sample(range(mx), n)\n",
    "    \n",
    "    arrays = [array.detach().clone()[:, rand_idx, :] for array in arrays]\n",
    "    \n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dice_loss(pred, target):\n",
    "    \n",
    "    smooth = 0.\n",
    "\n",
    "    pred = torch.round(pred)\n",
    "\n",
    "    pflat = pred.flatten()\n",
    "    tflat = target.flatten()\n",
    "    intersection = (pflat * tflat).sum()\n",
    "\n",
    "    A_sum = torch.sum(pflat * pflat)\n",
    "    B_sum = torch.sum(tflat * tflat)\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and validation epoch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, models, optims, criterion, batch_count, ARGS):\n",
    "    losses = []\n",
    "    \n",
    "    for _, _, _, pcmra, coords, _, mask_array in dataloader:\n",
    "        siren_in, siren_labels = choose_random_coords(coords, mask_array, n=ARGS.n_coords_sample)\n",
    "\n",
    "        cnn_out = models[\"cnn\"](pcmra)\n",
    "        if \"mapping\" in models.keys(): \n",
    "            gamma, beta = models[\"mapping\"](cnn_out)\n",
    "        else: \n",
    "            gamma, beta = cnn_out\n",
    "            \n",
    "        siren_out = models[\"siren\"](siren_in, gamma, beta)\n",
    "         \n",
    "        loss = criterion(siren_out, siren_labels) \n",
    "                \n",
    "        losses.append(loss.item())\n",
    "        loss = loss / ARGS.acc_steps\n",
    "        loss.backward()\n",
    "\n",
    "        batch_count += 1\n",
    "        if batch_count % ARGS.acc_steps == 0: \n",
    "            for _, optim in optims.items():\n",
    "                optim.step()\n",
    "                optim.zero_grad()\n",
    "    \n",
    "    \n",
    "    mean, std = round(np.mean(losses), 6), round(np.std(losses), 6)\n",
    "    \n",
    "    return mean, std, batch_count\n",
    "\n",
    "\n",
    "def val_epoch(dataloader, models, criterion, n_eval=100):\n",
    "    losses = []\n",
    "    d_losses = []\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for idx, subj, proj, pcmra, coords, pcmra_array, mask_array in dataloader:    \n",
    "        siren_out = get_complete_image(models, pcmra, coords)\n",
    "        \n",
    "        loss = criterion(siren_out, mask_array)  \n",
    "        d_loss = calc_dice_loss(siren_out, mask_array) \n",
    "\n",
    "        losses.append(loss.item())\n",
    "        d_losses.append(d_loss.item())\n",
    "        \n",
    "        i += 1\n",
    "        if i == n_eval:\n",
    "            break    \n",
    "    \n",
    "    loss_mean, loss_std = round(np.mean(losses), 6), round(np.std(losses), 6)\n",
    "    d_loss_mean, d_loss_std = round(np.mean(d_losses), 6), round(np.std(d_losses), 6)\n",
    "    \n",
    "    return loss_mean, loss_std, d_loss_mean, d_loss_std\n",
    "\n",
    "\n",
    "def get_complete_image(models, pcmra, coords, val_n = 10000): \n",
    "    for model in models.values(): \n",
    "        model.eval() #evaluation mode    \n",
    "        \n",
    "    image = torch.Tensor([]).cuda() # initialize results tensor\n",
    "    \n",
    "    cnn_out = models[\"cnn\"](pcmra)\n",
    "    \n",
    "    if \"mapping\" in models.keys(): \n",
    "        gamma, beta = models[\"mapping\"](cnn_out)\n",
    "    else: \n",
    "        gamma, beta = cnn_out\n",
    "                \n",
    "    n_slices = math.ceil(coords.shape[1] / val_n) # number of batches\n",
    "    for i in range(n_slices):\n",
    "        coords_in = coords[:, (i*val_n) : ((i+1)*val_n), :]\n",
    "        siren_out = models[\"siren\"](coords_in, gamma, beta)\n",
    "        image = torch.cat((image, siren_out.detach()), 1)\n",
    "    \n",
    "    for model in models.values(): \n",
    "        model.train() #train mode\n",
    "    \n",
    "    return image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(folder, best, models, optims): \n",
    "    path = f\"saved_runs/{folder}/\"\n",
    "\n",
    "    for key in models.keys(): \n",
    "        models[key].load_state_dict(torch.load(f\"{path}/{key}_{best}.pt\"))\n",
    "        optims[key].load_state_dict(torch.load(f\"{path}/{key}_optim_{best}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all helper functions.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded all helper functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CNN setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnn(ARGS): \n",
    "    \n",
    "    if ARGS.cnn_setup == 1: \n",
    "        # output cnn torch.Size([1, 128, 2, 4, 4])\n",
    "        # maxpool,  small kernel, small linear\n",
    "        \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 128, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 128, \"out\": 128, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 128, \"out\": 128, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 128, \"out\": 128, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 128, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "        \n",
    "    elif ARGS.cnn_setup == 2: \n",
    "        # output cnn torch.Size([1, 128, 2, 4, 4])\n",
    "        # maxpool,  small kernel, medium linear\n",
    "        \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 3: \n",
    "        # output cnn torch.Size([1, 128, 2, 4, 4])\n",
    "        # maxpool,  small kernel, large linear\n",
    "        \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 4: \n",
    "        # output cnn torch.Size([1, 128, 2, 4, 4])\n",
    "        # maxpool,  small kernel, large linear but short linear\n",
    "        \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 3, \"str\": 1, \"pad\": 1, \"max\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "        \n",
    "    \n",
    "    elif ARGS.cnn_setup == 5: \n",
    "        # output cnn torch.Size([1, 128, 2, 4, 4])\n",
    "        # stride of 2, small kernel, small linear\n",
    "        \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "\n",
    "    elif ARGS.cnn_setup == 6: \n",
    "        # output cnn torch.Size([1, 128, 2, 4, 4])\n",
    "        # stride of 1, 2, small kernel, small linear\n",
    "\n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 3, \"str\": 1, \"pad\": 1, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 16, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"} \n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 3, \"str\": 1, \"pad\": 1, \"act\": \"relu\"} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 32, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 3, \"str\": 1, \"pad\": 1, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 64, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "\n",
    "    elif ARGS.cnn_setup == 7: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\"}\n",
    "               , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\"} \n",
    "               , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\"} \n",
    "               , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\"} \n",
    "               , {\"type\": \"flatten\"}\n",
    "               , {\"type\": \"lin\", \"in\": 4096, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "               , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "               , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "               , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "               , {\"type\": \"lin\", \"in\": 512, \"out\": 512}\n",
    "               , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "              ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 8: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\"}\n",
    "               , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\"} \n",
    "               , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\"} \n",
    "               , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\"} \n",
    "               , {\"type\": \"flatten\"}\n",
    "               , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "               , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "               , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "               , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "              ])\n",
    "        \n",
    "    elif ARGS.cnn_setup == 9: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\", \n",
    "                    \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (3, 8, 8)} \n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\"} \n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "\n",
    "    elif ARGS.cnn_setup == 10: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\",\n",
    "                   \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "\n",
    "    \n",
    "    elif ARGS.cnn_setup == 11: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\",\n",
    "                   \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 512, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 12:\n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\",\n",
    "                   \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 3, \"str\": 2, \"pad\": 1, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 13: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                   \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 14: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 15: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 16, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                   \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 32, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 64, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"conv\", \"in\": 128, \"out\": 128, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 16: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                   \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                     \"norm\": \"layer\", \"ln\": (2, 4, 4)}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 17: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (24, 64, 64)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 16, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 32, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 64, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 128, \"out\": 128, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\", \n",
    "                    \"norm\": \"layer\", \"ln\": (2, 4, 4)}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 18: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (24, 64, 64)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 16, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 32, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 64, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 128, \"out\": 128, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 19: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 7, \"str\": 1, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (24, 64, 64)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 16, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 7, \"str\": 1, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 32, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 7, \"str\": 1, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 64, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 7, \"str\": 1, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 128, \"out\": 128, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\"}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "        \n",
    "    elif ARGS.cnn_setup == 20: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 7, \"str\": 1, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (24, 64, 64)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 16, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 7, \"str\": 1, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 32, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 7, \"str\": 1, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 64, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 7, \"str\": 1, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 128, \"out\": 128, \"ker\": 7, \"str\": 2, \"pad\": 3, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (2, 4, 4)}\n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "    elif ARGS.cnn_setup == 21: \n",
    "        cnn = CNN([{\"type\": \"conv\", \"in\": 1, \"out\": 16, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (24, 64, 64)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 16, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"max\": 2, \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 16, \"out\": 32, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (12, 32, 32)}\n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 32, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 32, \"out\": 64, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (6, 16, 16)} \n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 64, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 64, \"out\": 128, \"ker\": 5, \"str\": 1, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (3, 8, 8)}\n",
    "                   , {\"type\": \"conv\", \"in\": 128, \"out\": 128, \"ker\": 5, \"str\": 2, \"pad\": 2, \"act\": \"relu\",\n",
    "                    \"norm\": \"layer\", \"ln\": (2, 4, 4)}   \n",
    "                   , {\"type\": \"flatten\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 4096, \"out\": 2048, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 2048, \"out\": 1024, \"act\": \"leakyrelu\"}\n",
    "                   , {\"type\": \"lin\", \"in\": 1024, \"out\": 512}\n",
    "                   , {\"type\": \"split\", \"n_tensors\": 2, \"tensor_size\": 256}\n",
    "                  ])\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    return cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Show_images(object):\n",
    "    \"\"\"\n",
    "    Scroll through slices. Takes an unspecified number of subfigures per figure.\n",
    "    suptitles: either a str or a list. Represents the \n",
    "    main title of a figure. \n",
    "    images_titles: a list with tuples, each tuple an np.array and a \n",
    "    title for the array subfigure. \n",
    "    \"\"\"\n",
    "    def __init__(self, suptitles, *images_titles):\n",
    "        # if string if given, make list with that title for \n",
    "        # each slice.\n",
    "        if type(suptitles) == str: \n",
    "            self.suptitles = []\n",
    "            for i in range(images_titles[0][0].shape[2]): \n",
    "                self.suptitles.append(suptitles)\n",
    "        else: \n",
    "            self.suptitles = suptitles\n",
    "                    \n",
    "        self.fig, self.ax = plt.subplots(1,len(images_titles))\n",
    "\n",
    "        # split tuples with (image, title) into lists\n",
    "        self.images = [x[0] for x in images_titles]\n",
    "        self.titles = [x[1] for x in images_titles]\n",
    "\n",
    "        # get the number of slices that are to be shown\n",
    "        rows, cols, self.slices = self.images[0].shape        \n",
    "        self.ind = 0\n",
    "\n",
    "        self.fig.suptitle(self.suptitles[self.ind]) # set title \n",
    "\n",
    "        self.plots = []\n",
    "        \n",
    "        # start at slice 10 if more than 20 slices, \n",
    "        # otherwise start at middle slice.\n",
    "        if self.images[0].shape[2] > 20: \n",
    "            self.ind = 10\n",
    "        else:\n",
    "            self.ind = self.images[0].shape[2] // 2\n",
    "        \n",
    "        # make sure ax is an np array\n",
    "        if type(self.ax) == np.ndarray:\n",
    "            pass\n",
    "        else: \n",
    "            self.ax = np.array([self.ax])\n",
    "        \n",
    "        # create title for each subfigure in slice\n",
    "        for (sub_ax, image, title) in zip(self.ax, self.images, self.titles): \n",
    "            sub_ax.set_title(title)\n",
    "            plot = sub_ax.imshow(image[:, :, self.ind], vmin=0, vmax=1)\n",
    "            self.plots.append(plot)\n",
    "\n",
    "            \n",
    "        # link figure to mouse scroll movement\n",
    "        self.plot_show = self.fig.canvas.mpl_connect('scroll_event', self.onscroll)\n",
    "        \n",
    "\n",
    "    def onscroll(self, event):\n",
    "        \"\"\"\n",
    "        Shows next or previous slice with mouse scroll.\n",
    "        \"\"\"\n",
    "        if event.button == 'up':\n",
    "            self.ind = (self.ind - 1) % self.slices\n",
    "        else:\n",
    "            self.ind = (self.ind + 1) % self.slices\n",
    "        \n",
    "        self.update()\n",
    "        \n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Updates the figure.\n",
    "        \"\"\"\n",
    "        self.fig.suptitle(self.suptitles[self.ind])\n",
    "        \n",
    "        for plot, image in zip(self.plots, self.images):\n",
    "            plot.set_data(image[:, :, self.ind])\n",
    "        \n",
    "        self.ax[0].set_ylabel('Slice Number: %s' % self.ind)\n",
    "        self.plots[0].axes.figure.canvas.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitc17f53f707db4b89be7c32a22adf91a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
