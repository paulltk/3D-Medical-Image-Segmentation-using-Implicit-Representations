{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1623420897440,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "nSVvRFEyJ73e",
    "outputId": "bee9b323-2387-48b4-8a96-2c6d8bb359a4"
   },
   "outputs": [],
   "source": [
    "%run /home/ptenkaate/scratch/Master-Thesis/convert_ipynb_to_py_files.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yeZRQtBNXlg"
   },
   "source": [
    "#### Colab run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16358,
     "status": "ok",
     "timestamp": 1623420177063,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "RkezriwYKMI1",
    "outputId": "9153c5f2-a41f-4135-ea14-f4979c59c1ec"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 733,
     "status": "ok",
     "timestamp": 1623420180449,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "MxfXBh-4Mq7g",
    "outputId": "49acbf6a-5989-4ebc-ceef-c0f581f48bb9"
   },
   "outputs": [],
   "source": [
    "# cd drive/MyDrive/master_thesis/pi-gan_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10248,
     "status": "ok",
     "timestamp": 1623420355945,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "diXDczG5Y-79",
    "outputId": "8535c61a-9f46-497f-bfaf-87921bbfade9"
   },
   "outputs": [],
   "source": [
    "# !pip install kornia\n",
    "# !pip install pydicom\n",
    "# !pip install torchinfo\n",
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 946,
     "status": "ok",
     "timestamp": 1623420359333,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "BdmZCajHJ73l",
    "outputId": "ef6e074b-ff94-4d19-9fa5-b2c85a8a2aab",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported CNN and Mapping functions.\n",
      "Imported PI-Gan model.\n",
      "Loaded all helper functions.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math \n",
    "import skimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from py_files.new_dataset import *\n",
    "\n",
    "from py_files.cnn_model import *\n",
    "from py_files.pigan_model import *\n",
    "\n",
    "from py_files.seq_pi_gan_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXGVJS5sJ73n"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1623420424650,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "FW0Qoi6lJ73n"
   },
   "outputs": [],
   "source": [
    "def train():  \n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    ##### path to wich the model should be saved #####\n",
    "    path = get_folder(ARGS)\n",
    "    \n",
    "    print(path)\n",
    "    ##### save ARGS #####\n",
    "    with open(f\"{path}/ARGS.txt\", \"w\") as f:\n",
    "        print(vars(ARGS), file=f)\n",
    "        \n",
    "    ##### data preparation #####\n",
    "    train_dl, val_dl, test_dl = initialize_dataloaders(ARGS)\n",
    "    print(\"train batch:\", next(iter(train_dl))[1][:5])\n",
    "    print(\"eval batch:\", next(iter(val_dl))[1][:5])\n",
    "    print(\"test batch:\", next(iter(test_dl))[1][:5])\n",
    "            \n",
    "    ##### initialize models and optimizers #####\n",
    "    models, optims, schedulers = load_models_and_optims(ARGS)\n",
    "    \n",
    "    \n",
    "    ##### load pretrained model #####\n",
    "    if ARGS.pretrained: \n",
    "        print(f\"Loading pretrained model from '{ARGS.pretrained}'.\")\n",
    "        load_pretrained_models(ARGS.pretrained, ARGS.pretrained_best_dataset, ARGS.pretrained_best_loss,\n",
    "                    models, optims, pretrained_models = ARGS.pretrained_models)\n",
    "    \n",
    "        if ARGS.pretrained_lr_reset:\n",
    "            orig_lr = {\"cnn\": ARGS.pretrained_lr_reset, \"mapping\": ARGS.pretrained_lr_reset, \n",
    "                       \"siren\": ARGS.pretrained_lr_reset, \"pcmra_mapping\": ARGS.pretrained_lr_reset, \n",
    "                       \"pcmra_siren\": ARGS.pretrained_lr_reset}\n",
    "            for name, optim in optims.items():\n",
    "                for param_group in optim.param_groups: \n",
    "                    if param_group[\"lr\"] != orig_lr[name]: \n",
    "                        param_group[\"lr\"] = ARGS.pretrained_lr_reset\n",
    "                print(f\"{name} lr: {optim.param_groups[0]['lr']}\")\n",
    "\n",
    "    ##### loss function #####\n",
    "    criterions = [nn.BCELoss(), nn.MSELoss()]\n",
    "        \n",
    "    ##### epoch, train loss mean, train loss std, val loss mean, val loss std #####\n",
    "    mask_losses, pcmra_losses, dice_losses = np.empty((0, 5)), np.empty((0, 5)), np.empty((0, 5))\n",
    "    \n",
    "    for ep in range(ARGS.pcmra_epochs):\n",
    "    \n",
    "        t = time.time() \n",
    "\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "\n",
    "        loss, _ = train_model(train_dl, models, optims, schedulers, criterions[1], ARGS, output=\"pcmra\")\n",
    "        \n",
    "        \n",
    "        if ep % ARGS.eval_every == 0: \n",
    "\n",
    "            print(f\"Epoch {ep} took {round(time.time() - t, 2)} seconds.\")\n",
    "            \n",
    "            t_pcmra_mean, t_pcmra_std, _, _ = \\\n",
    "                val_model(train_dl, models, criterions[1], ARGS, output=\"pcmra\")\n",
    "            \n",
    "            v_pcmra_mean, v_pcmra_std, _, _ = \\\n",
    "                val_model(val_dl, models, criterions[1], ARGS, output=\"pcmra\")\n",
    "\n",
    "            pcmra_losses = np.append(pcmra_losses, [[ep ,t_pcmra_mean, t_pcmra_std, \n",
    "                                         v_pcmra_mean, v_pcmra_std]], axis=0)\n",
    "            \n",
    "            save_loss(path, pcmra_losses, models, optims, name=\"pcmra_loss\", \n",
    "                      save_models=True)\n",
    "        \n",
    "    \n",
    "    for ep in range(ARGS.mask_epochs):\n",
    "    \n",
    "        t = time.time() \n",
    "\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "\n",
    "        loss, _ = train_model(train_dl, models, optims, schedulers, criterions[0], ARGS, output=\"mask\")\n",
    "        \n",
    "        \n",
    "        if ep % ARGS.eval_every == 0: \n",
    "\n",
    "            print(f\"Epoch {ep} took {round(time.time() - t, 2)} seconds.\")\n",
    "            \n",
    "            t_mask_mean, t_mask_std, t_dice_mean, t_dice_std = \\\n",
    "                val_model(train_dl, models, criterions[0], ARGS, output=\"mask\")\n",
    "            \n",
    "            v_mask_mean, v_mask_std, v_dice_mean, v_dice_std = \\\n",
    "                val_model(val_dl, models, criterions[0], ARGS, output=\"mask\")\n",
    "\n",
    "            mask_losses = np.append(mask_losses, [[ep ,t_mask_mean, t_mask_std, \n",
    "                                         v_mask_mean, v_mask_std]], axis=0)\n",
    "            \n",
    "            dice_losses = np.append(dice_losses, [[ep ,t_dice_mean, t_dice_std, \n",
    "                                         v_dice_mean, v_dice_std]], axis=0)\n",
    "            \n",
    "            save_loss(path, mask_losses, models, optims, name=\"mask_loss\", \n",
    "                      save_models=True)\n",
    "            \n",
    "            save_loss(path, dice_losses, models, optims, name=\"dice_loss\", \n",
    "                      save_models=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCWHjP8GJ73q"
   },
   "source": [
    "## Run as .ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "error",
     "timestamp": 1623420425878,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "SSjdoRDcJ73r",
    "outputId": "d362fa57-32ff-45ec-cb05-0483989635fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for cnn_setup, mapping_setup in [(-1, -1)]:\n",
    "\n",
    "#     ARGS = init_ARGS()\n",
    "    \n",
    "#     ARGS.cnn_setup = cnn_setup\n",
    "#     ARGS.mapping_setup = mapping_setup\n",
    "    \n",
    "#     ARGS.batch_size = 1\n",
    "    \n",
    "#     ARGS.pcmra_epochs = 0\n",
    "    \n",
    "#     ARGS.patience = 100 \n",
    "    \n",
    "#     ARGS.first_omega_0 = 30\n",
    "    \n",
    "#     print(vars(ARGS))\n",
    "\n",
    "#     train()  \n",
    "\n",
    "#     torch.cuda.empty_cache()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1623420693563,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "QgFKP1o1J73s",
    "outputId": "9eac712d-2017-4c37-f959-5087ab482a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ARGS class initialized.\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 86\n",
      "Val subjects: 29\n",
      "Test subjects: 29\n"
     ]
    }
   ],
   "source": [
    "ARGS = init_ARGS()\n",
    "\n",
    "ARGS.flip = False \n",
    "ARGS.crop = False \n",
    "ARGS.rotate = False \n",
    "\n",
    "train_dl, val_dl, test_dl = initialize_dataloaders(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1623420693563,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "QgFKP1o1J73s",
    "outputId": "9eac712d-2017-4c37-f959-5087ab482a41",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize the blurring layer\n",
    "sigma = 1.0\n",
    "size = math.ceil(3*sigma)\n",
    "blur_layer = GaussianSmoothing(1, [size,size,size], sigma, dim=3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1623420693563,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "QgFKP1o1J73s",
    "outputId": "9eac712d-2017-4c37-f959-5087ab482a41"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "\n",
    "batch = transform_batch(batch, ARGS)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1623420693563,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "QgFKP1o1J73s",
    "outputId": "9eac712d-2017-4c37-f959-5087ab482a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03341102600097656\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "\n",
    "def get_surface_and_norm(batch): \n",
    "    masks_blurred = blur_layer(batch[-2])     \n",
    "\n",
    "    grad, grad_magn = gradient3d(masks_blurred, normalize=True, s=2)\n",
    "\n",
    "    surface = (grad_magn >= 0.5*torch.max(grad_magn)).type(torch.float32)\n",
    "    norm = grad * surface\n",
    "    \n",
    "    return surface, norm\n",
    "\n",
    "def reshape_arrays(*arrays): \n",
    "    return [array.view(array.shape[0], array.shape[1], -1).permute(0, 2, 1) for array in arrays]\n",
    "    \n",
    "\n",
    "def get_siren_batch(batch, n=5000): \n",
    "    \n",
    "    idx, subj, proj, pcmras, masks, loss_covers = batch\n",
    "    \n",
    "    subjects = []\n",
    "    \n",
    "    # initialize a coords matrix\n",
    "    coords = get_coords(*pcmras.shape[2:]).to(pcmras.device)\n",
    "    \n",
    "    # reshape all matrixes \n",
    "    pcmra_array, mask_array, loss_cover_array = reshape_arrays(pcmras, masks, loss_covers)\n",
    "    \n",
    "    # select n coords and their corresponding values\n",
    "    for pcmra, mask, loss_cover in zip(pcmra_array, mask_array, loss_cover_array):\n",
    "        \n",
    "        # select n random coords that have a non zero loss_cover\n",
    "        idx = (loss_cover != 0).nonzero()[:, 0].cpu().numpy()\n",
    "        idx = np.random.choice(idx, n)\n",
    "        \n",
    "        subject = [coords[idx, :].unsqueeze(0), pcmra[idx, :].unsqueeze(0), mask[idx, :].unsqueeze(0)]\n",
    "    \n",
    "        subjects.append(subject)\n",
    "\n",
    "get_siren_batch(batch, n=5000)\n",
    "\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1623420693563,
     "user": {
      "displayName": "Paul ten Kaate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihEcWnD7dnWVuLjWRFp5A0AGfS2b-MdUZ0Bmclse4=s64",
      "userId": "16522113836666229259"
     },
     "user_tz": -120
    },
    "id": "QgFKP1o1J73s",
    "outputId": "9eac712d-2017-4c37-f959-5087ab482a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012083053588867188\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "    \n",
    "def get_siren_batch_sdf(batch, n=5000, sdf_split=0.5): \n",
    "    \n",
    "    idx, subj, proj, pcmras, masks, loss_covers = batch\n",
    "    subjects = []\n",
    "\n",
    "    surface_n, random_n = int(n*sdf_split), n - int(n*sdf_split)\n",
    "\n",
    "    # get the surface and norm of the mask\n",
    "    surfaces, norms = get_surface_and_norm(batch)\n",
    "\n",
    "\n",
    "\n",
    "    # initialize a coords matrix\n",
    "    coords = get_coords(*pcmras.shape[2:]).to(pcmras.device)\n",
    "\n",
    "    # reshape all matrixes \n",
    "    surface_array, norm_array = reshape_arrays(surfaces, norms)\n",
    "    pcmra_array, mask_array, loss_cover_array = reshape_arrays(pcmras, masks, loss_covers)\n",
    "    coords_array = coords.unsqueeze(0).repeat(pcmras.shape[0], 1, 1)\n",
    "\n",
    "    # select n coords and their corresponding values\n",
    "    for pcmra, mask, loss_cover, surface, norm in \\\n",
    "        zip(pcmra_array, mask_array, loss_cover_array, surface_array, norm_array):\n",
    "\n",
    "\n",
    "        # select n * sfd_split points that lie on the surface\n",
    "        surface_idx = (surface != 0).nonzero()[:, 0].flatten().cpu().numpy()\n",
    "        surface_idx = np.random.choice(surface_idx, surface_n)\n",
    "\n",
    "        # select n random coords that have a non zero loss_cover\n",
    "        random_idx = (loss_cover != 0).nonzero()[:, 0].cpu().numpy()\n",
    "        random_idx = np.random.choice(random_idx, random_n)\n",
    "\n",
    "        idx = np.concatenate((surface_idx, random_idx))\n",
    "        print(idx)\n",
    "\n",
    "        subject = [coords[idx, :].unsqueeze(0), pcmra[idx, :].unsqueeze(0), mask[idx, :].unsqueeze(0),\n",
    "                 surface[idx, :].unsqueeze(0), norm[idx, :].unsqueeze(0)]\n",
    "\n",
    "\n",
    "        subjects.append(subject)\n",
    "\n",
    "    coords_array = torch.cat([subj[0] for subj in subjects], 0)\n",
    "    pcmra_array = torch.cat([subj[1] for subj in subjects], 0)\n",
    "    mask_array = torch.cat([subj[2] for subj in subjects], 0)\n",
    "    surface_array = torch.cat([subj[3] for subj in subjects], 0)\n",
    "    norm_array = torch.cat([subj[4] for subj in subjects], 0)\n",
    "    \n",
    "\n",
    "\n",
    "print(time.time() - t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(masks.shape)\n",
    "\n",
    "# t = time.time()\n",
    "# with torch.no_grad():\n",
    "#     masks_blurred = blur_layer(masks)     \n",
    "\n",
    "# grad, grad_magn = gradient3d(masks_blurred, normalize=True, s=2)\n",
    "\n",
    "# surface_indicator = (grad_magn >= 0.5*torch.max(grad_magn)).type(torch.float32)\n",
    "# si = surface_indicator\n",
    "\n",
    "# surface_norm = grad * surface_indicator\n",
    "\n",
    "# print(time.time() - t)\n",
    "\n",
    "# surface_array = surface_indicator.view(24, -1, 1)\n",
    "# sa = surface_array\n",
    "# print(sa.shape)\n",
    "\n",
    "# print(time.time() - t)\n",
    "\n",
    "# idxs = []\n",
    "# for s in sa:\n",
    "#     s = s.squeeze()\n",
    "#     surface_idx = (s != 0).nonzero().flatten().cpu().numpy()\n",
    "#     idxs.append(np.random.choice(surface_idx, 2500))\n",
    "    \n",
    "# #     print(surface_idx.shape)\n",
    "\n",
    "# print(time.time() - t)\n",
    "\n",
    "# print(np.array(idxs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing on a\n",
    "    1d, 2d or 3d tensor. Filtering is performed seperately for each channel\n",
    "    in the input using a depthwise convolution.\n",
    "    Arguments:\n",
    "        channels (int, sequence): Number of channels of the input tensors. Output will\n",
    "            have this number of channels as well.\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "        dim (int, optional): The number of dimensions of the data.\n",
    "            Default value is 2 (spatial).\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size, sigma, dim=2):\n",
    "        super(GaussianSmoothing, self).__init__()\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size] * dim\n",
    "            self.kernel_radius = kernel_size // 2\n",
    "        else:\n",
    "            self.kernel_radius = kernel_size[0] // 2\n",
    "\n",
    "        if isinstance(sigma, numbers.Number):\n",
    "            sigma = [sigma] * dim\n",
    "\n",
    "\n",
    "        # The gaussian kernel is the product of the\n",
    "        # gaussian function of each dimension.\n",
    "        kernel = 1\n",
    "        meshgrids = torch.meshgrid(\n",
    "            [\n",
    "                torch.arange(size, dtype=torch.float32)\n",
    "                for size in kernel_size\n",
    "            ]\n",
    "        )\n",
    "        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
    "            mean = (size - 1) / 2\n",
    "            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
    "                      torch.exp(-((mgrid - mean) / (2 * std)) ** 2)\n",
    "\n",
    "        # Make sure sum of values in gaussian kernel equals 1.\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "\n",
    "        # Reshape to depthwise convolutional weight\n",
    "        kernel = kernel.view(1, 1, *kernel.size())\n",
    "        kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))\n",
    "\n",
    "        self.register_buffer('weight', kernel)\n",
    "        self.groups = channels\n",
    "\n",
    "        if dim == 1:\n",
    "            self.conv = F.conv1d\n",
    "        elif dim == 2:\n",
    "            self.conv = F.conv2d\n",
    "        elif dim == 3:\n",
    "            self.conv = F.conv3d\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Apply gaussian filter to input.\n",
    "        Arguments:\n",
    "            input (torch.Tensor): Input to apply gaussian filter on.\n",
    "        Returns:\n",
    "            filtered (torch.Tensor): Filtered output.\n",
    "        \"\"\"\n",
    "        return self.conv(input, weight=self.weight, groups=self.groups, padding=self.kernel_radius)\n",
    "    \n",
    "    \n",
    "def gradient3d(data, normalize=False, s=2):\n",
    "    data_padded = torch.nn.functional.pad(data, (1,1,1,1,1,1,0,0,0,0))\n",
    "\n",
    "    grad_x = (data_padded[:,:, s:,   1:-1, 1:-1] - data_padded[:,:, 0:-s, 1:-1, 1:-1]) / s\n",
    "    grad_y = (data_padded[:,:, 1:-1, s:,   1:-1] - data_padded[:,:, 1:-1, 0:-s, 1:-1]) / s\n",
    "    grad_z = (data_padded[:,:, 1:-1, 1:-1, s:  ] - data_padded[:,:, 1:-1, 1:-1 ,0:-s]) / s\n",
    "    \n",
    "    grad = torch.cat([grad_x,grad_y,grad_z], dim=1)\n",
    "    grad_magn = torch.sqrt(grad_x**2 + grad_y**2 + grad_z**2)\n",
    "    \n",
    "    if normalize:\n",
    "        eps=1e-8\n",
    "        grad = grad/(grad_magn + eps)\n",
    "    \n",
    "    return grad, grad_magn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHqsyQs5J73t"
   },
   "source": [
    "## Run as .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4YS9Jm4J73u"
   },
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    PARSER = argparse.ArgumentParser()\n",
    "\n",
    "    \n",
    "    # Arguments for training\n",
    "    PARSER.add_argument('--device', type=str, default=\"GPU\", \n",
    "                        help='Device that should be used.')\n",
    "\n",
    "    PARSER.add_argument('--print_models', type=str2bool, nargs=\"?\", const=True, default=False, \n",
    "                        help='Print the models after initialization or not.')\n",
    "\n",
    "    PARSER.add_argument('--name', type=str, default=\"\", \n",
    "                        help='Name of the folder where the output should be saved.')\n",
    "    \n",
    "    \n",
    "\n",
    "    # pretrained params \n",
    "    \n",
    "    PARSER.add_argument('--pretrained', type=str, default=None, \n",
    "                        help='Folder name of pretrained model that should be loaded.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_best_dataset', type=str, default=\"train\", \n",
    "                        help='Pretrained model with lowest [train, val] loss.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_best_loss', type=str, default=\"mask\", \n",
    "                        help='Pretrained model with lowest [train, val] loss.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_models', type=str, default=None, \n",
    "                        help='Choose which pretrained models to load. None = all models')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_lr_reset', type=str, default=None, \n",
    "                        help='Reset the lr to a value.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # data\n",
    "    PARSER.add_argument('--dataset', type=str, default=\"new\", \n",
    "                        help='The dataset which we train on.')\n",
    "    \n",
    "    PARSER.add_argument('--seed', type=int, default=34, \n",
    "                        help='Seed for initializig dataloader')\n",
    "    \n",
    "    PARSER.add_argument('--rotate', type=str2bool, nargs=\"?\", const=True, default=True, \n",
    "                        help='Rotations of the same image')\n",
    "    \n",
    "    PARSER.add_argument('--translate', type=str2bool, nargs=\"?\", const=True, default=True, \n",
    "                        help='Translations of the same image')\n",
    "    \n",
    "    PARSER.add_argument('--translate_max_pixels', type=int, default=20, \n",
    "                        help='Translation max in height and width.')\n",
    "    \n",
    "    PARSER.add_argument('--flip', type=str2bool, nargs=\"?\", const=True, default=True, \n",
    "                        help='Flips the train image')\n",
    "    \n",
    "    PARSER.add_argument('--crop', type=str2bool, nargs=\"?\", const=True, default=True, \n",
    "                        help='Crops the train image')\n",
    "\n",
    "    PARSER.add_argument('--stretch', type=str2bool, nargs=\"?\", const=True, default=True, \n",
    "                        help='Stretches the train image')\n",
    "\n",
    "    PARSER.add_argument('--stretch_factor', type=float, default=1.2, \n",
    "                        help='Stretch maximum of the train image')\n",
    "\n",
    "    PARSER.add_argument('--norm_min_max', type=list, default=[0, 1], \n",
    "                        help='List with min and max for normalizing input.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train variables\n",
    "    PARSER.add_argument('--pcmra_epochs', type=int, default=5000, \n",
    "                        help='Number of epochs for pcmra training.')\n",
    "\n",
    "    PARSER.add_argument('--mask_epochs', type=int, default=5000, \n",
    "                        help='Number of epochs for mask training.')\n",
    "    \n",
    "    PARSER.add_argument('--batch_size', type=int, default=24, \n",
    "                        help='Number of epochs.')\n",
    "        \n",
    "    PARSER.add_argument('--eval_every', type=int, default=50, \n",
    "                        help='Set the # epochs after which evaluation should be done.')\n",
    "    \n",
    "    PARSER.add_argument('--shuffle', type=str2bool, nargs=\"?\", const=True, default=True, \n",
    "                        help='Shuffle the train dataloader?')\n",
    "    \n",
    "    PARSER.add_argument('--n_coords_sample', type=int, default=5000, \n",
    "                        help='Number of coordinates that should be sampled for each subject.')\n",
    "    \n",
    "    PARSER.add_argument('--min_lr', type=float, default=1e-5, \n",
    "                        help='Minimum lr, input for lr scheduler.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # CNN\n",
    "    PARSER.add_argument('--cnn_setup', type=int, default=-1, \n",
    "                        help='Setup of the CNN.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_train_cnn', type=str2bool, nargs=\"?\", const=True, default=True, \n",
    "                        help='Whether to also train the cnn during pcmra reconstruction.')\n",
    "\n",
    "    PARSER.add_argument('--mask_train_cnn', type=str2bool, nargs=\"?\", const=True, default=True, \n",
    "                        help='Whether to also train the cnn during mask segmentation.')\n",
    "\n",
    "\n",
    "    \n",
    "    # Mapping\n",
    "    PARSER.add_argument('--mapping_setup', type=int, default=-1, \n",
    "                        help='Setup of the Mapping network.')\n",
    "\n",
    "    \n",
    "    \n",
    "    # SIREN\n",
    "    PARSER.add_argument('--dim_hidden', type=int, default=256, \n",
    "                        help='Dimension of hidden SIREN layers.')\n",
    "    \n",
    "    PARSER.add_argument('--siren_hidden_layers', type=int, default=3, \n",
    "                        help='Number of hidden SIREN layers.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--first_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of first layer.')\n",
    "    \n",
    "    PARSER.add_argument('--hidden_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of hidden layer.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_first_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of first layer of PCMRA siren.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_hidden_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of hidden layer of PCMRA siren.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # optimizers\n",
    "    PARSER.add_argument('--cnn_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of cnn optim.')\n",
    "\n",
    "    PARSER.add_argument('--cnn_wd', type=float, default=0, \n",
    "                        help='Weight decay of cnn optim.')\n",
    "\n",
    "    \n",
    "    PARSER.add_argument('--mapping_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_mapping_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "    \n",
    "\n",
    "    PARSER.add_argument('--siren_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "\n",
    "    PARSER.add_argument('--siren_wd', type=float, default=0, \n",
    "                        help='Weight decay of siren optim.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_siren_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of PCMRA siren optim.')    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_siren_wd', type=float, default=0, \n",
    "                        help='Weight decay of PCMRA siren optim.')\n",
    "    \n",
    "    PARSER.add_argument('--patience', type=int, default=200, \n",
    "                        help='Patience of the LR scheduler.')\n",
    "    \n",
    "    \n",
    "    ARGS = PARSER.parse_args()\n",
    "    \n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "seq_pi_gan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
