{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run /home/ptenkaate/scratch/Master-Thesis/convert_ipynb_to_py_files.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported CNN and Mapping functions.\n",
      "Imported PI-Gan model.\n",
      "Loaded all helper functions.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math \n",
    "import skimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from py_files.new_dataset import *\n",
    "\n",
    "from py_files.cnn_model import *\n",
    "from py_files.pigan_model import *\n",
    "\n",
    "from py_files.seq_pi_gan_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():  \n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    ##### path to wich the model should be saved #####\n",
    "    path = get_folder(ARGS)\n",
    "    \n",
    "    ##### save ARGS #####\n",
    "    with open(f\"{path}/ARGS.txt\", \"w\") as f:\n",
    "        print(vars(ARGS), file=f)\n",
    "        \n",
    "    ##### data preparation #####\n",
    "    train_dl, val_dl, test_dl = initialize_dataloaders(ARGS)\n",
    "    print(next(iter(test_dl))[1])\n",
    "            \n",
    "    ##### initialize models and optimizers #####\n",
    "    models, optims, schedulers = load_models_and_optims(ARGS)\n",
    "    \n",
    "    \n",
    "    ##### load pretrained model #####\n",
    "    if ARGS.pretrained: \n",
    "        print(f\"Loading pretrained model from '{ARGS.pretrained}'.\")\n",
    "        load_pretrained_models(ARGS.pretrained, ARGS.pretrained_best_dataset, ARGS.pretrained_best_loss,\n",
    "                    models, optims, pretrained_models = ARGS.pretrained_models)\n",
    "    \n",
    "        if ARGS.pretrained_lr_reset:\n",
    "            orig_lr = {\"cnn\": ARGS.cnn_lr, \"mapping\": ARGS.mapping_lr, \"siren\": ARGS.siren_lr, \n",
    "                       \"pcmra_mapping\": ARGS.pcmra_mapping_lr, \"pcmra_siren\": ARGS.pcmra_siren_lr}\n",
    "            for name, optim in optims.items():\n",
    "                for param_group in optim.param_groups: \n",
    "                    if param_group[\"lr\"] != orig_lr[name]: \n",
    "                        param_group[\"lr\"] = ARGS.pretrained_lr_reset\n",
    "                print(f\"{name} lr: {optim.param_groups[0]['lr']}\")\n",
    "\n",
    "    ##### loss function #####\n",
    "    criterions = [nn.BCELoss(), nn.MSELoss()]\n",
    "        \n",
    "    ##### epoch, train loss mean, train loss std, val loss mean, val loss std #####\n",
    "    mask_losses, pcmra_losses, dice_losses = np.empty((0, 5)), np.empty((0, 5)), np.empty((0, 5))\n",
    "    \n",
    "    for ep in range(ARGS.pcmra_epochs):\n",
    "    \n",
    "        t = time.time() \n",
    "\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "\n",
    "        loss, _ = train_model(train_dl, models, optims, schedulers, criterions[1], ARGS, output=\"pcmra\")\n",
    "        \n",
    "        \n",
    "        if ep % ARGS.eval_every == 0: \n",
    "\n",
    "            print(f\"Epoch {ep} took {round(time.time() - t, 2)} seconds.\")\n",
    "            \n",
    "            t_pcmra_mean, t_pcmra_std, _, _ = \\\n",
    "                val_model(train_dl, models, criterions[1], ARGS, output=\"pcmra\", n_eval=100)\n",
    "            \n",
    "            v_pcmra_mean, v_pcmra_std, _, _ = \\\n",
    "                val_model(val_dl, models, criterions[1], ARGS, output=\"pcmra\", n_eval=100)\n",
    "\n",
    "            pcmra_losses = np.append(pcmra_losses, [[ep ,t_pcmra_mean, t_pcmra_std, \n",
    "                                         v_pcmra_mean, v_pcmra_std]], axis=0)\n",
    "            \n",
    "            save_loss(path, pcmra_losses, models, optims, name=\"pcmra_loss\", \n",
    "                      save_models=True)\n",
    "        \n",
    "    \n",
    "    for ep in range(ARGS.mask_epochs):\n",
    "    \n",
    "        t = time.time() \n",
    "\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "\n",
    "        loss, _ = train_model(train_dl, models, optims, schedulers, criterions[0], ARGS, output=\"mask\")\n",
    "        \n",
    "        \n",
    "        if ep % ARGS.eval_every == 0: \n",
    "\n",
    "            print(f\"Epoch {ep} took {round(time.time() - t, 2)} seconds.\")\n",
    "            \n",
    "            t_mask_mean, t_mask_std, t_dice_mean, t_dice_std = \\\n",
    "                val_model(train_dl, models, criterions[0], ARGS, output=\"mask\", n_eval=100)\n",
    "            \n",
    "            v_mask_mean, v_mask_std, v_dice_mean, v_dice_std = \\\n",
    "                val_model(val_dl, models, criterions[0], ARGS, output=\"mask\", n_eval=100)\n",
    "\n",
    "            mask_losses = np.append(mask_losses, [[ep ,t_mask_mean, t_mask_std, \n",
    "                                         v_mask_mean, v_mask_std]], axis=0)\n",
    "            \n",
    "            dice_losses = np.append(dice_losses, [[ep ,t_dice_mean, t_dice_std, \n",
    "                                         v_dice_mean, v_dice_std]], axis=0)\n",
    "            \n",
    "            save_loss(path, mask_losses, models, optims, name=\"mask_loss\", \n",
    "                      save_models=True)\n",
    "            \n",
    "            save_loss(path, dice_losses, models, optims, name=\"dice_loss\", \n",
    "                      save_models=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as .ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ARGS class initialized.\n",
      "{'device': 'GPU', 'print_models': False, 'name': 'cnn setup -2 mapping setup 7', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'dataset': 'full', 'rotate': True, 'translate': True, 'flip': True, 'crop': True, 'stretch': True, 'norm_min_max': [0, 1], 'seed': 34, 'pcmra_epochs': 5000, 'mask_epochs': 2500, 'batch_size': 12, 'eval_every': 50, 'shuffle': True, 'n_coords_sample': 5000, 'cnn_setup': -2, 'pcmra_train_cnn': True, 'mask_train_cnn': False, 'mapping_setup': 7, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30.0, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'patience': 100}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 84\n",
      "Val subjects: 28\n",
      "Test subjects: 28\n",
      "('16-03-18_Sanjay2_kt-pca_done2', '16-05-11 Sandra_kt-pca_done2', '16-06-24_Ot_kt-pca_done1', '17-02-08_VaderUiterwijk', '16-04-20_105_done', '16-06-29_115_done1', '16-06-29_116_done1', '16-08-17_117_done1', '16-08-19_119_done1', '16-08-26_121_done1', 'RESV_025', 'RESV_103')\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "PCMRA_SIREN\n",
      "Epoch 0 took 7.3 seconds.\n",
      "pcmra_loss      Train: 0.007351, \t Eval: 0.007108\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 50 took 7.64 seconds.\n",
      "pcmra_loss      Train: 0.003543, \t Eval: 0.003061\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 100 took 7.1 seconds.\n",
      "pcmra_loss      Train: 0.003231, \t Eval: 0.002837\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 150 took 5.08 seconds.\n",
      "pcmra_loss      Train: 0.003098, \t Eval: 0.002725\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 200 took 7.02 seconds.\n",
      "pcmra_loss      Train: 0.002972, \t Eval: 0.002611\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 250 took 7.31 seconds.\n",
      "pcmra_loss      Train: 0.002906, \t Eval: 0.002538\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 300 took 4.82 seconds.\n",
      "pcmra_loss      Train: 0.002875, \t Eval: 0.002563\n",
      "New best train loss, saving model.\n",
      "Epoch 350 took 6.79 seconds.\n",
      "pcmra_loss      Train: 0.002803, \t Eval: 0.002524\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 400 took 5.48 seconds.\n",
      "pcmra_loss      Train: 0.002684, \t Eval: 0.002435\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 450 took 5.64 seconds.\n",
      "pcmra_loss      Train: 0.002692, \t Eval: 0.002336\n",
      "New best eval  loss, saving model.\n",
      "Epoch 500 took 5.69 seconds.\n",
      "pcmra_loss      Train: 0.002518, \t Eval: 0.002283\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 550 took 4.09 seconds.\n",
      "pcmra_loss      Train: 0.002434, \t Eval: 0.002191\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 600 took 5.65 seconds.\n",
      "pcmra_loss      Train: 0.002358, \t Eval: 0.002132\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 650 took 5.64 seconds.\n",
      "pcmra_loss      Train: 0.002322, \t Eval: 0.002144\n",
      "New best train loss, saving model.\n",
      "Epoch 700 took 5.37 seconds.\n",
      "pcmra_loss      Train: 0.002264, \t Eval: 0.002058\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 750 took 5.65 seconds.\n",
      "pcmra_loss      Train: 0.002445, \t Eval: 0.002144\n",
      "Epoch 800 took 5.37 seconds.\n",
      "pcmra_loss      Train: 0.002179, \t Eval: 0.002051\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 850 took 4.76 seconds.\n",
      "pcmra_loss      Train: 0.002196, \t Eval: 0.001989\n",
      "New best eval  loss, saving model.\n",
      "Epoch 900 took 5.12 seconds.\n",
      "pcmra_loss      Train: 0.002148, \t Eval: 0.002021\n",
      "New best train loss, saving model.\n",
      "Epoch 950 took 7.55 seconds.\n",
      "pcmra_loss      Train: 0.002155, \t Eval: 0.001978\n",
      "New best eval  loss, saving model.\n",
      "Epoch 1000 took 4.76 seconds.\n",
      "pcmra_loss      Train: 0.002045, \t Eval: 0.001963\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "Epoch 1050 took 7.04 seconds.\n",
      "pcmra_loss      Train: 0.002133, \t Eval: 0.002037\n"
     ]
    }
   ],
   "source": [
    "for cnn_setup, mapping_setup in [(-2, 7)]:\n",
    "    \n",
    "    ARGS = init_ARGS()\n",
    "    \n",
    "    ARGS.batch_size = 12\n",
    "    \n",
    "    ARGS.dataset = \"full\"\n",
    "    \n",
    "    ARGS.cnn_setup = cnn_setup\n",
    "    ARGS.mapping_setup = mapping_setup\n",
    "    \n",
    "    ARGS.name = f\"cnn setup {ARGS.cnn_setup} mapping setup {ARGS.mapping_setup}\"\n",
    "    \n",
    "    ARGS.pcmra_epochs = 5000\n",
    "    ARGS.mask_epochs  = 2500\n",
    "    ARGS.eval_every = 50\n",
    "\n",
    "    print(vars(ARGS))\n",
    "\n",
    "    train()  \n",
    "\n",
    "    torch.cuda.empty_cache()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cnn_setup, mapping_setup in [(-1, 7)]:\n",
    "    \n",
    "    ARGS = init_ARGS()\n",
    "    \n",
    "    ARGS.batch_size = 12\n",
    "    \n",
    "    ARGS.dataset = \"full\"\n",
    "    \n",
    "    ARGS.cnn_setup = cnn_setup\n",
    "    ARGS.mapping_setup = mapping_setup\n",
    "    \n",
    "    ARGS.name = f\"cnn setup {ARGS.cnn_setup} mapping setup {ARGS.mapping_setup}\"\n",
    "    \n",
    "    ARGS.pcmra_epochs = 5000\n",
    "    ARGS.mask_epochs  = 2500\n",
    "    ARGS.eval_every = 50\n",
    "\n",
    "    print(vars(ARGS))\n",
    "\n",
    "    train()  \n",
    "\n",
    "    torch.cuda.empty_cache()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### NO TRANSFORMATION\n",
    "\n",
    "\n",
    "# for cnn_setup, mapping_setup in [(14, 7)]:\n",
    "    \n",
    "#     ARGS = init_ARGS()\n",
    "    \n",
    "#     ARGS.rotate = False\n",
    "#     ARGS.translate = False\n",
    "#     ARGS.flip = False\n",
    "#     ARGS.stretch = False\n",
    "#     ARGS.crop = False\n",
    "\n",
    "#     ARGS.cnn_setup = cnn_setup\n",
    "#     ARGS.mapping_setup = mapping_setup\n",
    "    \n",
    "#     ARGS.pcmra_epochs = 2500\n",
    "#     ARGS.mask_epochs  = 1500\n",
    "#     ARGS.eval_every = 50\n",
    "\n",
    "#     ARGS.name = f\"cnn setup {ARGS.cnn_setup} mapping setup {ARGS.mapping_setup} no transformation\"\n",
    "\n",
    "#     print(vars(ARGS))\n",
    "\n",
    "#     train()  \n",
    "\n",
    "#     torch.cuda.empty_cache()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn_setup, mapping_setup in [(14, 7)]:\n",
    "    \n",
    "#     ARGS = init_ARGS()\n",
    "    \n",
    "#     ARGS.cnn_setup = cnn_setup\n",
    "#     ARGS.mapping_setup = mapping_setup\n",
    "    \n",
    "         \n",
    "#     ARGS.pretrained = \"pi-gan 25-05-2021 04:50:07 cnn setup 14 mapping setup 7 weight decay 1e-5 cnn and sirens\"\n",
    "#     ARGS.pretrained_lr_reset = 1e-4\n",
    "#     ARGS.pretrained_best_loss = \"pcmra\"\n",
    "   \n",
    "#     ARGS.pcmra_siren_wd = 1e-5\n",
    "#     ARGS.siren_wd = 1e-5\n",
    "#     ARGS.cnn_wd = 1e-5\n",
    "\n",
    "\n",
    "#     ARGS.name = f\"cnn setup {ARGS.cnn_setup} mapping setup {ARGS.mapping_setup} weight decay 1e-5 cnn and sirens\"\n",
    "    \n",
    "#     ARGS.pcmra_epochs = 5000\n",
    "#     ARGS.mask_epochs  = 2500\n",
    "#     ARGS.eval_every = 50\n",
    "\n",
    "#     print(vars(ARGS))\n",
    "\n",
    "#     train()  \n",
    "\n",
    "#     torch.cuda.empty_cache()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    PARSER = argparse.ArgumentParser()\n",
    "\n",
    "    # Arguments for training\n",
    "    PARSER.add_argument('--device', type=str, default=\"GPU\", \n",
    "                        help='Device that should be used.')\n",
    "\n",
    "    PARSER.add_argument('--print_models', type=bool, default=False, \n",
    "                        help='Print the models after initialization or not.')\n",
    "\n",
    "    PARSER.add_argument('--name', type=str, default=\"\", \n",
    "                        help='Name of the folder where the output should be saved.')\n",
    "    \n",
    "\n",
    "    # pretrained params \n",
    "    \n",
    "    PARSER.add_argument('--pretrained', type=str, default=None, \n",
    "                        help='Folder name of pretrained model that should be loaded.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_best_dataset', type=str, default=\"train\", \n",
    "                        help='Pretrained model with lowest [train, val] loss.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_best_loss', type=str, default=\"mask\", \n",
    "                        help='Pretrained model with lowest [train, val] loss.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_models', type=str, default=None, \n",
    "                        help='Choose which pretrained models to load. None = all models')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_lr_reset', type=str, default=None, \n",
    "                        help='Reset the lr to a value.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # data\n",
    "    PARSER.add_argument('--dataset', type=str, default=\"small\", \n",
    "                        help='The dataset which we train on.')\n",
    "    \n",
    "    PARSER.add_argument('--rotate', type=bool, default=True, \n",
    "                        help='Rotations of the same image')\n",
    "    \n",
    "    PARSER.add_argument('--translate', type=bool, default=True, \n",
    "                        help='Translations of the same image')\n",
    "    \n",
    "    PARSER.add_argument('--flip', type=bool, default=True, \n",
    "                        help='Flips the train image')\n",
    "    \n",
    "    PARSER.add_argument('--crop', type=bool, default=True, \n",
    "                        help='Crops the train image')\n",
    "\n",
    "    PARSER.add_argument('--stretch', type=bool, default=True, \n",
    "                        help='Stretches the train image')\n",
    "\n",
    "    PARSER.add_argument('--norm_min_max', type=list, default=[0, 1], \n",
    "                        help='List with min and max for normalizing input.')\n",
    "    \n",
    "    PARSER.add_argument('--seed', type=int, default=34, \n",
    "                        help='Seed for initializig dataloader')\n",
    "    \n",
    "    \n",
    "    # train variables\n",
    "    PARSER.add_argument('--pcmra_epochs', type=int, default=5000, \n",
    "                        help='Number of epochs for pcmra training.')\n",
    "\n",
    "    PARSER.add_argument('--mask_epochs', type=int, default=2500, \n",
    "                        help='Number of epochs for mask training.')\n",
    "    \n",
    "    PARSER.add_argument('--batch_size', type=int, default=24, \n",
    "                        help='Number of epochs.')\n",
    "        \n",
    "    PARSER.add_argument('--eval_every', type=int, default=50, \n",
    "                        help='Set the # epochs after which evaluation should be done.')\n",
    "    \n",
    "    PARSER.add_argument('--shuffle', type=bool, default=True, \n",
    "                        help='Shuffle the train dataloader?')\n",
    "    \n",
    "    PARSER.add_argument('--n_coords_sample', type=int, default=5000, \n",
    "                        help='Number of coordinates that should be sampled for each subject.')\n",
    "    \n",
    "    \n",
    "    # CNN\n",
    "    PARSER.add_argument('--cnn_setup', type=int, default=1, \n",
    "                        help='Setup of the CNN.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_train_cnn', type=bool, default=True, \n",
    "                        help='Whether to also train the cnn during pcmra reconstruction.')\n",
    "\n",
    "    PARSER.add_argument('--mask_train_cnn', type=bool, default=False, \n",
    "                        help='Whether to also train the cnn during mask segmentation.')\n",
    "\n",
    "\n",
    "    \n",
    "    # Mapping\n",
    "    PARSER.add_argument('--mapping_setup', type=int, default=2, \n",
    "                        help='Setup of the Mapping network.')\n",
    "\n",
    "    \n",
    "    # SIREN\n",
    "    PARSER.add_argument('--dim_hidden', type=int, default=256, \n",
    "                        help='Dimension of hidden SIREN layers.')\n",
    "    \n",
    "    PARSER.add_argument('--siren_hidden_layers', type=int, default=3, \n",
    "                        help='Number of hidden SIREN layers.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--first_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of first layer.')\n",
    "    \n",
    "    PARSER.add_argument('--hidden_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of hidden layer.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_first_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of first layer of PCMRA siren.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_hidden_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of hidden layer of PCMRA siren.')\n",
    "    \n",
    "    \n",
    "    # optimizers\n",
    "    PARSER.add_argument('--cnn_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of cnn optim.')\n",
    "\n",
    "    PARSER.add_argument('--cnn_wd', type=float, default=0, \n",
    "                        help='Weight decay of cnn optim.')\n",
    "\n",
    "    \n",
    "    PARSER.add_argument('--mapping_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_mapping_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "    \n",
    "\n",
    "    PARSER.add_argument('--siren_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "\n",
    "    PARSER.add_argument('--siren_wd', type=float, default=0, \n",
    "                        help='Weight decay of siren optim.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_siren_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of PCMRA siren optim.')    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_siren_wd', type=float, default=0, \n",
    "                        help='Weight decay of PCMRA siren optim.')\n",
    "    \n",
    "    PARSER.add_argument('--patience', type=int, default=100, \n",
    "                        help='Patience of the LR scheduler.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ARGS = PARSER.parse_args()\n",
    "    \n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitc17f53f707db4b89be7c32a22adf91a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
