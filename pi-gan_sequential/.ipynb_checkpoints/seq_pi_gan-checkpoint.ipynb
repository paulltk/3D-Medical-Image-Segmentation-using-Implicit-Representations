{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/data_classes/new_dataset.ipynb to script\n",
      "[NbConvertApp] Writing 1730 bytes to py_files/new_dataset.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/model_classes/cnn_model.ipynb to script\n",
      "[NbConvertApp] Writing 48797 bytes to py_files/cnn_model.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/model_classes/pigan_model.ipynb to script\n",
      "[NbConvertApp] Writing 3587 bytes to py_files/pigan_model.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/pi-gan/pi_gan_functions.ipynb to script\n",
      "[NbConvertApp] Writing 21735 bytes to py_files/pi_gan_functions.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/pi-gan_sequential/seq_pi_gan_functions.ipynb to script\n",
      "[NbConvertApp] Writing 25968 bytes to py_files/seq_pi_gan_functions.py\n",
      "[NbConvertApp] Converting notebook /home/ptenkaate/scratch/Master-Thesis/pi-gan_sequential/seq_pi_gan.ipynb to script\n",
      "[NbConvertApp] Writing 13160 bytes to py_files/seq_pi_gan.py\n",
      "Saved py files.\n"
     ]
    }
   ],
   "source": [
    "%run /home/ptenkaate/scratch/Master-Thesis/convert_ipynb_to_py_files.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported CNN and Mapping functions.\n",
      "Imported PI-Gan model.\n",
      "Loaded all helper functions.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math \n",
    "import skimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from py_files.new_dataset import *\n",
    "\n",
    "from py_files.cnn_model import *\n",
    "from py_files.pigan_model import *\n",
    "\n",
    "from py_files.seq_pi_gan_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():  \n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    ##### path to wich the model should be saved #####\n",
    "    path = get_folder(ARGS)\n",
    "    \n",
    "    ##### save ARGS #####\n",
    "    with open(f\"{path}/ARGS.txt\", \"w\") as f:\n",
    "        print(vars(ARGS), file=f)\n",
    "        \n",
    "    ##### data preparation #####\n",
    "    train_dl, val_dl, test_dl = initialize_dataloaders(ARGS)\n",
    "    print(\"train batch:\", next(iter(train_dl))[1][:5])\n",
    "    print(\"eval batch:\", next(iter(val_dl))[1][:5])\n",
    "    print(\"test batch:\", next(iter(test_dl))[1][:5])\n",
    "            \n",
    "    ##### initialize models and optimizers #####\n",
    "    models, optims, schedulers = load_models_and_optims(ARGS)\n",
    "    \n",
    "    \n",
    "    ##### load pretrained model #####\n",
    "    if ARGS.pretrained: \n",
    "        print(f\"Loading pretrained model from '{ARGS.pretrained}'.\")\n",
    "        load_pretrained_models(ARGS.pretrained, ARGS.pretrained_best_dataset, ARGS.pretrained_best_loss,\n",
    "                    models, optims, pretrained_models = ARGS.pretrained_models)\n",
    "    \n",
    "        if ARGS.pretrained_lr_reset:\n",
    "            orig_lr = {\"cnn\": ARGS.cnn_lr, \"mapping\": ARGS.mapping_lr, \"siren\": ARGS.siren_lr, \n",
    "                       \"pcmra_mapping\": ARGS.pcmra_mapping_lr, \"pcmra_siren\": ARGS.pcmra_siren_lr}\n",
    "            for name, optim in optims.items():\n",
    "                for param_group in optim.param_groups: \n",
    "                    if param_group[\"lr\"] != orig_lr[name]: \n",
    "                        param_group[\"lr\"] = ARGS.pretrained_lr_reset\n",
    "                print(f\"{name} lr: {optim.param_groups[0]['lr']}\")\n",
    "\n",
    "    ##### loss function #####\n",
    "    criterions = [nn.BCELoss(), nn.MSELoss()]\n",
    "        \n",
    "    ##### epoch, train loss mean, train loss std, val loss mean, val loss std #####\n",
    "    mask_losses, pcmra_losses, dice_losses = np.empty((0, 5)), np.empty((0, 5)), np.empty((0, 5))\n",
    "    \n",
    "    for ep in range(ARGS.pcmra_epochs):\n",
    "    \n",
    "        t = time.time() \n",
    "\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "\n",
    "        loss, _ = train_model(train_dl, models, optims, schedulers, criterions[1], ARGS, output=\"pcmra\")\n",
    "        \n",
    "        \n",
    "        if ep % ARGS.eval_every == 0: \n",
    "\n",
    "            print(f\"Epoch {ep} took {round(time.time() - t, 2)} seconds.\")\n",
    "            \n",
    "            t_pcmra_mean, t_pcmra_std, _, _ = \\\n",
    "                val_model(train_dl, models, criterions[1], ARGS, output=\"pcmra\")\n",
    "            \n",
    "            v_pcmra_mean, v_pcmra_std, _, _ = \\\n",
    "                val_model(val_dl, models, criterions[1], ARGS, output=\"pcmra\")\n",
    "\n",
    "            pcmra_losses = np.append(pcmra_losses, [[ep ,t_pcmra_mean, t_pcmra_std, \n",
    "                                         v_pcmra_mean, v_pcmra_std]], axis=0)\n",
    "            \n",
    "            save_loss(path, pcmra_losses, models, optims, name=\"pcmra_loss\", \n",
    "                      save_models=True)\n",
    "        \n",
    "    \n",
    "    for ep in range(ARGS.mask_epochs):\n",
    "    \n",
    "        t = time.time() \n",
    "\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "\n",
    "        loss, _ = train_model(train_dl, models, optims, schedulers, criterions[0], ARGS, output=\"mask\")\n",
    "        \n",
    "        \n",
    "        if ep % ARGS.eval_every == 0: \n",
    "\n",
    "            print(f\"Epoch {ep} took {round(time.time() - t, 2)} seconds.\")\n",
    "            \n",
    "            t_mask_mean, t_mask_std, t_dice_mean, t_dice_std = \\\n",
    "                val_model(train_dl, models, criterions[0], ARGS, output=\"mask\")\n",
    "            \n",
    "            v_mask_mean, v_mask_std, v_dice_mean, v_dice_std = \\\n",
    "                val_model(val_dl, models, criterions[0], ARGS, output=\"mask\")\n",
    "\n",
    "            mask_losses = np.append(mask_losses, [[ep ,t_mask_mean, t_mask_std, \n",
    "                                         v_mask_mean, v_mask_std]], axis=0)\n",
    "            \n",
    "            dice_losses = np.append(dice_losses, [[ep ,t_dice_mean, t_dice_std, \n",
    "                                         v_dice_mean, v_dice_std]], axis=0)\n",
    "            \n",
    "            save_loss(path, mask_losses, models, optims, name=\"mask_loss\", \n",
    "                      save_models=True)\n",
    "            \n",
    "            save_loss(path, dice_losses, models, optims, name=\"dice_loss\", \n",
    "                      save_models=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as .ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ARGS class initialized.\n",
      "{'device': 'GPU', 'print_models': False, 'name': '', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'dataset': 'new', 'rotate': True, 'translate': True, 'translate_max_pixels': 20, 'flip': True, 'crop': True, 'stretch': True, 'stretch_factor': 1.2, 'norm_min_max': [0, 1], 'seed': 34, 'pcmra_epochs': 0, 'mask_epochs': 5000, 'batch_size': 24, 'eval_every': 200, 'shuffle': True, 'n_coords_sample': 5000, 'cnn_setup': -4, 'pcmra_train_cnn': True, 'mask_train_cnn': True, 'mapping_setup': -2, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'patience': 150, 'min_lr': 5e-06}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 86\n",
      "Val subjects: 29\n",
      "Test subjects: 29\n",
      "train batch: ('43', '13', '5', '65', '58')\n",
      "eval batch: ('9', '35', '37', '30', '31')\n",
      "test batch: ('3', '4', '1', '32', '21')\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "PCMRA_SIREN\n",
      "Epoch 0 took 0.87 seconds.\n",
      "mask_loss       Train: 0.568775, \t Eval: 0.566918\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 1.000000, \t Eval: 1.000000\n",
      "Epoch 200 took 1.05 seconds.\n",
      "mask_loss       Train: 0.084110, \t Eval: 0.088614\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 1.000000, \t Eval: 1.000000\n",
      "Epoch 400 took 0.9 seconds.\n",
      "mask_loss       Train: 0.077102, \t Eval: 0.083161\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.930282, \t Eval: 0.943413\n",
      "Epoch 600 took 1.13 seconds.\n",
      "mask_loss       Train: 0.070800, \t Eval: 0.075216\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.928889, \t Eval: 0.930965\n",
      "Epoch 800 took 0.81 seconds.\n",
      "mask_loss       Train: 0.068560, \t Eval: 0.072993\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.759544, \t Eval: 0.759066\n",
      "Epoch 1000 took 0.91 seconds.\n",
      "mask_loss       Train: 0.066629, \t Eval: 0.071637\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.847898, \t Eval: 0.845568\n",
      "Epoch  1086: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch  1086: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch  1086: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 1200 took 0.91 seconds.\n",
      "mask_loss       Train: 0.064722, \t Eval: 0.072621\n",
      "New best train loss, saving model.\n",
      "dice_loss       Train: 0.785345, \t Eval: 0.792739\n",
      "Epoch 1400 took 0.77 seconds.\n",
      "mask_loss       Train: 0.062755, \t Eval: 0.071221\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.665362, \t Eval: 0.707796\n",
      "Epoch 1600 took 0.62 seconds.\n",
      "mask_loss       Train: 0.064032, \t Eval: 0.072469\n",
      "dice_loss       Train: 0.725789, \t Eval: 0.731044\n",
      "Epoch 1800 took 0.78 seconds.\n",
      "mask_loss       Train: 0.059159, \t Eval: 0.067330\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.606062, \t Eval: 0.654452\n",
      "Epoch  1912: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch  1912: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch  1912: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 2000 took 0.81 seconds.\n",
      "mask_loss       Train: 0.057302, \t Eval: 0.068252\n",
      "New best train loss, saving model.\n",
      "dice_loss       Train: 0.598636, \t Eval: 0.656370\n",
      "Epoch 2200 took 0.63 seconds.\n",
      "mask_loss       Train: 0.054703, \t Eval: 0.066014\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.536723, \t Eval: 0.604910\n",
      "Epoch 2400 took 0.73 seconds.\n",
      "mask_loss       Train: 0.057080, \t Eval: 0.068723\n",
      "dice_loss       Train: 0.627557, \t Eval: 0.673936\n",
      "Epoch 2600 took 0.62 seconds.\n",
      "mask_loss       Train: 0.056692, \t Eval: 0.067474\n",
      "dice_loss       Train: 0.601019, \t Eval: 0.647393\n",
      "Epoch 2800 took 0.91 seconds.\n",
      "mask_loss       Train: 0.056971, \t Eval: 0.068037\n",
      "dice_loss       Train: 0.558230, \t Eval: 0.598494\n",
      "Epoch 3000 took 0.91 seconds.\n",
      "mask_loss       Train: 0.054818, \t Eval: 0.066462\n",
      "dice_loss       Train: 0.554242, \t Eval: 0.584112\n",
      "Epoch  3116: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch  3116: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch  3116: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 3200 took 0.9 seconds.\n",
      "mask_loss       Train: 0.053629, \t Eval: 0.065436\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.519879, \t Eval: 0.564502\n",
      "Epoch  3267: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch  3267: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch  3267: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 3400 took 0.89 seconds.\n",
      "mask_loss       Train: 0.053376, \t Eval: 0.065880\n",
      "New best train loss, saving model.\n",
      "dice_loss       Train: 0.505678, \t Eval: 0.557524\n",
      "Epoch  3432: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch  3432: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch  3432: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch 3600 took 1.07 seconds.\n",
      "mask_loss       Train: 0.054117, \t Eval: 0.066596\n",
      "dice_loss       Train: 0.523038, \t Eval: 0.573392\n",
      "Epoch 3800 took 1.06 seconds.\n",
      "mask_loss       Train: 0.052752, \t Eval: 0.066135\n",
      "New best train loss, saving model.\n",
      "dice_loss       Train: 0.514865, \t Eval: 0.579171\n",
      "Epoch 4000 took 0.88 seconds.\n",
      "mask_loss       Train: 0.052771, \t Eval: 0.066087\n",
      "dice_loss       Train: 0.505554, \t Eval: 0.556946\n",
      "Epoch 4200 took 0.9 seconds.\n",
      "mask_loss       Train: 0.053181, \t Eval: 0.066251\n",
      "dice_loss       Train: 0.496341, \t Eval: 0.555056\n",
      "Epoch 4400 took 1.08 seconds.\n",
      "mask_loss       Train: 0.051893, \t Eval: 0.065510\n",
      "New best train loss, saving model.\n",
      "dice_loss       Train: 0.480857, \t Eval: 0.547553\n",
      "Epoch 4600 took 0.78 seconds.\n",
      "mask_loss       Train: 0.051959, \t Eval: 0.065667\n",
      "dice_loss       Train: 0.476627, \t Eval: 0.536469\n",
      "Epoch 4800 took 0.8 seconds.\n",
      "mask_loss       Train: 0.051929, \t Eval: 0.065708\n",
      "dice_loss       Train: 0.489401, \t Eval: 0.559821\n",
      "WARNING: ARGS class initialized.\n",
      "{'device': 'GPU', 'print_models': False, 'name': '', 'pretrained': None, 'pretrained_best_dataset': 'train', 'pretrained_best_loss': 'mask', 'pretrained_models': None, 'pretrained_lr_reset': None, 'dataset': 'new', 'rotate': True, 'translate': True, 'translate_max_pixels': 20, 'flip': True, 'crop': True, 'stretch': True, 'stretch_factor': 1.2, 'norm_min_max': [0, 1], 'seed': 34, 'pcmra_epochs': 0, 'mask_epochs': 5000, 'batch_size': 24, 'eval_every': 200, 'shuffle': True, 'n_coords_sample': 5000, 'cnn_setup': -3, 'pcmra_train_cnn': True, 'mask_train_cnn': True, 'mapping_setup': -1, 'dim_hidden': 256, 'siren_hidden_layers': 3, 'first_omega_0': 30, 'hidden_omega_0': 30.0, 'pcmra_first_omega_0': 30.0, 'pcmra_hidden_omega_0': 30.0, 'cnn_lr': 0.0001, 'cnn_wd': 0, 'mapping_lr': 0.0001, 'pcmra_mapping_lr': 0.0001, 'siren_lr': 0.0001, 'siren_wd': 0, 'pcmra_siren_lr': 0.0001, 'pcmra_siren_wd': 0, 'patience': 150, 'min_lr': 5e-06}\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "Train subjects: 86\n",
      "Val subjects: 29\n",
      "Test subjects: 29\n",
      "train batch: ('19', '23', '117', '58', '107')\n",
      "eval batch: ('9', '35', '37', '30', '31')\n",
      "test batch: ('3', '4', '1', '32', '21')\n",
      "----------------------------------\n",
      "Using device for training: cuda\n",
      "----------------------------------\n",
      "CNN\n",
      "MAPPING\n",
      "SIREN\n",
      "PCMRA_MAPPING\n",
      "PCMRA_SIREN\n",
      "Epoch 0 took 2.5 seconds.\n",
      "mask_loss       Train: 0.599516, \t Eval: 0.600075\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 1.000000, \t Eval: 1.000000\n",
      "Epoch 200 took 2.84 seconds.\n",
      "mask_loss       Train: 0.062048, \t Eval: 0.065260\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.500288, \t Eval: 0.500320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400 took 3.74 seconds.\n",
      "mask_loss       Train: 0.050254, \t Eval: 0.054223\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.356680, \t Eval: 0.363491\n",
      "Epoch 600 took 4.04 seconds.\n",
      "mask_loss       Train: 0.043031, \t Eval: 0.046699\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.339923, \t Eval: 0.349597\n",
      "Epoch 800 took 3.48 seconds.\n",
      "mask_loss       Train: 0.038744, \t Eval: 0.044148\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.286934, \t Eval: 0.300901\n",
      "Epoch 1000 took 3.57 seconds.\n",
      "mask_loss       Train: 0.037588, \t Eval: 0.042700\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.310889, \t Eval: 0.315538\n",
      "Epoch 1200 took 4.07 seconds.\n",
      "mask_loss       Train: 0.034519, \t Eval: 0.043064\n",
      "New best train loss, saving model.\n",
      "dice_loss       Train: 0.273982, \t Eval: 0.304787\n",
      "Epoch 1400 took 4.03 seconds.\n",
      "mask_loss       Train: 0.032617, \t Eval: 0.041273\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.240928, \t Eval: 0.265546\n",
      "Epoch  1583: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch  1583: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch  1583: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 1600 took 3.83 seconds.\n",
      "mask_loss       Train: 0.029928, \t Eval: 0.038061\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.223355, \t Eval: 0.249683\n",
      "Epoch 1800 took 2.83 seconds.\n",
      "mask_loss       Train: 0.027454, \t Eval: 0.037479\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.209221, \t Eval: 0.244723\n",
      "Epoch 2000 took 3.53 seconds.\n",
      "mask_loss       Train: 0.026809, \t Eval: 0.035831\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.220955, \t Eval: 0.254227\n",
      "Epoch 2200 took 5.25 seconds.\n",
      "mask_loss       Train: 0.024722, \t Eval: 0.035705\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.194800, \t Eval: 0.237781\n",
      "Epoch  2308: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch  2308: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch  2308: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 2400 took 4.0 seconds.\n",
      "mask_loss       Train: 0.023722, \t Eval: 0.034699\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.183062, \t Eval: 0.228049\n",
      "Epoch 2600 took 2.83 seconds.\n",
      "mask_loss       Train: 0.023152, \t Eval: 0.033416\n",
      "New best train loss, saving model.\n",
      "New best eval  loss, saving model.\n",
      "dice_loss       Train: 0.178734, \t Eval: 0.223378\n",
      "Epoch  2733: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch  2733: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch  2733: reducing learning rate of group 0 to 1.2500e-05.\n"
     ]
    }
   ],
   "source": [
    "# cnn_setup = -3\n",
    "# mapping_setup = -1\n",
    "omega = 30 \n",
    "\n",
    "# for omega in [30, 100, 10, 300]:  \n",
    "for cnn_setup, mapping_setup in [(-6, -5)]:\n",
    "\n",
    "    ARGS = init_ARGS()\n",
    "    \n",
    "#     ARGS.pcmra_first_omega_0 = omega\n",
    "    ARGS.first_omega_0 = omega\n",
    "    ARGS.mask_train_cnn = True \n",
    "    \n",
    "    ARGS.dataset = \"new\"\n",
    "    \n",
    "    ARGS.batch_size = 12\n",
    "    \n",
    "    ARGS.cnn_setup = cnn_setup\n",
    "    ARGS.mapping_setup = mapping_setup\n",
    "    \n",
    "    ARGS.pcmra_epochs = 0\n",
    "    ARGS.mask_epochs = 5000\n",
    "    \n",
    "    ARGS.min_lr = 5e-6\n",
    "    ARGS.patience = 150\n",
    "    \n",
    "    ARGS.translate_max_pixels = 20\n",
    "      \n",
    "    ARGS.eval_every = 200 \n",
    "    \n",
    "    print(vars(ARGS))\n",
    "\n",
    "    train()  \n",
    "\n",
    "    torch.cuda.empty_cache()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_setup = -1\n",
    "# mapping_setup = -1\n",
    "# # omega = 30 \n",
    "\n",
    "# for omega in [30, 100, 10, 300]:  \n",
    "# # for cnn_setup, mapping_setup in [(-1, -1)]:\n",
    "\n",
    "#     ARGS = init_ARGS()\n",
    "    \n",
    "#     ARGS.pcmra_first_omega_0 = omega\n",
    "# #     ARGS.first_omega_0 = omega\n",
    "# #     ARGS.mask_train_cnn = True \n",
    "    \n",
    "#     ARGS.dataset = \"new\"\n",
    "    \n",
    "#     ARGS.cnn_setup = cnn_setup\n",
    "#     ARGS.mapping_setup = mapping_setup\n",
    "    \n",
    "#     ARGS.pcmra_epochs = 5000\n",
    "#     ARGS.mask_epochs = 0\n",
    "    \n",
    "#     ARGS.min_lr = 5e-6\n",
    "#     ARGS.patience = 100\n",
    "    \n",
    "#     ARGS.translate_max_pixels = 20\n",
    "      \n",
    "#     ARGS.eval_every = 50 \n",
    "    \n",
    "#     print(vars(ARGS))\n",
    "\n",
    "#     train()  \n",
    "\n",
    "#     torch.cuda.empty_cache()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    PARSER = argparse.ArgumentParser()\n",
    "\n",
    "    \n",
    "    # Arguments for training\n",
    "    PARSER.add_argument('--device', type=str, default=\"GPU\", \n",
    "                        help='Device that should be used.')\n",
    "\n",
    "    PARSER.add_argument('--print_models', type=str2bool, nargs='?', const=True, default=False, \n",
    "                        help='Print the models after initialization or not.')\n",
    "\n",
    "    PARSER.add_argument('--name', type=str, default=\"\", \n",
    "                        help='Name of the folder where the output should be saved.')\n",
    "    \n",
    "    \n",
    "\n",
    "    # pretrained params \n",
    "    \n",
    "    PARSER.add_argument('--pretrained', type=str, default=None, \n",
    "                        help='Folder name of pretrained model that should be loaded.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_best_dataset', type=str, default=\"train\", \n",
    "                        help='Pretrained model with lowest [train, val] loss.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_best_loss', type=str, default=\"mask\", \n",
    "                        help='Pretrained model with lowest [train, val] loss.')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_models', type=str, default=None, \n",
    "                        help='Choose which pretrained models to load. None = all models')\n",
    "    \n",
    "    PARSER.add_argument('--pretrained_lr_reset', type=str, default=None, \n",
    "                        help='Reset the lr to a value.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # data\n",
    "    PARSER.add_argument('--dataset', type=str, default=\"new\", \n",
    "                        help='The dataset which we train on.')\n",
    "    \n",
    "    PARSER.add_argument('--seed', type=int, default=34, \n",
    "                        help='Seed for initializig dataloader')\n",
    "    \n",
    "    PARSER.add_argument('--rotate', type=str2bool, nargs='?', const=True, default=True, \n",
    "                        help='Rotations of the same image')\n",
    "    \n",
    "    PARSER.add_argument('--translate', type=str2bool, nargs='?', const=True, default=True, \n",
    "                        help='Translations of the same image')\n",
    "    \n",
    "    PARSER.add_argument('--translate_max_pixels', type=int, default=20, \n",
    "                        help='Translation max in height and width.')\n",
    "    \n",
    "    PARSER.add_argument('--flip', type=str2bool, nargs='?', const=True, default=True, \n",
    "                        help='Flips the train image')\n",
    "    \n",
    "    PARSER.add_argument('--crop', type=str2bool, nargs='?', const=True, default=True, \n",
    "                        help='Crops the train image')\n",
    "\n",
    "    PARSER.add_argument('--stretch', type=str2bool, nargs='?', const=True, default=True, \n",
    "                        help='Stretches the train image')\n",
    "\n",
    "    PARSER.add_argument('--stretch_factor', type=float, default=1.2, \n",
    "                        help='Stretch maximum of the train image')\n",
    "\n",
    "    PARSER.add_argument('--norm_min_max', type=list, default=[0, 1], \n",
    "                        help='List with min and max for normalizing input.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train variables\n",
    "    PARSER.add_argument('--pcmra_epochs', type=int, default=5000, \n",
    "                        help='Number of epochs for pcmra training.')\n",
    "\n",
    "    PARSER.add_argument('--mask_epochs', type=int, default=5000, \n",
    "                        help='Number of epochs for mask training.')\n",
    "    \n",
    "    PARSER.add_argument('--batch_size', type=int, default=24, \n",
    "                        help='Number of epochs.')\n",
    "        \n",
    "    PARSER.add_argument('--eval_every', type=int, default=50, \n",
    "                        help='Set the # epochs after which evaluation should be done.')\n",
    "    \n",
    "    PARSER.add_argument('--shuffle', type=str2bool, nargs='?', const=True, default=True, \n",
    "                        help='Shuffle the train dataloader?')\n",
    "    \n",
    "    PARSER.add_argument('--n_coords_sample', type=int, default=5000, \n",
    "                        help='Number of coordinates that should be sampled for each subject.')\n",
    "    \n",
    "    PARSER.add_argument('--min_lr', type=float, default=1e-5, \n",
    "                        help='Minimum lr, input for lr scheduler.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # CNN\n",
    "    PARSER.add_argument('--cnn_setup', type=int, default=-1, \n",
    "                        help='Setup of the CNN.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_train_cnn', type=str2bool, nargs='?', const=True, default=True, \n",
    "                        help='Whether to also train the cnn during pcmra reconstruction.')\n",
    "\n",
    "    PARSER.add_argument('--mask_train_cnn', type=str2bool, nargs='?', const=True, default=True, \n",
    "                        help='Whether to also train the cnn during mask segmentation.')\n",
    "\n",
    "\n",
    "    \n",
    "    # Mapping\n",
    "    PARSER.add_argument('--mapping_setup', type=int, default=-1, \n",
    "                        help='Setup of the Mapping network.')\n",
    "\n",
    "    \n",
    "    \n",
    "    # SIREN\n",
    "    PARSER.add_argument('--dim_hidden', type=int, default=256, \n",
    "                        help='Dimension of hidden SIREN layers.')\n",
    "    \n",
    "    PARSER.add_argument('--siren_hidden_layers', type=int, default=3, \n",
    "                        help='Number of hidden SIREN layers.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--first_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of first layer.')\n",
    "    \n",
    "    PARSER.add_argument('--hidden_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of hidden layer.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_first_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of first layer of PCMRA siren.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_hidden_omega_0', type=float, default=30., \n",
    "                        help='Omega_0 of hidden layer of PCMRA siren.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # optimizers\n",
    "    PARSER.add_argument('--cnn_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of cnn optim.')\n",
    "\n",
    "    PARSER.add_argument('--cnn_wd', type=float, default=0, \n",
    "                        help='Weight decay of cnn optim.')\n",
    "\n",
    "    \n",
    "    PARSER.add_argument('--mapping_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "    \n",
    "    PARSER.add_argument('--pcmra_mapping_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "    \n",
    "\n",
    "    PARSER.add_argument('--siren_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of siren optim.')\n",
    "\n",
    "    PARSER.add_argument('--siren_wd', type=float, default=0, \n",
    "                        help='Weight decay of siren optim.')\n",
    "    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_siren_lr', type=float, default=1e-4, \n",
    "                        help='Learning rate of PCMRA siren optim.')    \n",
    "    \n",
    "    PARSER.add_argument('--pcmra_siren_wd', type=float, default=0, \n",
    "                        help='Weight decay of PCMRA siren optim.')\n",
    "    \n",
    "    PARSER.add_argument('--patience', type=int, default=200, \n",
    "                        help='Patience of the LR scheduler.')\n",
    "    \n",
    "    \n",
    "    ARGS = PARSER.parse_args()\n",
    "    \n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitc17f53f707db4b89be7c32a22adf91a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
